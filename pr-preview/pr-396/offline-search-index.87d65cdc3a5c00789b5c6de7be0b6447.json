[{"body":"Overview This walkthrough teaches you how to build a production-ready job application using Humus to solve the One Billion Row Challenge. You’ll learn how to:\nBuild a job that processes large datasets Parse and aggregate 1 billion temperature measurements Start with local files for fast iteration Refactor to cloud storage (MinIO/S3) Test your job with minimal infrastructure before adding observability Retrofit OpenTelemetry instrumentation (traces, metrics, logs) to working code Monitor your job in Grafana with distributed tracing Use Humus patterns for configuration, error handling, and graceful shutdown This walkthrough follows a practical development workflow: get your core algorithm working with local files first, then progressively add cloud storage and observability.\nWhat You’ll Build A complete job application that:\nParses temperature measurement data in the format city;temperature Calculates min/mean/max statistics per city Writes formatted results to output Starts with local file I/O for fast development Refactors to MinIO (S3-compatible storage) for production Exports telemetry to Tempo (traces), Mimir (metrics), and Loki (logs) Input: 1 billion lines like Tokyo;35.6\\nJakarta;-6.2\\n... Output: One line per city: Jakarta=-10.0/26.5/45.3\\nTokyo=-5.2/35.6/50.1\\n...\nPrerequisites Go 1.24+ installed Podman or Docker for running infrastructure Basic understanding of Go (contexts, interfaces, error handling) Familiarity with command-line tools What You’ll Learn Humus Framework Patterns Builder + Runner pattern: How Humus composes apps with middleware Config embedding: Using job.Config with custom configuration Automatic OTel: Zero-manual-setup observability Graceful shutdown: OS signal handling and resource cleanup OpenTelemetry Integration Creating manual spans for fine-grained tracing Recording custom metrics Structured logging with trace correlation Viewing distributed traces in Grafana Job Architecture Separating concerns (storage, parsing, calculation, orchestration) Streaming large files without loading into memory Error handling and context propagation Time Estimate Setup: 10 minutes Code walkthrough: 30-45 minutes Running and monitoring: 15 minutes Walkthrough Sections Project Setup - Directory structure and dependencies Building a Basic Job - Core job structure with minimal config 1BRC Algorithm with Local Files - Parsing and calculating with local file I/O Running With Local Files - Test quickly with zero infrastructure Refactoring to MinIO Storage - Upgrade to S3-compatible cloud storage Infrastructure Setup - Adding the LGTM observability stack Adding Observability - Retrofitting traces, metrics, and logs Running and Monitoring - Execute and view telemetry in Grafana Source Code The complete working example is located at:\ngithub.com/z5labs/humus/example/job/1brc-walkthrough Next Steps Begin with Project Setup to understand the code structure.\n","categories":"","description":"Build a production job to solve the One Billion Row Challenge","excerpt":"Build a production job to solve the One Billion Row Challenge","ref":"/humus/pr-preview/pr-396/walkthroughs/1brc-job/","tags":"","title":"1BRC Job Walkthrough"},{"body":"This guide will help you get started with Humus, from installation to building your first service.\nPrerequisites Before you begin, ensure you have:\nGo 1.21 or later installed on your system Basic familiarity with Go programming Understanding of REST APIs, gRPC, or batch processing (depending on your use case) What You’ll Learn This section covers:\nInstallation - Installing Humus and setting up your environment Your First Service - Building a Hello World REST service Configuration - Understanding the YAML configuration system Project Structure - Recommended project layout patterns Quick Start If you want to jump right in:\n# Install Humus go get github.com/z5labs/humus # Create a new project mkdir my-service \u0026\u0026 cd my-service go mod init my-service Then follow the First Service guide to build your first application.\nNext Steps Once you’re comfortable with the basics, explore:\nCore Concepts - Understand Humus architecture REST Services - Build HTTP APIs gRPC Services - Build gRPC microservices Job Services - Build batch processors ","categories":"","description":"Get up and running with Humus","excerpt":"Get up and running with Humus","ref":"/humus/pr-preview/pr-396/getting-started/","tags":"","title":"Getting Started"},{"body":"In this walkthrough, you’ll build a complete order management REST API that demonstrates key Humus patterns including service orchestration, cursor-based pagination, and automatic OpenTelemetry instrumentation.\nWhat You’ll Build A REST API with two endpoints:\nGET /v1/orders - List orders with cursor-based pagination and filtering POST /v1/order - Place orders with multi-service orchestration The API integrates with three backend services:\nData Service - DynamoDB-like storage for orders Restriction Service - Checks account restrictions (fraud, trading holds) Eligibility Service - Validates order eligibility Architecture Overview The API orchestrates calls to three backend services with different flows for each endpoint:\nGET /v1/orders - List Orders Flow sequenceDiagram participant Client participant Orders API participant Data Service Client-\u003e\u003eOrders API: GET /v1/orders?cursor=abc\u0026limit=10 Orders API-\u003e\u003eData Service: Query orders with pagination Data Service--\u003e\u003eOrders API: Return orders + next cursor Orders API--\u003e\u003eClient: JSON response with orders array POST /v1/order - Place Order Flow sequenceDiagram participant Client participant Orders API participant Restriction Service participant Eligibility Service participant Data Service Client-\u003e\u003eOrders API: POST /v1/order (order details) Orders API-\u003e\u003eRestriction Service: Check account restrictions Restriction Service--\u003e\u003eOrders API: Restriction status Orders API-\u003e\u003eEligibility Service: Validate order eligibility Eligibility Service--\u003e\u003eOrders API: Eligibility status Orders API-\u003e\u003eData Service: Save order Data Service--\u003e\u003eOrders API: Order saved Orders API--\u003e\u003eClient: Order confirmation Learning Objectives By the end of this walkthrough, you will:\nStructure a Humus REST application - Understand the endpoint/service/app pattern where endpoints define interfaces Implement service interfaces - Create HTTP client wrappers with proper error handling Use RPC patterns - Apply rest.ProduceJson for GET and rest.HandleJson for POST Add query parameters - Use rest.QueryParam with validation Implement cursor-based pagination - Build scalable pagination with base64 cursors Orchestrate services - Chain multiple service calls with error handling View distributed traces - See how OpenTelemetry automatically instruments your API Prerequisites Go 1.24 or later - Check with go version Podman (or Docker) - For running infrastructure services curl or similar HTTP client - For testing endpoints Basic Go knowledge - Structs, interfaces, HTTP handlers Time Estimate 45-60 minutes to complete all sections\nSource Code The complete source code is available at: example/rest/orders-walkthrough/\nWalkthrough Sections Project Setup - Create directory structure and initialize the module Minimal Running App - Create a minimal REST API to verify setup Scaffolding Endpoints - Define domain types and scaffold endpoints with dummy responses Backend Services - Implement data storage, restriction, and eligibility service clients List Orders Endpoint - Implement GET /v1/orders with pagination Place Order Endpoint - Implement POST /v1/order with orchestration Basic Testing - Test your API endpoints with Wiremock Observability Infrastructure - Add LGTM stack and OpenTelemetry Collector Comprehensive Testing - End-to-end testing with full observability validation Let’s get started!\nBegin with Project Setup →\n","categories":"","description":"Build a production-ready REST API with service orchestration and observability","excerpt":"Build a production-ready REST API with service orchestration and …","ref":"/humus/pr-preview/pr-396/walkthroughs/orders-rest/","tags":"","title":"Orders REST API Walkthrough"},{"body":"Let’s start by creating the project structure for our 1BRC job.\nDirectory Structure Create the following directory structure:\nmkdir -p 1brc-walkthrough/{app,service,onebrc,tool} cd 1brc-walkthrough The final structure will be:\n1brc-walkthrough/ ├── main.go # Entry point ├── config.yaml # Application configuration ├── go.mod # Module definition ├── app/ │ └── app.go # Job initialization and config ├── onebrc/ │ ├── handler.go # Job orchestration │ ├── parser.go # Parse \"city;temp\" format │ └── calculator.go # Compute statistics ├── service/ │ └── minio.go # MinIO/S3 storage abstraction ├── tool/ │ └── main.go # Generate test data ├── podman-compose.yaml # Infrastructure orchestration ├── tempo-config.yaml # Tempo trace backend config ├── mimir-config.yaml # Mimir metrics backend config ├── loki-config.yaml # Loki logs backend config ├── otel-collector-config.yaml # OpenTelemetry collector config └── grafana-datasources.yaml # Grafana datasource definitions Initialize Go Module Create go.mod:\nmodule 1brc-walkthrough go 1.24.0 require github.com/z5labs/humus v0.20.2 Package Organization Each package has a specific responsibility:\napp/ - Job initialization and configuration onebrc/ - Core business logic: orchestration, parsing, and calculation service/ - Storage abstraction layer (MinIO/S3 client) tool/ - Standalone utility to generate test data The infrastructure YAML files will be created in later sections as we add observability.\nWhat’s Next In the next section, we’ll build a basic “hello world” job to verify everything works.\nNext: Building a Basic Job →\n","categories":"","description":"Create the directory structure and understand the project layout","excerpt":"Create the directory structure and understand the project layout","ref":"/humus/pr-preview/pr-396/walkthroughs/1brc-job/01-project-setup/","tags":"","title":"Project Setup"},{"body":"Let’s start by creating the project structure for our orders REST API.\nDirectory Structure Create the following directory structure:\nmkdir -p rest-orders-walkthrough/{app,endpoint,service,wiremock/mappings} cd rest-orders-walkthrough The final structure will be:\nrest-orders-walkthrough/ ├── main.go # Entry point ├── config.yaml # Configuration with OTel settings ├── go.mod # Module definition ├── app/ │ └── app.go # API initialization and config ├── endpoint/ │ ├── model.go # Domain types (Order, pagination) │ ├── interfaces.go # Service interfaces │ ├── list_orders.go # GET /v1/orders │ └── place_order.go # POST /v1/order ├── service/ │ ├── data.go # Data service client │ ├── restriction.go # Restriction service client │ └── eligibility.go # Eligibility service client ├── podman-compose.yaml # Infrastructure stack └── wiremock/ └── mappings/ # Wiremock stub definitions Initialize Go Module Create go.mod:\nmodule rest-orders-walkthrough go 1.24.0 require github.com/z5labs/humus v0.16.0 Package Organization Each package has a specific responsibility:\nendpoint/ - Domain types, service interfaces, and REST endpoint handlers service/ - Service HTTP client implementations (depend on endpoint types) app/ - Application initialization and configuration This follows idiomatic Go where consumers define interfaces - the endpoint package defines what it needs from services, and service implementations import endpoint types to satisfy those interfaces.\nWhat’s Next In the next section, we’ll create a minimal running API to verify everything is set up correctly.\nNext: Minimal Running App →\n","categories":"","description":"Create the directory structure and initialize the Go module","excerpt":"Create the directory structure and initialize the Go module","ref":"/humus/pr-preview/pr-396/walkthroughs/orders-rest/project-setup/","tags":"","title":"Project Setup"},{"body":"Let’s create the simplest possible job to verify everything works.\nMinimal Configuration Create config.yaml:\n# Empty config for now - we'll add settings as we need them That’s it! No configuration needed yet.\nSimple Handler Create onebrc/handler.go:\npackage onebrc import ( \"context\" \"log/slog\" \"os\" ) type Handler struct { log *slog.Logger } func NewHandler() *Handler { return \u0026Handler{ log: slog.New(slog.NewJSONHandler(os.Stdout, nil)), } } func (h *Handler) Handle(ctx context.Context) error { h.log.InfoContext(ctx, \"Hello from 1BRC job!\") h.log.InfoContext(ctx, \"Job completed successfully\") return nil } Key points:\nImplements the job.Handler interface with a single Handle(context.Context) error method Uses structured logging with slog Returns nil to indicate success Application Initialization Create app/app.go:\npackage app import ( \"context\" \"1brc-walkthrough/onebrc\" \"github.com/z5labs/humus/job\" ) type Config struct { // Empty for now - we'll add fields as needed } func Init(ctx context.Context, cfg Config) (*job.App, error) { handler := onebrc.NewHandler() return job.NewApp(handler), nil } Responsibilities:\nDefine your config structure (empty for now) Create and wire up dependencies Return a *job.App with your handler Entry Point Create main.go:\npackage main import ( \"bytes\" _ \"embed\" \"1brc-walkthrough/app\" \"github.com/z5labs/humus/job\" ) //go:embed config.yaml var configBytes []byte func main() { job.Run(bytes.NewReader(configBytes), app.Init) } How it works:\n//go:embed embeds config.yaml at compile time job.Run() parses config, calls Init, runs the handler, and handles graceful shutdown Run It go mod tidy go run . You should see output like:\n{\"time\":\"2024-11-23T22:50:00Z\",\"level\":\"INFO\",\"msg\":\"Hello from 1BRC job!\"} {\"time\":\"2024-11-23T22:50:00Z\",\"level\":\"INFO\",\"msg\":\"Job completed successfully\"} The job runs, logs messages, and exits cleanly. Press Ctrl+C if you want to test graceful shutdown (though it exits immediately anyway).\nUnderstanding job.Run When you call job.Run(configReader, initFunc):\nParse config - Reads YAML and unmarshals into your Config struct Call Init - Invokes your initialization function with the parsed config Wrap handler - Adds middleware for panic recovery and OS signal handling Execute - Calls handler.Handle(ctx) Graceful shutdown - Ensures clean exit What’s Next Now that we have a working job, let’s implement the core 1BRC algorithm with local file I/O.\nNext: 1BRC Algorithm →\n","categories":"","description":"Create a minimal \"hello world\" job to verify setup","excerpt":"Create a minimal \"hello world\" job to verify setup","ref":"/humus/pr-preview/pr-396/walkthroughs/1brc-job/02-basic-job/","tags":"","title":"Building a Basic Job"},{"body":"This section covers the fundamental concepts and patterns that power Humus applications.\nWhat You’ll Learn Understanding these core concepts will help you build better applications with Humus:\nConfiguration System - Deep dive into YAML config composition and templating Observability - OpenTelemetry integration for traces, metrics, and logs Lifecycle Management - Graceful shutdown and signal handling Architecture Overview Humus is built on Bedrock, a foundational framework for application lifecycle management. Humus extends Bedrock with:\nService-Specific Builders - Pre-configured builders for REST, gRPC, and Job services Automatic Instrumentation - Built-in OpenTelemetry integration Standardized Patterns - Consistent interfaces across service types Key Principles Convention Over Configuration Humus provides sensible defaults so you can start quickly. Override only what you need:\n// Minimal configuration needed type Config struct { rest.Config `config:\",squash\"` } Composition Over Inheritance Build complex applications by composing simple pieces:\n// Compose multiple configuration sources source := config.MultiSource( config.FromYaml(\"defaults.yaml\"), config.FromYaml(\"overrides.yaml\"), ) Separation of Concerns Humus separates:\nConfiguration - What to run Initialization - How to build it Execution - When to run it func main() { // Configuration source source := rest.YamlSource(\"config.yaml\") // Initialization function init := app.Init // Execution rest.Run(source, init) } Common Patterns The Init Function Every Humus service has an Init function that receives configuration and returns the service:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Build and return your service } This function is called after configuration is loaded but before the service starts.\nConfiguration Embedding Embed framework configs to inherit standard fields:\ntype Config struct { rest.Config `config:\",squash\"` // Inherits HTTP server config Database DatabaseConfig `config:\"database\"` // Your custom config } Error Handling Use errors to fail fast during initialization:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { if cfg.Database.Host == \"\" { return nil, fmt.Errorf(\"database host required\") } // ... } Next Steps Dive deeper into specific concepts:\nConfiguration System - Master config composition Observability - Learn about traces, metrics, and logs Lifecycle Management - Understand graceful shutdown Or explore service-specific features:\nREST Services gRPC Services Job Services ","categories":"","description":"Understanding Humus architecture and patterns","excerpt":"Understanding Humus architecture and patterns","ref":"/humus/pr-preview/pr-396/concepts/","tags":"","title":"Core Concepts"},{"body":"Let’s get a minimal API running immediately to verify everything works.\nConfiguration File Create config.yaml:\nhttp: port: {{env \"HTTP_PORT\" | default 8090}} This minimal config just sets the HTTP port. The {{env \"VAR\" | default \"value\"}} syntax uses Go templating to read environment variables with fallbacks.\nApplication Initialization Create app/app.go:\npackage app import ( \"context\" \"github.com/z5labs/humus/rest\" ) type Config struct { rest.Config `config:\",squash\"` } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"Orders API\", \"v1.0.0\") return api, nil } Main Entry Point Create main.go:\npackage main import ( \"bytes\" _ \"embed\" \"rest-orders-walkthrough/app\" \"github.com/z5labs/humus/rest\" ) //go:embed config.yaml var configBytes []byte func main() { rest.Run(bytes.NewReader(configBytes), app.Init) } Key points:\n//go:embed embeds config.yaml at compile time rest.Run() handles OTel initialization, server lifecycle, and graceful shutdown Logs go to stdout by default (no external infrastructure needed yet) Run It go mod tidy go run . You should see output like:\n2025-11-21T06:43:07.123Z INFO Starting HTTP server {\"port\": 8090} The API is running! Press Ctrl+C to stop it.\nTest the Health Endpoints Humus automatically provides liveness and readiness health endpoints:\n# Liveness probe curl http://localhost:8090/health/liveness # Readiness probe curl http://localhost:8090/health/readiness Both endpoints return 200 OK by default (empty response body).\nOpenAPI Documentation Humus also automatically generates OpenAPI documentation:\ncurl http://localhost:8090/openapi.json You’ll see the OpenAPI 3.0 spec for your API (currently with no operations defined).\nWhat’s Next Now that we have a running API, let’s quickly scaffold our endpoints to see how fast we can get them working.\nNext: Scaffolding Endpoints →\n","categories":"","description":"Create a minimal REST API to verify setup","excerpt":"Create a minimal REST API to verify setup","ref":"/humus/pr-preview/pr-396/walkthroughs/orders-rest/minimal-app/","tags":"","title":"Minimal Running App"},{"body":"Let’s implement the core 1BRC algorithm using simple local file operations. This allows you to get the business logic working quickly before adding cloud storage.\nParser Implementation Create onebrc/parser.go:\npackage onebrc import ( \"bufio\" \"errors\" \"fmt\" \"io\" \"strconv\" \"strings\" ) type CityStats struct { Min float64 Max float64 Sum float64 Count int64 } func Parse(r *bufio.Reader) (map[string]*CityStats, error) { stats := make(map[string]*CityStats) for { line, err := r.ReadString('\\n') if err != nil { if errors.Is(err, io.EOF) { if len(line) \u003e 0 { if err := parseLine(line, stats); err != nil { return nil, err } } break } return nil, fmt.Errorf(\"read line: %w\", err) } if err := parseLine(line, stats); err != nil { return nil, err } } return stats, nil } func parseLine(line string, stats map[string]*CityStats) error { line = strings.TrimSpace(line) if line == \"\" { return nil } parts := strings.SplitN(line, \";\", 2) if len(parts) != 2 { return fmt.Errorf(\"invalid line format: %s\", line) } city := parts[0] temp, err := strconv.ParseFloat(parts[1], 64) if err != nil { return fmt.Errorf(\"parse temperature: %w\", err) } cityStats, exists := stats[city] if !exists { stats[city] = \u0026CityStats{ Min: temp, Max: temp, Sum: temp, Count: 1, } return nil } // Update existing stats if temp \u003c cityStats.Min { cityStats.Min = temp } if temp \u003e cityStats.Max { cityStats.Max = temp } cityStats.Sum += temp cityStats.Count++ return nil } Streaming approach:\nReads line-by-line, never loading entire file into memory Aggregates on-the-fly in a map Handles EOF properly Memory-efficient even for billion-row files Calculator Implementation Create onebrc/calculator.go:\npackage onebrc import ( \"fmt\" \"sort\" \"strings\" ) type CityResult struct { City string Min float64 Mean float64 Max float64 } func Calculate(stats map[string]*CityStats) []CityResult { results := make([]CityResult, 0, len(stats)) for city, s := range stats { mean := s.Sum / float64(s.Count) results = append(results, CityResult{ City: city, Min: s.Min, Mean: mean, Max: s.Max, }) } // Sort alphabetically by city name sort.Slice(results, func(i, j int) bool { return results[i].City \u003c results[j].City }) return results } func FormatResults(results []CityResult) string { if len(results) == 0 { return \"\" } var sb strings.Builder for _, r := range results { sb.WriteString(fmt.Sprintf(\"%s=%.1f/%.1f/%.1f\\n\", r.City, round(r.Min), round(r.Mean), round(r.Max))) } return sb.String() } // IEEE 754 rounding to 1 decimal place func round(value float64) float64 { return float64(int(value*10+0.5)) / 10 } Output format:\nAbha=-23.0/18.0/59.2 Jakarta=-10.0/26.5/45.3 Tokyo=-5.2/35.6/50.1 One city per line: city=min/mean/max\nImplement Handler with Local File I/O Create onebrc/handler.go:\npackage onebrc import ( \"bufio\" \"context\" \"fmt\" \"log/slog\" \"os\" ) type Handler struct { inputFile string outputFile string log *slog.Logger } func NewHandler(inputFile, outputFile string) *Handler { return \u0026Handler{ inputFile: inputFile, outputFile: outputFile, log: slog.New(slog.NewJSONHandler(os.Stdout, nil)), } } func (h *Handler) Handle(ctx context.Context) error { h.log.InfoContext(ctx, \"starting 1BRC processing\", slog.String(\"input_file\", h.inputFile), slog.String(\"output_file\", h.outputFile), ) // 1. Open input file f, err := os.Open(h.inputFile) if err != nil { h.log.ErrorContext(ctx, \"failed to open input file\", slog.Any(\"error\", err)) return fmt.Errorf(\"open input file: %w\", err) } defer func() { if cerr := f.Close(); cerr != nil { h.log.WarnContext(ctx, \"failed to close input file\", slog.Any(\"error\", cerr)) } }() // 2. Parse cityStats, err := Parse(bufio.NewReader(f)) if err != nil { h.log.ErrorContext(ctx, \"failed to parse temperature data\", slog.Any(\"error\", err)) return fmt.Errorf(\"parse: %w\", err) } // 3. Calculate results := Calculate(cityStats) // 4. Write results output := FormatResults(results) err = os.WriteFile(h.outputFile, []byte(output), 0644) if err != nil { h.log.ErrorContext(ctx, \"failed to write results\", slog.Any(\"error\", err)) return fmt.Errorf(\"write results: %w\", err) } h.log.InfoContext(ctx, \"1BRC processing completed successfully\", slog.Int(\"cities_processed\", len(cityStats)), ) return nil } Why local files first?\nNo external dependencies (MinIO, S3) Fast iteration cycle Easy to test and debug Proves the algorithm works Can refactor to cloud storage later Update Configuration Now that the handler is implemented, let’s wire it up through the configuration system.\nFirst, update config.yaml to add the file paths:\nonebrc: input_file: {{env \"INPUT_FILE\" | default \"measurements.txt\"}} output_file: {{env \"OUTPUT_FILE\" | default \"results.txt\"}} Then update app/app.go to define the Config struct and use the handler:\npackage app import ( \"context\" \"1brc-walkthrough/onebrc\" \"github.com/z5labs/humus/job\" ) type Config struct { OneBRC struct { InputFile string `config:\"input_file\"` OutputFile string `config:\"output_file\"` } `config:\"onebrc\"` } func Init(ctx context.Context, cfg Config) (*job.App, error) { handler := onebrc.NewHandler( cfg.OneBRC.InputFile, cfg.OneBRC.OutputFile, ) return job.NewApp(handler), nil } How config mapping works:\nThe onebrc: key in YAML maps to the OneBRC struct field via the config:\"onebrc\" tag The input_file: and output_file: keys map to InputFile and OutputFile fields Template expressions like {{env \"INPUT_FILE\"}} allow environment variable overrides What’s Next Now let’s test the complete job with local files to verify everything works.\nNext: Running Without OTel →\n","categories":"","description":"Implement the core parsing and calculation logic using local file I/O","excerpt":"Implement the core parsing and calculation logic using local file I/O","ref":"/humus/pr-preview/pr-396/walkthroughs/1brc-job/03-1brc-algorithm/","tags":"","title":"1BRC Algorithm with Local Files"},{"body":"Humus provides four distinct service types, each optimized for different use cases.\nService Types REST/HTTP Services Build OpenAPI-compliant HTTP APIs with automatic schema generation and type-safe handlers.\nBest For:\nWeb APIs and microservices Public-facing services Services requiring OpenAPI documentation Key Features:\nAutomatic OpenAPI 3.0 generation Built-in health endpoints Type-safe request/response handling Path parameter and query validation Get Started with REST →\ngRPC Services Create high-performance gRPC microservices with automatic instrumentation and health checks.\nBest For:\nInternal microservices High-performance service-to-service communication Services with strongly-typed contracts Key Features:\nAutomatic gRPC health service Built-in interceptors for OTel Service registration Protocol Buffers support Get Started with gRPC →\nJob Services Build one-off job executors for batch processing, migrations, and scheduled tasks.\nBest For:\nDatabase migrations Batch processing Scheduled tasks (with external scheduler) One-time operations Key Features:\nSimple Handler interface Full observability support Same lifecycle management as long-running services Context-aware execution Get Started with Jobs →\nQueue Services Process messages from message queues with flexible delivery semantics and automatic concurrency management.\nBest For:\nEvent-driven architectures Message queue processing (Kafka, RabbitMQ, etc.) Asynchronous workloads Stream processing Key Features:\nAt-most-once and at-least-once delivery semantics Pluggable queue runtimes Automatic concurrency management (Kafka: goroutine-per-partition) Built-in OpenTelemetry instrumentation Get Started with Queues →\nCommon Features All service types include:\nOpenTelemetry Integration - Automatic traces, metrics, and logs Graceful Shutdown - Clean shutdown on SIGTERM/SIGINT Configuration Management - YAML with template support Panic Recovery - Automatic panic recovery in handlers Lifecycle Management - Managed by Bedrock framework ","categories":"","description":"Service types and capabilities","excerpt":"Service types and capabilities","ref":"/humus/pr-preview/pr-396/features/","tags":"","title":"Features"},{"body":"Now that we have a running API, let’s scaffold our two endpoints with dummy responses. This demonstrates how quickly you can get working endpoints before implementing any business logic.\nWe’ll organize the code into a clean structure:\nendpoint/ - Domain models and HTTP endpoint handlers app/ - Application initialization Define Common Domain Models First, create endpoint/model.go with common types shared across endpoints:\npackage endpoint // OrderStatus represents the current state of an order. type OrderStatus string const ( // OrderStatusPending indicates the order is waiting to be processed. OrderStatusPending OrderStatus = \"pending\" // OrderStatusProcessing indicates the order is currently being processed. OrderStatusProcessing OrderStatus = \"processing\" // OrderStatusCompleted indicates the order has been successfully completed. OrderStatusCompleted OrderStatus = \"completed\" // OrderStatusFailed indicates the order has failed. OrderStatusFailed OrderStatus = \"failed\" ) // Order represents a customer order. type Order struct { OrderID string `json:\"order_id\"` AccountID string `json:\"account_id\"` CustomerID string `json:\"customer_id\"` Status OrderStatus `json:\"status\"` } // PageInfo contains pagination information for list responses. type PageInfo struct { HasNextPage bool `json:\"has_next_page\"` EndCursor string `json:\"end_cursor,omitempty\"` } Design Decisions Typed Constants: We use a typed OrderStatus constant rather than plain strings—this provides better type safety and IDE support.\nCursor-Based Pagination: We use cursor-based pagination instead of offset-based for several reasons:\nConsistency - No skipped or duplicated items when data changes Performance - Database can efficiently seek to cursor position Scalability - Works well with large datasets The cursor is an opaque token (base64-encoded OrderID) that points to the last item returned.\nJSON Tags: All fields have explicit JSON tags:\njson:\"order_id\" - Uses snake_case for API consistency json:\"end_cursor,omitempty\" - Omits field if empty This ensures the API response matches the OpenAPI schema exactly.\nWhy in the Endpoint Package? These endpoint-specific types (like ListOrdersResponse, PageInfo) are defined in the endpoint package because:\nAPI response structure - These types define the shape of HTTP responses returned to clients Consumer-defined interfaces - Following idiomatic Go, the endpoint package defines the service interfaces it needs (in endpoint/interfaces.go) Separation from service layer - Service packages define their own domain types (service.Order, service.OrderStatus), while endpoints convert these to API response types Package Architecture:\nservice/ packages define domain types and business logic types endpoint/ package defines API request/response types and service interfaces Endpoints import service types and convert them to API responses (see orderFromService() helper) Services never import from endpoint package - this prevents circular dependencies Only types shared across multiple endpoints in the same package belong in model.go.\nCreate List Orders Endpoint Create endpoint/list_orders.go to handle GET /v1/orders:\npackage endpoint import ( \"context\" \"net/http\" \"github.com/z5labs/humus/rest\" ) // ListOrdersResponse is the response for listing orders with pagination. type ListOrdersResponse struct { Orders []Order `json:\"orders\"` PageInfo PageInfo `json:\"page_info\"` } // ListOrders creates the GET /v1/orders endpoint. func ListOrders() rest.ApiOption { handler := rest.ProducerFunc[ListOrdersResponse](func(ctx context.Context) (*ListOrdersResponse, error) { // Return empty list for now return \u0026ListOrdersResponse{ Orders: []Order{}, PageInfo: PageInfo{ HasNextPage: false, }, }, nil }) return rest.Handle( http.MethodGet, rest.BasePath(\"/v1\").Segment(\"orders\"), rest.ProduceJson(handler), ) } This is a Producer—it produces a response without consuming a request body. Perfect for GET endpoints. The response type is defined here since it’s specific to this endpoint.\nCreate Place Order Endpoint Create endpoint/place_order.go to handle POST /v1/order:\npackage endpoint import ( \"context\" \"net/http\" \"github.com/z5labs/humus/rest\" ) // PlaceOrderRequest is the request body for placing an order. type PlaceOrderRequest struct { CustomerID string `json:\"customer_id\"` AccountID string `json:\"account_id\"` } // PlaceOrderResponse is the response for a successfully placed order. type PlaceOrderResponse struct { OrderID string `json:\"order_id\"` } // PlaceOrder creates the POST /v1/order endpoint. func PlaceOrder() rest.ApiOption { handler := rest.HandlerFunc[PlaceOrderRequest, PlaceOrderResponse]( func(ctx context.Context, req *PlaceOrderRequest) (*PlaceOrderResponse, error) { // Return dummy success response return \u0026PlaceOrderResponse{ OrderID: \"dummy-order-123\", }, nil }, ) return rest.Handle( http.MethodPost, rest.BasePath(\"/v1\").Segment(\"order\"), rest.HandleJson(handler), ) } Request/response types are defined alongside their endpoints—they represent the API contract specific to that operation.\nWire Up the Application Update app/app.go to register both endpoints:\npackage app import ( \"context\" \"github.com/z5labs/humus/example/rest/orders-walkthrough/endpoint\" \"github.com/z5labs/humus/rest\" ) // Config defines the application configuration. type Config struct { rest.Config `config:\",squash\"` } // Init initializes the REST API with all endpoints. func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi( cfg.OpenApi.Title, cfg.OpenApi.Version, endpoint.ListOrders(), endpoint.PlaceOrder(), ) return api, nil } Key patterns demonstrated here:\nSeparation of concerns - Endpoints and app logic are in separate packages GET endpoint - Uses rest.ProducerFunc (no request body) with rest.ProduceJson POST endpoint - Uses rest.HandlerFunc (request + response) with rest.HandleJson Dummy responses - Hardcoded values allow immediate testing No dependencies - No services or database needed yet Type organization - Common types in model.go, endpoint-specific types alongside their handlers Run the API go run . Test the List Endpoint curl http://localhost:8090/v1/orders Response:\n{ \"orders\": [], \"page_info\": { \"has_next_page\": false } } Test the Place Order Endpoint curl -X POST http://localhost:8090/v1/order \\ -H \"Content-Type: application/json\" \\ -d '{ \"account_id\": \"ACC123\", \"customer_id\": \"CUST456\" }' Response:\n{ \"order_id\": \"dummy-order-123\" } Inspect the OpenAPI Schema Humus automatically generates OpenAPI documentation from your Go types:\ncurl http://localhost:8090/openapi.json | jq You’ll see both operations defined with schemas for ListOrdersResponse, PlaceOrderRequest, and PlaceOrderResponse. The JSON struct tags determine the schema field names.\nLook for:\npaths[\"/v1/orders\"][\"get\"] - The list endpoint paths[\"/v1/order\"][\"post\"] - The place order endpoint components.schemas - All your type definitions What We Accomplished In just a few minutes, you:\n✅ Created a clean package structure (endpoint, app) ✅ Defined domain types with type-safe constants ✅ Registered two working endpoints with dummy handlers ✅ Tested both with real HTTP requests ✅ Generated OpenAPI documentation automatically This demonstrates Humus’s productivity: you defined types, registered handlers, and got working endpoints with OpenAPI docs—no schema files, no decorators, just Go code.\nThe clean code organization also sets you up for success as the application grows:\nendpoint/model.go - Common domain types shared across endpoints endpoint/*.go - Each endpoint file contains its specific request/response types and handler app/ - Application composition and wiring What’s Next Now that we have working endpoints with our domain model defined, let’s implement the backend services that power our order management system.\nNext: Backend Services →\n","categories":"","description":"Quickly scaffold endpoints with dummy responses","excerpt":"Quickly scaffold endpoints with dummy responses","ref":"/humus/pr-preview/pr-396/walkthroughs/orders-rest/scaffolding-endpoints/","tags":"","title":"Scaffolding Endpoints"},{"body":"This section covers advanced patterns, customizations, and deep dives into Humus internals.\nTopics Covered Builder + Runner Pattern - Deep dive into the core architecture Custom Health Monitors - Building custom health checks OTel Integration - Advanced OpenTelemetry configuration Middleware - Creating custom middleware and interceptors Testing - Testing patterns for Humus applications Multi-Source Config - Advanced configuration composition Concurrent Utilities - Thread-safe utilities Prerequisites Before diving into advanced topics, ensure you’re familiar with:\nGetting Started Core Concepts At least one service type (REST, gRPC, or Job) When to Use Advanced Topics These topics are useful when:\nYou need to customize framework behavior You’re building reusable components You want to understand how Humus works internally You need advanced configuration strategies You’re implementing custom patterns Next Steps Choose a topic based on your needs, or start with Builder + Runner Pattern to understand the core architecture.\n","categories":"","description":"Deep dives into Humus internals and patterns","excerpt":"Deep dives into Humus internals and patterns","ref":"/humus/pr-preview/pr-396/advanced/","tags":"","title":"Advanced Topics"},{"body":"Let’s implement the three backend services that power our order management system: a data storage service for persisting orders, and two validation services that check if an order can be placed.\nData Service The Data Service provides a DynamoDB-like interface for storing and querying orders. Following Go idioms, the endpoint package (consumer) defines the DataService interface, while the service package (provider) defines the types and implements the interface.\nHTTP Client Implementation Create service/data.go with the types and HTTP client implementation:\npackage service import ( \"bytes\" \"context\" \"encoding/json\" \"fmt\" \"net/http\" ) // OrderStatus represents the current state of an order. type OrderStatus string const ( // OrderStatusPending indicates the order is waiting to be processed. OrderStatusPending OrderStatus = \"pending\" // OrderStatusProcessing indicates the order is currently being processed. OrderStatusProcessing OrderStatus = \"processing\" // OrderStatusCompleted indicates the order has been successfully completed. OrderStatusCompleted OrderStatus = \"completed\" // OrderStatusFailed indicates the order has failed. OrderStatusFailed OrderStatus = \"failed\" ) // Order represents a customer order. type Order struct { OrderID string `json:\"order_id\"` AccountID string `json:\"account_id\"` CustomerID string `json:\"customer_id\"` Status OrderStatus `json:\"status\"` } // QueryRequest contains the parameters for a Query operation. type QueryRequest struct { AccountID string `json:\"account_id\"` Status OrderStatus `json:\"status,omitempty\"` Cursor string `json:\"cursor,omitempty\"` Limit int `json:\"limit\"` } // QueryResponse contains the result of a Query operation. type QueryResponse struct { Orders []Order `json:\"orders\"` HasMore bool `json:\"has_more\"` NextCursor string `json:\"next_cursor\"` } // PutItemRequest contains the parameters for a PutItem operation. type PutItemRequest struct { Order Order `json:\"order\"` } // PutItemResponse contains the result of a PutItem operation. type PutItemResponse struct { } // DataClient is a client for the data service. type DataClient struct { baseURL string httpClient *http.Client } // NewDataClient creates a new data service client. func NewDataClient(baseURL string, httpClient *http.Client) *DataClient { return \u0026DataClient{ baseURL: baseURL, httpClient: httpClient, } } Query Implementation The Query method sends all parameters in a JSON request body, following DynamoDB’s API pattern:\nfunc (s *DataClient) Query(ctx context.Context, req *QueryRequest) (*QueryResponse, error) { u := s.baseURL + \"/data/orders\" var body bytes.Buffer if err := json.NewEncoder(\u0026body).Encode(req); err != nil { return nil, fmt.Errorf(\"failed to encode request: %w\", err) } httpReq, err := http.NewRequestWithContext(ctx, http.MethodPost, u, \u0026body) if err != nil { return nil, fmt.Errorf(\"failed to create request: %w\", err) } httpReq.Header.Set(\"Content-Type\", \"application/json\") resp, err := s.httpClient.Do(httpReq) if err != nil { return nil, fmt.Errorf(\"failed to execute request: %w\", err) } defer func() { _ = resp.Body.Close() }() if resp.StatusCode != http.StatusOK { return nil, fmt.Errorf(\"unexpected status code: %d\", resp.StatusCode) } var result QueryResponse if err := json.NewDecoder(resp.Body).Decode(\u0026result); err != nil { return nil, fmt.Errorf(\"failed to decode response: %w\", err) } return \u0026result, nil } PutItem Implementation func (s *DataClient) PutItem(ctx context.Context, req *PutItemRequest) (*PutItemResponse, error) { u := s.baseURL + \"/data/orders\" var body bytes.Buffer if err := json.NewEncoder(\u0026body).Encode(req); err != nil { return nil, fmt.Errorf(\"failed to encode request: %w\", err) } httpReq, err := http.NewRequestWithContext(ctx, http.MethodPost, u, \u0026body) if err != nil { return nil, fmt.Errorf(\"failed to create request: %w\", err) } httpReq.Header.Set(\"Content-Type\", \"application/json\") resp, err := s.httpClient.Do(httpReq) if err != nil { return nil, fmt.Errorf(\"failed to execute request: %w\", err) } defer func() { _ = resp.Body.Close() }() if resp.StatusCode != http.StatusCreated { return nil, fmt.Errorf(\"unexpected status code: %d\", resp.StatusCode) } return \u0026PutItemResponse{}, nil } Restriction Service Create service/restriction.go:\npackage service import ( \"context\" \"encoding/json\" \"fmt\" \"net/http\" ) // Restriction represents a single restriction on an account. type Restriction struct { Code string `json:\"code\"` Description string `json:\"description\"` } // CheckRestrictionsRequest contains the parameters for checking account restrictions. type CheckRestrictionsRequest struct { AccountID string `json:\"account_id\"` } // CheckRestrictionsResponse contains the result of a restriction check. type CheckRestrictionsResponse struct { Restrictions []Restriction `json:\"restrictions\"` } // RestrictionClient is a client for the restriction service. type RestrictionClient struct { baseURL string httpClient *http.Client } // NewRestrictionClient creates a new restriction service client. func NewRestrictionClient(baseURL string, httpClient *http.Client) *RestrictionClient { return \u0026RestrictionClient{ baseURL: baseURL, httpClient: httpClient, } } func (s *RestrictionClient) CheckRestrictions(ctx context.Context, req *CheckRestrictionsRequest) (*CheckRestrictionsResponse, error) { u := fmt.Sprintf(\"%s/restrictions/%s\", s.baseURL, req.AccountID) httpReq, err := http.NewRequestWithContext(ctx, http.MethodGet, u, nil) if err != nil { return nil, fmt.Errorf(\"failed to create request: %w\", err) } resp, err := s.httpClient.Do(httpReq) if err != nil { return nil, fmt.Errorf(\"failed to execute request: %w\", err) } defer func() { _ = resp.Body.Close() }() if resp.StatusCode != http.StatusOK { return nil, fmt.Errorf(\"unexpected status code: %d\", resp.StatusCode) } var result CheckRestrictionsResponse if err := json.NewDecoder(resp.Body).Decode(\u0026result); err != nil { return nil, fmt.Errorf(\"failed to decode response: %w\", err) } return \u0026result, nil } Eligibility Service Create service/eligibility.go:\npackage service import ( \"context\" \"encoding/json\" \"fmt\" \"net/http\" ) // CheckEligibilityRequest contains the parameters for checking order eligibility. type CheckEligibilityRequest struct { AccountID string `json:\"account_id\"` } // CheckEligibilityResponse contains the result of an eligibility check. type CheckEligibilityResponse struct { Eligible bool `json:\"eligible\"` Reason string `json:\"reason\"` } // EligibilityClient is a client for the eligibility service. type EligibilityClient struct { baseURL string httpClient *http.Client } // NewEligibilityClient creates a new eligibility service client. func NewEligibilityClient(baseURL string, httpClient *http.Client) *EligibilityClient { return \u0026EligibilityClient{ baseURL: baseURL, httpClient: httpClient, } } func (s *EligibilityClient) CheckEligibility(ctx context.Context, req *CheckEligibilityRequest) (*CheckEligibilityResponse, error) { u := fmt.Sprintf(\"%s/eligibility/%s\", s.baseURL, req.AccountID) httpReq, err := http.NewRequestWithContext(ctx, http.MethodGet, u, nil) if err != nil { return nil, fmt.Errorf(\"failed to create request: %w\", err) } resp, err := s.httpClient.Do(httpReq) if err != nil { return nil, fmt.Errorf(\"failed to execute request: %w\", err) } defer func() { _ = resp.Body.Close() }() if resp.StatusCode != http.StatusOK { return nil, fmt.Errorf(\"unexpected status code: %d\", resp.StatusCode) } var result CheckEligibilityResponse if err := json.NewDecoder(resp.Body).Decode(\u0026result); err != nil { return nil, fmt.Errorf(\"failed to decode response: %w\", err) } return \u0026result, nil } Business Logic Scenarios The Wiremock stubs provide several test scenarios:\nRestriction Service:\nACC-001 - No restrictions (order can proceed) ACC-FRAUD - Fraud restriction (order blocked) ACC-BLOCKED - Multiple restrictions Eligibility Service:\nACC-001 - Eligible (order can proceed) ACC-INELIGIBLE - Ineligible - account type not supported ACC-NOFUNDS - Ineligible - insufficient funds Common Patterns All three service clients follow the same structure:\nExport domain types - Restriction, Order, OrderStatus Define Request/Response types - Each method takes a *Request and returns a *Response Export client struct - Holds baseURL and httpClient Constructor function - Returns pointer to concrete type Methods with consistent signature - (ctx context.Context, req *Request) (*Response, error) Key Benefits of Request/Response Pattern:\nExtensibility - Easy to add new fields without breaking method signatures Consistency - All service methods follow the same pattern Clarity - Request types document what data is needed Testability - Easy to construct test cases with different request parameters Key Architectural Decision: Services define their own types (like Restriction, CheckRestrictionsResponse, Order, OrderStatus) and never import from the endpoint package. This ensures clean separation of concerns and prevents circular dependencies.\nData Service Specific Patterns:\nDynamoDB-Like API - All parameters sent via JSON request body instead of query parameters, matching DynamoDB’s API pattern Context Propagation - All methods accept context.Context for timeouts and trace propagation Error Wrapping - Use fmt.Errorf(\"message: %w\", err) for error chains Resource Cleanup - Use defer func() { _ = resp.Body.Close() }() to ensure bodies are closed while satisfying the linter’s error checking Status Code Validation - Check for expected HTTP status codes Automatic Request Body Reuse - Passing *bytes.Buffer to http.NewRequestWithContext() automatically sets GetBody, enabling HTTP redirects and retries without manual configuration Optional Fields - Use omitempty JSON tags for optional parameters (status, cursor) so they’re omitted when empty Idiomatic Go: Consumer-Defined Interfaces Following idiomatic Go, interfaces are defined by the consumer, not the provider. The service package provides concrete implementations, while the endpoint package defines the interfaces it needs:\nendpoint/interfaces.go:\ntype DataService interface { Query(ctx context.Context, req *service.QueryRequest) (*service.QueryResponse, error) PutItem(ctx context.Context, req *service.PutItemRequest) (*service.PutItemResponse, error) } type RestrictionService interface { CheckRestrictions(ctx context.Context, req *service.CheckRestrictionsRequest) (*service.CheckRestrictionsResponse, error) } type EligibilityService interface { CheckEligibility(ctx context.Context, req *service.CheckEligibilityRequest) (*service.CheckEligibilityResponse, error) } This approach:\nKeeps dependencies minimal (endpoints only depend on methods they use) Makes testing easier (mock only what you need) Follows Go proverbs: “Accept interfaces, return structs” Testing with httptest.Server The service clients include comprehensive table-driven tests using httptest.Server:\nservice/restriction_test.go:\nfunc TestRestrictionServiceClient_CheckRestrictions(t *testing.T) { tests := []struct { name string req *CheckRestrictionsRequest mockResponse string mockStatusCode int wantErr bool wantRestrictions int }{ { name: \"no restrictions\", req: \u0026CheckRestrictionsRequest{AccountID: \"ACC-001\"}, mockResponse: `{ \"restrictions\": [] }`, mockStatusCode: http.StatusOK, wantErr: false, wantRestrictions: 0, }, // ... more test cases } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(tt.mockStatusCode) _, _ = w.Write([]byte(tt.mockResponse)) })) defer server.Close() client := NewRestrictionClient(server.URL, http.DefaultClient) resp, err := client.CheckRestrictions(context.Background(), tt.req) if tt.wantErr { require.Error(t, err) return } require.NoError(t, err) require.Len(t, resp.Restrictions, tt.wantRestrictions) }) } } Key testing patterns:\nTable-driven tests - Easy to add new scenarios Request/Response types - Clear test case structure httptest.Server - Mock HTTP backends without network calls testify/require - Fail-fast assertions (repository standard) See service/*_test.go for complete test suites covering success cases, errors, and edge cases.\nWhat’s Next With all backend services implemented and tested, let’s implement the first endpoint.\nNext: List Orders Endpoint →\n","categories":"","description":"Implement data storage, restriction, and eligibility service clients","excerpt":"Implement data storage, restriction, and eligibility service clients","ref":"/humus/pr-preview/pr-396/walkthroughs/orders-rest/backend-services/","tags":"","title":"Backend Services"},{"body":"Let’s verify your job works with simple local files before adding cloud storage and observability. This approach lets you iterate quickly and debug any issues.\nGenerate Test Data Create a simple tool to generate test data locally. Create tool/main.go:\npackage main import ( \"flag\" \"fmt\" \"log\" \"math/rand\" \"os\" ) var cities = []string{ \"Tokyo\", \"Jakarta\", \"Delhi\", \"Manila\", \"Shanghai\", \"Sao Paulo\", \"Mumbai\", \"Beijing\", \"Cairo\", \"Mexico City\", \"New York\", \"London\", \"Paris\", \"Moscow\", \"Sydney\", } func main() { count := flag.Int(\"count\", 10000, \"number of measurements to generate\") output := flag.String(\"output\", \"measurements.txt\", \"output file path\") flag.Parse() log.Printf(\"Generating %d measurements...\\n\", *count) f, err := os.Create(*output) if err != nil { log.Fatal(err) } defer f.Close() for i := 0; i \u003c *count; i++ { city := cities[rand.Intn(len(cities))] temp := -20.0 + rand.Float64()*70.0 // -20 to 50°C fmt.Fprintf(f, \"%s;%.1f\\n\", city, temp) } log.Printf(\"Generated %s successfully\\n\", *output) } Build and Generate Data # Generate test data (10K measurements for quick testing) go run tool/main.go -count 10000 -output measurements.txt You should see:\nGenerating 10000 measurements... Generated measurements.txt successfully Verify Test Data Check the generated file:\nhead -5 measurements.txt You should see lines like:\nTokyo;35.6 Jakarta;-6.2 Delhi;28.7 Manila;32.1 Shanghai;18.9 Build and Run Your Job # Ensure dependencies are installed go mod tidy # Run the job go run . You should see simple log output:\n{\"time\":\"2024-11-23T22:50:00Z\",\"level\":\"INFO\",\"msg\":\"starting 1BRC processing\",\"input_file\":\"measurements.txt\",\"output_file\":\"results.txt\"} {\"time\":\"2024-11-23T22:50:00Z\",\"level\":\"INFO\",\"msg\":\"1BRC processing completed successfully\",\"cities_processed\":15} That’s it! Your job runs in less than a second and processes the data.\nVerify Results Check the output file:\ncat results.txt Expected format:\nBeijing=-19.5/16.3/49.8 Cairo=-18.2/17.9/48.5 Delhi=-17.9/15.8/47.3 Jakarta=-19.8/14.2/49.1 London=-19.3/15.4/48.9 Mexico City=-18.7/16.1/49.5 Moscow=-19.1/14.8/49.2 Mumbai=-19.6/15.7/48.8 New York=-18.9/15.3/49.1 Paris=-19.4/16.2/49.3 Sao Paulo=-18.5/15.6/48.7 Shanghai=-19.2/15.9/49.4 Sydney=-18.8/16.5/49.6 Tokyo=-19.7/15.1/48.6 Each line shows: city=min/mean/max\nTest With Larger Dataset Try a larger dataset to see performance:\n# Generate 1 million measurements go run tool/main.go -count 1000000 -output measurements.txt # Run the job and measure execution time time go run . This will take a bit longer but you can verify the job handles larger files efficiently.\nWhat If It Doesn’t Work? Job crashes or errors:\nCheck that measurements.txt exists in the project root Verify the file has valid data in city;temperature format Check config.yaml has correct file paths No results.txt created:\nCheck for error messages in stdout Try with a smaller dataset first (1000 rows) Verify write permissions in the directory Empty results.txt:\nCheck that measurements.txt has valid data Verify format is city;temperature (semicolon separator) Check for parsing errors in the logs Why Start With Local Files? Benefits:\nZero infrastructure setup required Instant feedback loop Easy to debug with local files Proves the algorithm works correctly Can add cloud storage later without changing the core logic What’s Next Your job works with local files! Now let’s refactor it to use MinIO (S3-compatible storage) so you can process files in the cloud.\nNext: MinIO Integration →\n","categories":"","description":"Test your job quickly with local file I/O","excerpt":"Test your job quickly with local file I/O","ref":"/humus/pr-preview/pr-396/walkthroughs/1brc-job/04-running-without-otel/","tags":"","title":"Running With Local Files"},{"body":"Quick reference guides for Humus packages. For complete API documentation, see pkg.go.dev/github.com/z5labs/humus.\nPackage References REST Package - HTTP services and routing gRPC Package - gRPC services Job Package - Job services Health Package - Health monitoring Config Package - Configuration types External Documentation Go Package Documentation - Complete API reference Bedrock Documentation - Foundation framework OpenTelemetry Go - Observability SDK chi Router - HTTP router (used by REST) gRPC-Go - gRPC framework Quick Links REST Services rest.NewApi rest.Handle rest.ProduceJson rest.HandleJson rest.ConsumeOnlyJson gRPC Services grpc.NewApi grpc.Run Job Services job.Run job.Handler Health Monitoring health.Monitor health.Binary health.AndMonitor health.OrMonitor ","categories":"","description":"Quick reference for Humus packages","excerpt":"Quick reference for Humus packages","ref":"/humus/pr-preview/pr-396/reference/","tags":"","title":"API Reference"},{"body":"Let’s implement the GET /v1/orders endpoint with query parameters and pagination.\nConfiguration Structure First, create app/config.go to define the application configuration:\npackage app import \"github.com/z5labs/humus/rest\" // Config defines the application configuration. type Config struct { rest.Config `config:\",squash\"` Services struct { DataURL string `config:\"data_url\"` RestrictionURL string `config:\"restriction_url\"` EligibilityURL string `config:\"eligibility_url\"` } `config:\"services\"` } Key points:\nEmbed rest.Config with config:\",squash\" to inherit HTTP server and OTel settings Add custom Services struct for backend service URLs Tags use config: not json: for bedrock configuration system Endpoint Registration Create endpoint/list_orders.go:\npackage endpoint import ( \"context\" \"encoding/base64\" \"net/http\" \"strconv\" \"github.com/z5labs/humus/example/rest/orders-walkthrough/service\" \"github.com/z5labs/humus/rest\" ) // ListOrders creates the GET /v1/orders endpoint. func ListOrders(dataSvc DataService) rest.ApiOption { handler := \u0026listOrdersHandler{dataSvc: dataSvc} return rest.Handle( http.MethodGet, rest.BasePath(\"/v1\").Segment(\"orders\"), rest.ProduceJson(handler), rest.QueryParam(\"accountNumber\", rest.Required()), rest.QueryParam(\"after\"), rest.QueryParam(\"limit\"), rest.QueryParam(\"status\"), ) } Key components:\nrest.Handle() registers the endpoint rest.BasePath(\"/v1\").Segment(\"orders\") creates path /v1/orders rest.ProduceJson() returns JSON responses (GET pattern) rest.QueryParam() defines query parameters rest.Required() marks parameter as mandatory Handler Implementation type listOrdersHandler struct { dataSvc DataService } func (h *listOrdersHandler) Produce(ctx context.Context) (*ListOrdersResponse, error) { // Extract query parameters from context accountNumberValues := rest.QueryParamValue(ctx, \"accountNumber\") accountNumber := \"\" if len(accountNumberValues) \u003e 0 { accountNumber = accountNumberValues[0] } afterValues := rest.QueryParamValue(ctx, \"after\") afterCursor := \"\" if len(afterValues) \u003e 0 { afterCursor = afterValues[0] } limitValues := rest.QueryParamValue(ctx, \"limit\") limitStr := \"\" if len(limitValues) \u003e 0 { limitStr = limitValues[0] } statusValues := rest.QueryParamValue(ctx, \"status\") statusStr := \"\" if len(statusValues) \u003e 0 { statusStr = statusValues[0] } // Default limit limit := 10 if limitStr != \"\" { if l, err := strconv.Atoi(limitStr); err == nil \u0026\u0026 l \u003e 0 { limit = l } } // Decode cursor if provided cursor := \"\" if afterCursor != \"\" { decoded, err := base64.StdEncoding.DecodeString(afterCursor) if err == nil { cursor = string(decoded) } } // Parse status filter var status *service.OrderStatus if statusStr != \"\" { s := service.OrderStatus(statusStr) status = \u0026s } // Query data service result, err := h.dataSvc.Query(ctx, accountNumber, status, cursor, limit) if err != nil { return nil, err } // Convert service orders to endpoint orders orders := make([]Order, len(result.Orders)) for i, svcOrder := range result.Orders { orders[i] = orderFromService(svcOrder) } // Build response with cursor-based pagination response := \u0026ListOrdersResponse{ Orders: orders, PageInfo: PageInfo{ HasNextPage: result.HasMore, }, } // Encode next cursor if there are more results if result.HasMore \u0026\u0026 result.NextCursor != \"\" { response.PageInfo.EndCursor = base64.StdEncoding.EncodeToString([]byte(result.NextCursor)) } return response, nil } Cursor Encoding The cursor is base64-encoded for:\nOpacity - Clients treat it as opaque token Safety - Safe for URLs and JSON Flexibility - Can contain any string (OrderID, timestamp, etc.) Application Initialization Create app/app.go to initialize the API and register the endpoint:\npackage app import ( \"context\" \"net/http\" \"github.com/z5labs/humus/example/rest/orders-walkthrough/endpoint\" \"github.com/z5labs/humus/example/rest/orders-walkthrough/service\" \"github.com/z5labs/humus/rest\" \"go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp\" ) // Init initializes the REST API with all endpoints and services. func Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Create OTel-instrumented HTTP client for service calls httpClient := \u0026http.Client{ Transport: otelhttp.NewTransport(http.DefaultTransport), } // Initialize services dataSvc := service.NewDataClient(cfg.Services.DataURL, httpClient) // Create API with ListOrders endpoint api := rest.NewApi( cfg.OpenApi.Title, cfg.OpenApi.Version, endpoint.ListOrders(dataSvc), ) return api, nil } Important aspects:\nUse otelhttp.NewTransport to automatically instrument outgoing HTTP calls Initialize data service with URL from config Pass service to endpoint via dependency injection Register endpoint in rest.NewApi() Main Entry Point Create main.go as the application entry point:\npackage main import ( \"bytes\" _ \"embed\" \"github.com/z5labs/humus/example/rest/orders-walkthrough/app\" \"github.com/z5labs/humus/rest\" ) //go:embed config.yaml var configBytes []byte func main() { rest.Run(bytes.NewReader(configBytes), app.Init) } This is the standard Humus pattern:\nEmbed config.yaml at compile time Call rest.Run() with config reader and init function Framework handles OTel setup, server lifecycle, and graceful shutdown Configuration File Create config.yaml with service URLs and OTel configuration:\notel: resource: service_name: orders-api openapi: title: Orders API version: v1.0.0 http: port: {{env \"HTTP_PORT\" | default 8090}} services: data_url: {{env \"DATA_SERVICE_URL\" | default \"http://localhost:8080\"}} restriction_url: {{env \"RESTRICTION_SERVICE_URL\" | default \"http://localhost:8080\"}} eligibility_url: {{env \"ELIGIBILITY_SERVICE_URL\" | default \"http://localhost:8080\"}} The config uses Go templating:\n{{env \"VAR\"}} reads environment variables | default \"value\" provides fallbacks All three service URLs point to a mock server (we’ll set up Wiremock later) OTel is minimal for now (logs go to stdout) OpenAPI metadata defines the API title and version Testing the Endpoint Start the application:\ngo run . Query orders:\ncurl -s \"http://localhost:8090/v1/orders?accountNumber=ACC-001\" | jq . Response:\n{ \"orders\": [ { \"order_id\": \"ORD-001\", \"account_id\": \"ACC-001\", \"customer_id\": \"CUST-001\", \"status\": \"completed\" }, { \"order_id\": \"ORD-002\", \"account_id\": \"ACC-001\", \"customer_id\": \"CUST-001\", \"status\": \"pending\" } ], \"page_info\": { \"has_next_page\": true, \"end_cursor\": \"T1JELTAwMw==\" } } To get the next page, use the cursor:\ncurl -s \"http://localhost:8090/v1/orders?accountNumber=ACC-001\u0026after=T1JELTAwMw==\" | jq . What’s Next Now let’s implement the POST /v1/order endpoint with service orchestration.\nNext: Place Order Endpoint →\n","categories":"","description":"Implement GET /v1/orders with cursor-based pagination","excerpt":"Implement GET /v1/orders with cursor-based pagination","ref":"/humus/pr-preview/pr-396/walkthroughs/orders-rest/list-orders-endpoint/","tags":"","title":"List Orders Endpoint"},{"body":"Your job works with local files! Now let’s refactor it to use MinIO (S3-compatible storage) so you can process files in the cloud.\nWhy Cloud Storage? Benefits of MinIO/S3:\nScalable storage for large datasets Data durability and redundancy Separation of compute and storage Production-ready architecture Works with any S3-compatible service (AWS S3, MinIO, Backblaze B2, etc.) Starting MinIO Locally First, we need MinIO running. Create podman-compose.yaml:\nservices: minio: image: docker.io/minio/minio:latest command: server /data --console-address \":9001\" environment: - MINIO_ROOT_USER=minioadmin - MINIO_ROOT_PASSWORD=minioadmin ports: - \"9000:9000\" # API - \"9001:9001\" # Console volumes: - minio-data:/data:z volumes: minio-data: Start MinIO:\npodman-compose up -d Verify it’s running:\npodman ps You should see the minio container running. Access the console at http://localhost:9001 (login: minioadmin/minioadmin).\nAdd Storage Package We’ll create a new service package to abstract our storage operations.\nFirst, create the service directory:\nmkdir service Then create service/minio.go:\npackage service import ( \"context\" \"io\" \"github.com/minio/minio-go/v7\" \"github.com/minio/minio-go/v7/pkg/credentials\" ) type MinIOClient struct { mc *minio.Client } func NewMinIOClient(endpoint, accessKey, secretKey string) (*MinIOClient, error) { mc, err := minio.New(endpoint, \u0026minio.Options{ Creds: credentials.NewStaticV4(accessKey, secretKey, \"\"), Secure: false, // Use HTTP for local development }) if err != nil { return nil, err } return \u0026MinIOClient{mc: mc}, nil } func (c *MinIOClient) GetObject(ctx context.Context, bucket, key string) (io.ReadCloser, error) { return c.mc.GetObject(ctx, bucket, key, minio.GetObjectOptions{}) } func (c *MinIOClient) PutObject(ctx context.Context, bucket, key string, reader io.Reader, size int64) error { _, err := c.mc.PutObject(ctx, bucket, key, reader, size, minio.PutObjectOptions{}) return err } Key Design Decisions Wrapper pattern:\nHides MinIO-specific details Makes testing easier (mock the interface) Provides only needed methods Streaming I/O:\nGetObject returns io.ReadCloser for streaming reads PutObject accepts io.Reader to stream uploads No buffering of entire files in memory Context propagation:\nAll methods accept context.Context Enables cancellation and timeout Ready for trace spans (we’ll add later) Update Dependencies Update go.mod to add MinIO:\nmodule 1brc-walkthrough go 1.24.0 require ( github.com/z5labs/humus v0.20.2 github.com/minio/minio-go/v7 v7.0.97 ) Run go mod tidy to download the dependency.\nRefactor Handler to Use Storage Interface Update onebrc/handler.go to accept a storage interface instead of file paths:\npackage onebrc import ( \"bufio\" \"bytes\" \"context\" \"fmt\" \"io\" \"log/slog\" \"os\" ) type Storage interface { GetObject(ctx context.Context, bucket, key string) (io.ReadCloser, error) PutObject(ctx context.Context, bucket, key string, reader io.Reader, size int64) error } type Handler struct { storage Storage bucket string inputKey string outputKey string log *slog.Logger } func NewHandler(storage Storage, bucket, inputKey, outputKey string) *Handler { return \u0026Handler{ storage: storage, bucket: bucket, inputKey: inputKey, outputKey: outputKey, log: slog.New(slog.NewJSONHandler(os.Stdout, nil)), } } func (h *Handler) Handle(ctx context.Context) error { h.log.InfoContext(ctx, \"starting 1BRC processing\", slog.String(\"bucket\", h.bucket), slog.String(\"input_key\", h.inputKey), slog.String(\"output_key\", h.outputKey), ) // 1. Fetch from S3 rc, err := h.storage.GetObject(ctx, h.bucket, h.inputKey) if err != nil { h.log.ErrorContext(ctx, \"failed to fetch input object\", slog.Any(\"error\", err)) return fmt.Errorf(\"get object: %w\", err) } defer func() { if cerr := rc.Close(); cerr != nil { h.log.WarnContext(ctx, \"failed to close input object\", slog.Any(\"error\", cerr)) } }() // 2. Parse cityStats, err := Parse(bufio.NewReader(rc)) if err != nil { h.log.ErrorContext(ctx, \"failed to parse temperature data\", slog.Any(\"error\", err)) return fmt.Errorf(\"parse: %w\", err) } // 3. Calculate results := Calculate(cityStats) // 4. Write results output := FormatResults(results) outputBytes := []byte(output) err = h.storage.PutObject(ctx, h.bucket, h.outputKey, bytes.NewReader(outputBytes), int64(len(outputBytes))) if err != nil { h.log.ErrorContext(ctx, \"failed to upload results\", slog.Any(\"error\", err)) return fmt.Errorf(\"put object: %w\", err) } h.log.InfoContext(ctx, \"1BRC processing completed successfully\", slog.Int(\"cities_processed\", len(cityStats)), ) return nil } Key changes:\nChanged from file paths to Storage interface Uses GetObject instead of os.Open Uses PutObject instead of os.WriteFile Core parsing/calculation logic unchanged (that’s the beauty of interfaces!) Update Configuration Update config.yaml to add MinIO settings:\nminio: endpoint: {{env \"MINIO_ENDPOINT\" | default \"localhost:9000\"}} access_key: {{env \"MINIO_ACCESS_KEY\" | default \"minioadmin\"}} secret_key: {{env \"MINIO_SECRET_KEY\" | default \"minioadmin\"}} bucket: {{env \"MINIO_BUCKET\" | default \"onebrc\"}} onebrc: input_key: {{env \"INPUT_KEY\" | default \"measurements.txt\"}} output_key: {{env \"OUTPUT_KEY\" | default \"results.txt\"}} Note: We changed input_file/output_file to input_key/output_key to reflect S3 terminology.\nUpdate App Initialization Update app/app.go to create the MinIO client:\npackage app import ( \"context\" \"1brc-walkthrough/onebrc\" \"1brc-walkthrough/service\" \"github.com/z5labs/humus/job\" ) type Config struct { Minio struct { Endpoint string `config:\"endpoint\"` AccessKey string `config:\"access_key\"` SecretKey string `config:\"secret_key\"` Bucket string `config:\"bucket\"` } `config:\"minio\"` OneBRC struct { InputKey string `config:\"input_key\"` OutputKey string `config:\"output_key\"` } `config:\"onebrc\"` } func Init(ctx context.Context, cfg Config) (*job.App, error) { // Create MinIO client minioClient, err := service.NewMinIOClient( cfg.Minio.Endpoint, cfg.Minio.AccessKey, cfg.Minio.SecretKey, ) if err != nil { return nil, err } // Create handler with MinIO client handler := onebrc.NewHandler( minioClient, cfg.Minio.Bucket, cfg.OneBRC.InputKey, cfg.OneBRC.OutputKey, ) return job.NewApp(handler), nil } Update Data Generation Tool Update tool/main.go to upload directly to MinIO:\npackage main import ( \"bytes\" \"context\" \"flag\" \"fmt\" \"log\" \"math/rand\" \"github.com/minio/minio-go/v7\" \"github.com/minio/minio-go/v7/pkg/credentials\" ) var cities = []string{ \"Tokyo\", \"Jakarta\", \"Delhi\", \"Manila\", \"Shanghai\", \"Sao Paulo\", \"Mumbai\", \"Beijing\", \"Cairo\", \"Mexico City\", \"New York\", \"London\", \"Paris\", \"Moscow\", \"Sydney\", } func main() { count := flag.Int(\"count\", 10000, \"number of measurements to generate\") flag.Parse() // Connect to MinIO mc, err := minio.New(\"localhost:9000\", \u0026minio.Options{ Creds: credentials.NewStaticV4(\"minioadmin\", \"minioadmin\", \"\"), Secure: false, }) if err != nil { log.Fatal(err) } // Create bucket if needed ctx := context.Background() exists, err := mc.BucketExists(ctx, \"onebrc\") if err != nil { log.Fatal(err) } if !exists { err = mc.MakeBucket(ctx, \"onebrc\", minio.MakeBucketOptions{}) if err != nil { log.Fatal(err) } log.Println(\"Created bucket: onebrc\") } // Generate data log.Printf(\"Generating %d measurements...\\n\", *count) var buf bytes.Buffer for i := 0; i \u003c *count; i++ { city := cities[rand.Intn(len(cities))] temp := -20.0 + rand.Float64()*70.0 // -20 to 50°C buf.WriteString(fmt.Sprintf(\"%s;%.1f\\n\", city, temp)) } // Upload to MinIO data := buf.Bytes() _, err = mc.PutObject(ctx, \"onebrc\", \"measurements.txt\", bytes.NewReader(data), int64(len(data)), minio.PutObjectOptions{}) if err != nil { log.Fatal(err) } log.Printf(\"Uploaded %d bytes to onebrc/measurements.txt\\n\", len(data)) } Run the Refactored Job # Make sure MinIO is running podman ps # Generate test data go run tool/main.go -count 10000 # Ensure dependencies are installed go mod tidy # Run the job go run . You should see:\n{\"time\":\"...\",\"level\":\"INFO\",\"msg\":\"starting 1BRC processing\",\"bucket\":\"onebrc\",\"input_key\":\"measurements.txt\",\"output_key\":\"results.txt\"} {\"time\":\"...\",\"level\":\"INFO\",\"msg\":\"1BRC processing completed successfully\",\"cities_processed\":15} Verify Results in MinIO Console Open http://localhost:9001 Login with minioadmin/minioadmin Browse the onebrc bucket You should see both files: measurements.txt (input) results.txt (output) Download or preview results.txt Expected format:\nBeijing=-19.5/16.3/49.8 Cairo=-18.2/17.9/48.5 Delhi=-17.9/15.8/47.3 ... What We Refactored Before (local files):\nDirect os.Open and os.WriteFile File paths in configuration Simple and fast for development After (cloud storage):\nStorage interface with GetObject/PutObject Bucket and key configuration Production-ready architecture Same core business logic! Benefits of This Refactoring Testability:\nCan mock the Storage interface Unit tests don’t need MinIO running Easier to test error paths Flexibility:\nWorks with any S3-compatible service Easy to swap implementations Can add caching, retries, etc. Production-ready:\nScalable storage Cloud-native architecture Separation of concerns What’s Next Now let’s add the full LGTM observability stack so you can see traces, metrics, and logs in Grafana.\nNext: Infrastructure Setup →\n","categories":"","description":"Replace local file I/O with S3-compatible cloud storage","excerpt":"Replace local file I/O with S3-compatible cloud storage","ref":"/humus/pr-preview/pr-396/walkthroughs/1brc-job/05-minio-integration/","tags":"","title":"Refactoring to MinIO Storage"},{"body":"Your job is working with MinIO! Now let’s add the full observability stack to see traces, logs, and metrics in Grafana.\nUnderstanding the LGTM Stack The LGTM stack provides comprehensive observability:\nLoki - Log aggregation and querying Grafana - Unified visualization dashboard Tempo - Distributed tracing backend Mimir - Long-term metrics storage We’ll also add the OpenTelemetry Collector to receive and route telemetry data from your job.\nArchitecture Overview ┌─────────────┐ │ Your Job │ ──OTLP──▶ ┌──────────────────┐ └─────────────┘ │ OTel Collector │ └──────────────────┘ │ ┌──────────────┼──────────────┐ ▼ ▼ ▼ ┌─────────┐ ┌─────────┐ ┌─────────┐ │ Tempo │ │ Mimir │ │ Loki │ │ (traces)│ │(metrics)│ │ (logs) │ └─────────┘ └─────────┘ └─────────┘ │ │ │ └──────────────┼──────────────┘ ▼ ┌──────────────┐ │ Grafana │ │ (visualize) │ └──────────────┘ ┌─────────────┐ │ MinIO │ ◀───────── Data storage (input/output) └─────────────┘ Updating Podman Compose Update your podman-compose.yaml to add the observability stack (keeping MinIO):\nservices: minio: image: docker.io/minio/minio:latest command: server /data --console-address \":9001\" environment: - MINIO_ROOT_USER=minioadmin - MINIO_ROOT_PASSWORD=minioadmin ports: - \"9000:9000\" - \"9001:9001\" volumes: - minio-data:/data:z tempo: image: docker.io/grafana/tempo:latest command: [\"-config.file=/etc/tempo.yaml\"] volumes: - ./tempo-config.yaml:/etc/tempo.yaml:ro,z ports: - \"3200:3200\" loki: image: docker.io/grafana/loki:latest ports: - \"3100:3100\" command: -config.file=/etc/loki/local-config.yaml mimir: image: docker.io/grafana/mimir:latest command: [\"-config.file=/etc/mimir.yaml\"] volumes: - ./mimir-config.yaml:/etc/mimir.yaml:ro,z ports: - \"9009:9009\" otel-collector: image: docker.io/otel/opentelemetry-collector-contrib:latest command: [\"--config=/etc/otel-collector-config.yaml\"] volumes: - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro,z ports: - \"4317:4317\" depends_on: - tempo - loki - mimir grafana: image: docker.io/grafana/grafana:latest environment: - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin - GF_AUTH_DISABLE_LOGIN_FORM=true ports: - \"3000:3000\" volumes: - ./grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml:ro,z depends_on: - tempo - loki - mimir volumes: minio-data: Required Configuration Files You’ll need these configuration files in your project directory. These are available in the example directory at example/job/1brc-walkthrough/.\ntempo.yaml Configures Tempo to accept OTLP traces:\nserver: http_listen_port: 3200 distributor: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 storage: trace: backend: local local: path: /tmp/tempo/blocks mimir.yaml Configures Mimir for metrics storage:\ntarget: all server: http_listen_port: 9009 ingester: ring: replication_factor: 1 blocks_storage: backend: filesystem filesystem: dir: /tmp/mimir/blocks otel-collector-config.yaml Routes telemetry from your job to the backend services:\nreceivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 exporters: otlp/tempo: endpoint: tempo:4317 tls: insecure: true loki: endpoint: http://loki:3100/loki/api/v1/push otlphttp/mimir: endpoint: http://mimir:9009/otlp processors: batch: service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp/tempo] logs: receivers: [otlp] processors: [batch] exporters: [loki] metrics: receivers: [otlp] processors: [batch] exporters: [otlphttp/mimir] grafana-datasources.yaml Configures Grafana to read from all backends:\napiVersion: 1 datasources: - name: Tempo type: tempo access: proxy url: http://tempo:3200 isDefault: true - name: Loki type: loki access: proxy url: http://loki:3100 - name: Mimir type: prometheus access: proxy url: http://mimir:9009/prometheus Starting the Stack Start all services:\npodman-compose up -d First startup takes 1-2 minutes to pull images and initialize.\nCheck status:\npodman-compose ps All 6 services should show Up status.\nService Endpoints Service URL Credentials Grafana http://localhost:3000 None (anonymous admin) MinIO Console http://localhost:9001 minioadmin / minioadmin Tempo http://localhost:3200 N/A (internal) Mimir http://localhost:9009 N/A (internal) Loki http://localhost:3100 N/A (internal) OTel Collector localhost:4317 N/A (OTLP gRPC) Verifying the Setup Check Grafana: Open http://localhost:3000\nYou should see the Grafana welcome page No login required (anonymous access enabled) Verify Data Sources: Go to Configuration → Data Sources\nYou should see Tempo, Loki, and Mimir configured Regenerate test data (since we restarted MinIO):\ncd tool go run . -count 10000 cd .. What’s Next Now let’s add OpenTelemetry instrumentation to your job so it sends traces, metrics, and logs to this stack.\nNext: Adding Observability →\n","categories":"","description":"Adding the LGTM observability stack","excerpt":"Adding the LGTM observability stack","ref":"/humus/pr-preview/pr-396/walkthroughs/1brc-job/06-infrastructure/","tags":"","title":"Infrastructure Setup"},{"body":"Guides for migrating existing applications to Humus and integrating with other systems.\nMigration Guides From Vanilla Go HTTP - Migrate from net/http From chi Router - Migrate from chi-based applications From gRPC-Go - Migrate from vanilla gRPC applications Bedrock Integration - Deep dive into Bedrock framework Why Migrate to Humus? Consistency Standardized patterns across REST, gRPC, and Job services Common configuration format Unified observability approach Built-in Observability Automatic OpenTelemetry integration No manual instrumentation needed Consistent logging with trace correlation Production Ready Graceful shutdown out of the box Health check endpoints Panic recovery Signal handling Developer Experience Type-safe handlers (REST/RPC) Automatic OpenAPI generation Minimal boilerplate Clear separation of concerns Migration Strategy Assess Your Application - Identify service type and dependencies Start Small - Migrate one endpoint or service at a time Test Thoroughly - Ensure behavior matches original Deploy Incrementally - Use feature flags or canary deployments Compatibility Humus is compatible with:\nExisting HTTP Middleware - chi middleware works with REST services gRPC Interceptors - Standard interceptors work alongside Humus interceptors OpenTelemetry Collectors - Any OTLP-compatible backend Configuration Sources - YAML files, environment variables, or custom sources Next Steps Choose a migration guide based on your current stack, or explore Bedrock Integration to understand the foundation.\n","categories":"","description":"Migrating to Humus and integrating with other systems","excerpt":"Migrating to Humus and integrating with other systems","ref":"/humus/pr-preview/pr-396/integration/","tags":"","title":"Migration \u0026 Integration"},{"body":"Now let’s implement the POST endpoint that orchestrates multiple services.\nRequest and Response Types Create endpoint/place_order.go:\npackage endpoint import ( \"context\" \"errors\" \"net/http\" \"github.com/google/uuid\" \"github.com/sourcegraph/conc/pool\" \"github.com/z5labs/humus/example/rest/orders-walkthrough/service\" \"github.com/z5labs/humus/rest\" ) // PlaceOrderRequest is the request body for placing an order. type PlaceOrderRequest struct { CustomerID string `json:\"customer_id\"` AccountID string `json:\"account_id\"` } // PlaceOrderResponse is the response for a successfully placed order. type PlaceOrderResponse struct { OrderID string `json:\"order_id\"` } // ErrAccountRestricted indicates the account has restrictions preventing order placement. var ErrAccountRestricted = errors.New(\"account has restrictions\") // ErrAccountIneligible indicates the account is not eligible to place orders. var ErrAccountIneligible = errors.New(\"account is not eligible\") Note the error variable naming convention: ErrAccountRestricted follows Go’s ErrFoo pattern.\nEndpoint Registration // PlaceOrder creates the POST /v1/order endpoint. func PlaceOrder(restrictionSvc RestrictionService, eligibilitySvc EligibilityService, dataSvc DataService) rest.ApiOption { handler := \u0026placeOrderHandler{ restrictionSvc: restrictionSvc, eligibilitySvc: eligibilitySvc, dataSvc: dataSvc, } return rest.Handle( http.MethodPost, rest.BasePath(\"/v1\").Segment(\"order\"), rest.HandleJson(handler), ) } Key differences from GET:\nUses http.MethodPost Uses rest.HandleJson() which consumes request body AND returns response No query parameters needed Interfaces defined locally (see endpoint/interfaces.go) Concurrent Validation with conc/pool The handler runs validation checks concurrently for optimal performance:\ntype placeOrderHandler struct { restrictionSvc RestrictionService eligibilitySvc EligibilityService dataSvc DataService } func (h *placeOrderHandler) Handle(ctx context.Context, req *PlaceOrderRequest) (*PlaceOrderResponse, error) { // Run validation checks concurrently using conc/pool p := pool.New().WithContext(ctx) // Check restrictions concurrently p.Go(func(ctx context.Context) error { restrictions, err := h.restrictionSvc.CheckRestrictions(ctx, req.AccountID) if err != nil { return err } if len(restrictions) \u003e 0 { return ErrAccountRestricted } return nil }) // Check eligibility concurrently p.Go(func(ctx context.Context) error { eligibility, err := h.eligibilitySvc.CheckEligibility(ctx, req.AccountID) if err != nil { return err } if !eligibility.Eligible { return ErrAccountIneligible } return nil }) // Wait for both checks to complete if err := p.Wait(); err != nil { return nil, err } // Create and store the order orderID := uuid.New().String() order := service.Order{ OrderID: orderID, AccountID: req.AccountID, CustomerID: req.CustomerID, Status: service.OrderStatusPending, } if err := h.dataSvc.PutItem(ctx, order); err != nil { return nil, err } return \u0026PlaceOrderResponse{ OrderID: orderID, }, nil } The handler demonstrates:\nConcurrent validation - Both checks run in parallel using conc/pool Performance optimization - ~50% latency reduction when both services are healthy Fail-fast behavior - p.Wait() returns on first error Panic safety - conc/pool handles panics gracefully Context propagation - Cancellation flows to all goroutines Why conc/pool? The github.com/sourcegraph/conc/pool library provides:\nStructured concurrency - Automatic cleanup and error handling Context integration - Respects cancellation and deadlines Panic recovery - Converts panics to errors instead of crashing Production-tested - Used in Sourcegraph’s infrastructure Registering the Endpoint Update app/app.go to initialize the additional services and register the PlaceOrder endpoint:\n// Initialize services dataSvc := service.NewDataClient(cfg.Services.DataURL, httpClient) restrictionSvc := service.NewRestrictionClient(cfg.Services.RestrictionURL, httpClient) eligibilitySvc := service.NewEligibilityClient(cfg.Services.EligibilityURL, httpClient) // Create API with both endpoints api := rest.NewApi( cfg.OpenApi.Title, cfg.OpenApi.Version, endpoint.ListOrders(dataSvc), endpoint.PlaceOrder(restrictionSvc, eligibilitySvc, dataSvc), ) Changes from the previous step:\nInitialize restrictionSvc and eligibilitySvc (previously unused) Add endpoint.PlaceOrder() to the API registration All three services are now wired to their respective endpoints Testing the Endpoint Test successful order placement:\ncurl -s -X POST http://localhost:8090/v1/order \\ -H \"Content-Type: application/json\" \\ -d '{\"customer_id\":\"CUST-001\",\"account_id\":\"ACC-001\"}' | jq . Response:\n{ \"order_id\": \"649cfc69-8323-4c60-8745-c7071506943d\" } Test with restricted account:\ncurl -s -X POST http://localhost:8090/v1/order \\ -H \"Content-Type: application/json\" \\ -d '{\"customer_id\":\"CUST-001\",\"account_id\":\"ACC-FRAUD\"}' | jq . This will return an error because ACC-FRAUD has fraud restrictions in the Wiremock stub.\nOpenAPI Schema Check the auto-generated OpenAPI schema:\ncurl -s http://localhost:8090/openapi.json | jq '.paths[\"/v1/order\"].post' The framework automatically generates:\nRequest body schema from PlaceOrderRequest struct Response schema from PlaceOrderResponse struct Proper content types What’s Next With both endpoints complete, let’s test them using Wiremock to mock the backend services.\nNext: Basic Testing →\n","categories":"","description":"Implement POST /v1/order with service orchestration","excerpt":"Implement POST /v1/order with service orchestration","ref":"/humus/pr-preview/pr-396/walkthroughs/orders-rest/place-order-endpoint/","tags":"","title":"Place Order Endpoint"},{"body":"Now that your job works and the observability stack is running, let’s add OpenTelemetry instrumentation.\nUpdate Configuration First, update config.yaml to add OTel settings:\n# OpenTelemetry configuration otel: resource: service_name: 1brc-job-walkthrough trace: exporter: type: {{env \"OTEL_TRACE_EXPORTER\" | default \"otlp\"}} otlp: type: grpc target: {{env \"OTEL_OTLP_TARGET\" | default \"localhost:4317\"}} metric: exporter: type: {{env \"OTEL_METRIC_EXPORTER\" | default \"otlp\"}} otlp: type: grpc target: {{env \"OTEL_OTLP_TARGET\" | default \"localhost:4317\"}} log: exporter: type: {{env \"OTEL_LOG_EXPORTER\" | default \"otlp\"}} otlp: type: grpc target: {{env \"OTEL_OTLP_TARGET\" | default \"localhost:4317\"}} # MinIO configuration (unchanged) minio: endpoint: \"{{env \"MINIO_ENDPOINT\" | default \"localhost:9000\"}}\" access_key: \"{{env \"MINIO_ACCESS_KEY\" | default \"minioadmin\"}}\" secret_key: \"{{env \"MINIO_SECRET_KEY\" | default \"minioadmin\"}}\" bucket: \"{{env \"MINIO_BUCKET\" | default \"onebrc\"}}\" # 1BRC configuration (unchanged) onebrc: input_key: \"{{env \"ONEBRC_INPUT_KEY\" | default \"measurements.txt\"}}\" output_key: \"{{env \"ONEBRC_OUTPUT_KEY\" | default \"results.txt\"}}\" Update Config Struct Update app/app.go to embed job.Config:\npackage app import ( \"context\" \"1brc-walkthrough/onebrc\" \"1brc-walkthrough/service\" \"github.com/z5labs/humus/job\" ) type Config struct { job.Config `config:\",squash\"` // Add this line Minio struct { Endpoint string `config:\"endpoint\"` AccessKey string `config:\"access_key\"` SecretKey string `config:\"secret_key\"` Bucket string `config:\"bucket\"` } `config:\"minio\"` OneBRC struct { InputKey string `config:\"input_key\"` OutputKey string `config:\"output_key\"` } `config:\"onebrc\"` } // Init function remains the same func Init(ctx context.Context, cfg Config) (*job.App, error) { minioClient, err := service.NewMinIOClient( cfg.Minio.Endpoint, cfg.Minio.AccessKey, cfg.Minio.SecretKey, ) if err != nil { return nil, err } handler := onebrc.NewHandler( minioClient, cfg.Minio.Bucket, cfg.OneBRC.InputKey, cfg.OneBRC.OutputKey, ) return job.NewApp(handler), nil } The `config:\",squash\"` tag flattens the job.Config fields into your config YAML.\nAdd Instrumentation to Handler Now update onebrc/handler.go to add traces, metrics, and logs:\npackage onebrc import ( \"bufio\" \"bytes\" \"context\" \"fmt\" \"io\" \"log/slog\" \"github.com/z5labs/humus\" \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/attribute\" \"go.opentelemetry.io/otel/metric\" ) type Storage interface { GetObject(ctx context.Context, bucket, key string) (io.ReadCloser, error) PutObject(ctx context.Context, bucket, key string, reader io.Reader, size int64) error } type Handler struct { storage Storage bucket string inputKey string outputKey string log *slog.Logger tracer func(context.Context, string) (context.Context, func()) meter metric.Meter } func NewHandler(storage Storage, bucket, inputKey, outputKey string) *Handler { tracer := otel.Tracer(\"onebrc\") meter := otel.Meter(\"onebrc\") return \u0026Handler{ storage: storage, bucket: bucket, inputKey: inputKey, outputKey: outputKey, log: humus.Logger(\"onebrc\"), tracer: func(ctx context.Context, name string) (context.Context, func()) { ctx, span := tracer.Start(ctx, name) return ctx, func() { span.End() } }, meter: meter, } } func (h *Handler) Handle(ctx context.Context) error { ctx, end := h.tracer(ctx, \"handle\") defer end() h.log.InfoContext(ctx, \"starting 1BRC processing\", slog.String(\"bucket\", h.bucket), slog.String(\"input_key\", h.inputKey), slog.String(\"output_key\", h.outputKey), ) // 1. Fetch from S3 rc, err := h.storage.GetObject(ctx, h.bucket, h.inputKey) if err != nil { h.log.ErrorContext(ctx, \"failed to fetch input object\", slog.Any(\"error\", err)) return fmt.Errorf(\"get object: %w\", err) } defer func() { if cerr := rc.Close(); cerr != nil { h.log.WarnContext(ctx, \"failed to close input object\", slog.Any(\"error\", cerr)) } }() // 2. Parse (with span) parseCtx, parseEnd := h.tracer(ctx, \"parse\") cityStats, err := Parse(bufio.NewReader(rc)) parseEnd() if err != nil { h.log.ErrorContext(parseCtx, \"failed to parse temperature data\", slog.Any(\"error\", err)) return fmt.Errorf(\"parse: %w\", err) } // 3. Record metric counter, err := h.meter.Int64Counter(\"onebrc.cities.count\") if err != nil { h.log.ErrorContext(ctx, \"failed to create counter\", slog.Any(\"error\", err)) return fmt.Errorf(\"create counter: %w\", err) } counter.Add(ctx, int64(len(cityStats)), metric.WithAttributes(attribute.String(\"bucket\", h.bucket))) // 4. Calculate (with span) _, calcEnd := h.tracer(ctx, \"calculate\") results := Calculate(cityStats) calcEnd() // 5. Write results (with span) writeCtx, writeEnd := h.tracer(ctx, \"write_results\") output := FormatResults(results) outputBytes := []byte(output) err = h.storage.PutObject(writeCtx, h.bucket, h.outputKey, bytes.NewReader(outputBytes), int64(len(outputBytes))) writeEnd() if err != nil { h.log.ErrorContext(writeCtx, \"failed to upload results\", slog.Any(\"error\", err)) return fmt.Errorf(\"put object: %w\", err) } h.log.InfoContext(ctx, \"1BRC processing completed successfully\", slog.Int(\"cities_processed\", len(cityStats)), ) return nil } Key Changes Explained Configuration:\nAdded job.Config embedding with squash tag Added full OTel configuration section Humus automatically initializes the OTel SDK using these settings Handler Instrumentation:\nLogger: humus.Logger(\"onebrc\") gives you a structured logger integrated with OTel Tracer: otel.Tracer(\"onebrc\") creates trace spans Meter: otel.Meter(\"onebrc\") records metrics Child Spans: Create spans for parse, calculate, and write operations Structured Logs: Use InfoContext and ErrorContext for trace correlation Observability Patterns Spans:\nParent span handle encompasses all work Child spans for each major operation (parse, calculate, write) Call span.End() immediately after work completes Metrics:\nCreate instruments with descriptive names (onebrc.cities.count) Record with context for trace correlation Add attributes for dimensions (bucket name) Logs:\nUse InfoContext/ErrorContext for automatic trace ID injection Add structured fields with slog.String, slog.Int, etc. Log at operation boundaries (start, end, errors) What’s Next Now let’s run the job and view its telemetry in Grafana!\nNext: Running and Monitoring →\n","categories":"","description":"Retrofitting your job with traces, metrics, and logs","excerpt":"Retrofitting your job with traces, metrics, and logs","ref":"/humus/pr-preview/pr-396/walkthroughs/1brc-job/07-observability/","tags":"","title":"Adding Observability"},{"body":"Great! You’ve implemented both API endpoints. Now let’s test them with a real backend service using Wiremock.\nCreating Wiremock Stubs Wiremock will mock all three backend services (data, restriction, and eligibility) using JSON stub files.\nCreate the directory structure:\ncd example/rest/orders-walkthrough mkdir -p wiremock/mappings Data Service Stubs Create wiremock/mappings/data-service.json:\n{ \"mappings\": [ { \"request\": { \"method\": \"GET\", \"urlPathPattern\": \"/data/orders\", \"queryParameters\": { \"accountNumber\": { \"equalTo\": \"ACC-001\" } } }, \"response\": { \"status\": 200, \"headers\": { \"Content-Type\": \"application/json\" }, \"jsonBody\": { \"orders\": [ { \"order_id\": \"order-001\", \"account_id\": \"ACC-001\", \"customer_id\": \"CUST-001\", \"status\": \"completed\" }, { \"order_id\": \"order-002\", \"account_id\": \"ACC-001\", \"customer_id\": \"CUST-001\", \"status\": \"pending\" } ], \"page_info\": { \"has_next_page\": true, \"end_cursor\": \"b3JkZXItMDAy\" } } } }, { \"request\": { \"method\": \"GET\", \"urlPathPattern\": \"/data/orders\", \"queryParameters\": { \"accountNumber\": { \"equalTo\": \"ACC-001\" }, \"after\": { \"equalTo\": \"b3JkZXItMDAy\" } } }, \"response\": { \"status\": 200, \"headers\": { \"Content-Type\": \"application/json\" }, \"jsonBody\": { \"orders\": [ { \"order_id\": \"order-003\", \"account_id\": \"ACC-001\", \"customer_id\": \"CUST-001\", \"status\": \"completed\" } ], \"page_info\": { \"has_next_page\": false, \"end_cursor\": \"\" } } } }, { \"request\": { \"method\": \"GET\", \"urlPathPattern\": \"/data/orders\", \"queryParameters\": { \"accountNumber\": { \"equalTo\": \"ACC-001\" }, \"status\": { \"equalTo\": \"completed\" } } }, \"response\": { \"status\": 200, \"headers\": { \"Content-Type\": \"application/json\" }, \"jsonBody\": { \"orders\": [ { \"order_id\": \"order-001\", \"account_id\": \"ACC-001\", \"customer_id\": \"CUST-001\", \"status\": \"completed\" } ], \"page_info\": { \"has_next_page\": false, \"end_cursor\": \"\" } } } }, { \"request\": { \"method\": \"POST\", \"urlPathPattern\": \"/data/order\" }, \"response\": { \"status\": 201, \"headers\": { \"Content-Type\": \"application/json\" }, \"jsonBody\": { \"order_id\": \"649cfc69-8323-4c60-8745-c7071506943d\" } } } ] } Restriction Service Stubs Create wiremock/mappings/restriction-service.json:\n{ \"mappings\": [ { \"request\": { \"method\": \"GET\", \"urlPathPattern\": \"/restrictions\", \"queryParameters\": { \"accountId\": { \"equalTo\": \"ACC-001\" } } }, \"response\": { \"status\": 200, \"headers\": { \"Content-Type\": \"application/json\" }, \"jsonBody\": { \"restrictions\": [] } } }, { \"request\": { \"method\": \"GET\", \"urlPathPattern\": \"/restrictions\", \"queryParameters\": { \"accountId\": { \"equalTo\": \"ACC-FRAUD\" } } }, \"response\": { \"status\": 200, \"headers\": { \"Content-Type\": \"application/json\" }, \"jsonBody\": { \"restrictions\": [\"FRAUD\"] } } }, { \"request\": { \"method\": \"GET\", \"urlPathPattern\": \"/restrictions\", \"queryParameters\": { \"accountId\": { \"equalTo\": \"ACC-INELIGIBLE\" } } }, \"response\": { \"status\": 200, \"headers\": { \"Content-Type\": \"application/json\" }, \"jsonBody\": { \"restrictions\": [] } } } ] } Eligibility Service Stubs Create wiremock/mappings/eligibility-service.json:\n{ \"mappings\": [ { \"request\": { \"method\": \"GET\", \"urlPathPattern\": \"/eligibility\", \"queryParameters\": { \"accountId\": { \"equalTo\": \"ACC-001\" } } }, \"response\": { \"status\": 200, \"headers\": { \"Content-Type\": \"application/json\" }, \"jsonBody\": { \"eligible\": true } } }, { \"request\": { \"method\": \"GET\", \"urlPathPattern\": \"/eligibility\", \"queryParameters\": { \"accountId\": { \"equalTo\": \"ACC-FRAUD\" } } }, \"response\": { \"status\": 200, \"headers\": { \"Content-Type\": \"application/json\" }, \"jsonBody\": { \"eligible\": true } } }, { \"request\": { \"method\": \"GET\", \"urlPathPattern\": \"/eligibility\", \"queryParameters\": { \"accountId\": { \"equalTo\": \"ACC-INELIGIBLE\" } } }, \"response\": { \"status\": 200, \"headers\": { \"Content-Type\": \"application/json\" }, \"jsonBody\": { \"eligible\": false } } } ] } Starting Wiremock Now that the stubs are configured, start Wiremock with the following setup.\nCreate a minimal podman-compose.yaml (or use the existing one):\nservices: wiremock: image: docker.io/wiremock/wiremock:3.10.0 ports: - \"8080:8080\" volumes: - ./wiremock:/home/wiremock:z command: --verbose The wiremock/ directory contains JSON stub files that define mock responses for all backend service endpoints.\nStart Wiremock:\ncd example/rest/orders-walkthrough podman-compose up -d Verify it’s running:\npodman ps You should see the wiremock container running on port 8080.\nStarting the API In a separate terminal:\ncd example/rest/orders-walkthrough go run . The API starts on port 8090 and connects to Wiremock at http://localhost:8080.\nTesting GET /v1/orders List orders for an account:\ncurl -s \"http://localhost:8090/v1/orders?accountNumber=ACC-001\" | jq . Expected response:\n{ \"orders\": [ { \"order_id\": \"order-001\", \"account_id\": \"ACC-001\", \"customer_id\": \"CUST-001\", \"status\": \"completed\" }, { \"order_id\": \"order-002\", \"account_id\": \"ACC-001\", \"customer_id\": \"CUST-001\", \"status\": \"pending\" } ], \"page_info\": { \"has_next_page\": true, \"end_cursor\": \"b3JkZXItMDAy\" } } Test pagination using the cursor:\nCURSOR=$(curl -s \"http://localhost:8090/v1/orders?accountNumber=ACC-001\" | jq -r '.page_info.end_cursor') curl -s \"http://localhost:8090/v1/orders?accountNumber=ACC-001\u0026after=$CURSOR\" | jq . Filter by status:\ncurl -s \"http://localhost:8090/v1/orders?accountNumber=ACC-001\u0026status=completed\" | jq . Testing POST /v1/order Successful Order Placement curl -s -X POST http://localhost:8090/v1/order \\ -H \"Content-Type: application/json\" \\ -d '{\"customer_id\":\"CUST-001\",\"account_id\":\"ACC-001\"}' | jq . Expected response:\n{ \"order_id\": \"649cfc69-8323-4c60-8745-c7071506943d\" } Order Blocked by Restrictions Test with an account that has fraud restrictions:\ncurl -s -X POST http://localhost:8090/v1/order \\ -H \"Content-Type: application/json\" \\ -d '{\"customer_id\":\"CUST-001\",\"account_id\":\"ACC-FRAUD\"}' | jq . This returns an error because ACC-FRAUD has fraud restrictions in the Wiremock stub.\nOrder Blocked by Eligibility Test with an ineligible account:\ncurl -s -X POST http://localhost:8090/v1/order \\ -H \"Content-Type: application/json\" \\ -d '{\"customer_id\":\"CUST-001\",\"account_id\":\"ACC-INELIGIBLE\"}' | jq . This returns an error because ACC-INELIGIBLE is not eligible in the Wiremock stub.\nVerifying OpenAPI Schema Check the auto-generated OpenAPI specification:\ncurl -s http://localhost:8090/openapi.json | jq '.paths | keys' Expected output:\n[ \"/v1/order\", \"/v1/orders\" ] View the POST endpoint schema:\ncurl -s http://localhost:8090/openapi.json | jq '.paths[\"/v1/order\"].post' Health Checks Test the built-in health endpoints:\n# Liveness probe curl -s http://localhost:8090/health/liveness # Readiness probe curl -s http://localhost:8090/health/readiness Quick Validation Checklist Wiremock container is running API starts without errors GET /v1/orders returns paginated results Pagination cursor works correctly Status filtering works POST /v1/order creates orders successfully Restrictions block orders correctly Eligibility checks block ineligible accounts OpenAPI spec includes both endpoints Health checks respond Stopping Services When you’re done testing:\n# Stop the API (Ctrl+C in terminal) # Stop Wiremock podman-compose down What’s Next Your API is working! Now let’s add the full observability stack (Grafana, Tempo, Loki, Mimir) to see traces, logs, and metrics.\nNext: Infrastructure Setup →\n","categories":"","description":"Test your API endpoints with Wiremock","excerpt":"Test your API endpoints with Wiremock","ref":"/humus/pr-preview/pr-396/walkthroughs/orders-rest/basic-testing/","tags":"","title":"Basic Testing"},{"body":"Common questions, issues, and solutions for Humus applications.\nSections Frequently Asked Questions - Common questions about Humus Troubleshooting - Common issues and solutions Best Practices - Recommended patterns Quick Answers General Q: What’s the difference between Humus and Bedrock?\nA: Bedrock is the foundation framework providing application lifecycle management. Humus builds on Bedrock to provide service-specific patterns for REST, gRPC, and Job services.\nQ: Do I need to know Bedrock to use Humus?\nA: No. Humus abstracts Bedrock’s complexity. You only need to know Humus APIs for most use cases.\nQ: Can I use Humus with existing Go code?\nA: Yes. Humus is compatible with standard Go HTTP middleware, gRPC interceptors, and other Go libraries.\nREST Services Q: Can I use Humus with my existing chi router?\nA: Humus uses chi internally, so chi middleware is compatible. However, you should use Humus’s routing APIs for full OpenAPI support.\nQ: How do I add custom middleware?\nA: See Advanced Topics - Middleware.\nQ: Can I disable OpenAPI generation?\nA: Currently, OpenAPI generation is always enabled. It adds minimal overhead.\ngRPC Services Q: Can I use Protocol Buffers v2?\nA: Humus works with both proto2 and proto3. We recommend proto3 for new projects.\nQ: How do I add custom interceptors?\nA: See Advanced Topics - Middleware.\nConfiguration Q: Can I use environment variables directly without YAML?\nA: Currently, you need a YAML file, but it can reference environment variables using templates. You can also implement a custom config source.\nQ: How do I handle secrets?\nA: Use environment variables in your YAML templates: {{env \"SECRET_KEY\"}}. Never commit secrets to YAML files.\nObservability Q: Can I use Humus without OpenTelemetry?\nA: Yes. Set otel.sdk.disabled: true in your configuration.\nQ: How do I send telemetry to multiple backends?\nA: Use an OpenTelemetry Collector to fan out to multiple backends.\nQ: Can I use Prometheus metrics?\nA: Yes. Use the OpenTelemetry Prometheus exporter or collector.\nStill Have Questions? Check the Documentation Visit GitHub Discussions Review Troubleshooting Guide See Best Practices ","categories":"","description":"Common questions and solutions","excerpt":"Common questions and solutions","ref":"/humus/pr-preview/pr-396/faq/","tags":"","title":"FAQ \u0026 Troubleshooting"},{"body":"Thank you for your interest in contributing to Humus! This guide will help you get started.\nWays to Contribute Report Bugs - File issues on GitHub Suggest Features - Start a discussion Improve Documentation - Fix typos, add examples, clarify explanations Write Code - Fix bugs or implement features Share Examples - Contribute example applications Getting Started Fork the Repository - https://github.com/z5labs/humus Clone Your Fork git clone https://github.com/YOUR-USERNAME/humus.git cd humus Set Up Development Environment - See Development Setup Create a Branch git checkout -b feature/my-feature Development Workflow Running Tests # Run all tests with race detection and coverage go test -race -cover ./... # Run tests for a specific package go test -race -cover ./rest # Run a specific test go test -race -run TestName ./path/to/package Linting # Run golangci-lint golangci-lint run # Auto-fix issues where possible golangci-lint run --fix Building # Build all packages go build ./... # Verify no build errors go vet ./... Code Guidelines Style Follow standard Go conventions Use gofmt for formatting Run golangci-lint before committing Write clear, descriptive commit messages Testing Write tests for new features Maintain or improve code coverage Use table-driven tests where appropriate Mock external dependencies Documentation Document exported types and functions Include examples in godoc Update relevant documentation pages Add package-level documentation Pull Request Process Ensure Tests Pass\ngo test -race -cover ./... Ensure Linting Passes\ngolangci-lint run Update Documentation - If you’re adding features or changing behavior\nWrite a Clear PR Description\nWhat does this PR do? Why is this change needed? How was it tested? Link Related Issues - Use “Fixes #123” or “Relates to #456”\nBe Responsive - Address review comments promptly\nCode of Conduct Be respectful and inclusive Welcome newcomers Focus on constructive feedback Assume good intentions Questions? Development Questions - See Development Setup Testing Questions - See Testing Guide Documentation - See Documentation Guide General Questions - Visit GitHub Discussions Resources GitHub Repository Issue Tracker Discussions Development Setup Guide Testing Guide ","categories":"","description":"How to contribute to Humus","excerpt":"How to contribute to Humus","ref":"/humus/pr-preview/pr-396/contributing/","tags":"","title":"Contributing"},{"body":"Your API is working with Wiremock. Now let’s add the full observability stack to see traces, logs, and metrics in Grafana.\nUnderstanding the LGTM Stack The LGTM stack provides comprehensive observability:\nLoki - Log aggregation and querying Grafana - Unified visualization dashboard Tempo - Distributed tracing backend Mimir - Long-term metrics storage We’ll also add the OpenTelemetry Collector to receive and route telemetry data from your API.\nUpdating Podman Compose Update your podman-compose.yaml to add the observability stack:\nservices: wiremock: image: docker.io/wiremock/wiremock:3.10.0 ports: - \"8080:8080\" volumes: - ./wiremock:/home/wiremock:z command: --verbose tempo: image: docker.io/grafana/tempo:2.6.1 command: [\"-config.file=/etc/tempo.yaml\"] volumes: - ./tempo.yaml:/etc/tempo.yaml:ro,z ports: - \"3200:3200\" loki: image: docker.io/grafana/loki:3.3.2 ports: - \"3100:3100\" command: -config.file=/etc/loki/local-config.yaml mimir: image: docker.io/grafana/mimir:2.14.2 command: [\"-config.file=/etc/mimir.yaml\"] volumes: - ./mimir.yaml:/etc/mimir.yaml:ro,z ports: - \"9009:9009\" otel-collector: image: docker.io/otel/opentelemetry-collector-contrib:0.115.1 command: [\"--config=/etc/otel-collector-config.yaml\"] volumes: - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro,z ports: - \"4317:4317\" depends_on: - tempo - loki - mimir grafana: image: docker.io/grafana/grafana:11.4.0 environment: - GF_AUTH_ANONYMOUS_ENABLED=true - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin - GF_AUTH_DISABLE_LOGIN_FORM=true ports: - \"3000:3000\" volumes: - ./grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml:ro,z depends_on: - tempo - loki - mimir Required Configuration Files You’ll need these configuration files in your project directory. These are available in the example directory at example/rest/orders-walkthrough/:\ntempo.yaml Configures Tempo to accept OTLP traces:\nserver: http_listen_port: 3200 distributor: receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 storage: trace: backend: local local: path: /tmp/tempo/blocks mimir.yaml Configures Mimir for metrics storage:\ntarget: all server: http_listen_port: 9009 ingester: ring: replication_factor: 1 blocks_storage: backend: filesystem filesystem: dir: /tmp/mimir/blocks otel-collector-config.yaml Routes telemetry from your API to the backend services:\nreceivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 exporters: otlp/tempo: endpoint: tempo:4317 tls: insecure: true loki: endpoint: http://loki:3100/loki/api/v1/push otlphttp/mimir: endpoint: http://mimir:9009/otlp processors: batch: service: pipelines: traces: receivers: [otlp] processors: [batch] exporters: [otlp/tempo] logs: receivers: [otlp] processors: [batch] exporters: [loki] metrics: receivers: [otlp] processors: [batch] exporters: [otlphttp/mimir] grafana-datasources.yaml Configures Grafana to read from all backends:\napiVersion: 1 datasources: - name: Tempo type: tempo access: proxy url: http://tempo:3200 isDefault: true - name: Loki type: loki access: proxy url: http://loki:3100 - name: Mimir type: prometheus access: proxy url: http://mimir:9009/prometheus Restarting the Stack If Wiremock is still running from the previous step:\npodman-compose down Start the full stack:\npodman-compose up -d Verify all 6 containers are running:\npodman ps --format \"table {{.Names}}\\t{{.Status}}\" Expected output:\nNAMES STATUS orders-walkthrough-wiremock Up orders-walkthrough-tempo Up orders-walkthrough-loki Up orders-walkthrough-mimir Up orders-walkthrough-otel-collector Up orders-walkthrough-grafana Up Service Endpoints Wiremock: http://localhost:8080 - Mock backend services Grafana: http://localhost:3000 - Observability dashboard OTel Collector: localhost:4317 - OTLP gRPC receiver Tempo: http://localhost:3200 - Trace API Loki: http://localhost:3100 - Log API Mimir: http://localhost:9009 - Metrics API Verifying the Setup Check Grafana: Open http://localhost:3000\nYou should see the Grafana welcome page No login required (anonymous access enabled) Verify Data Sources: Go to Configuration → Data Sources\nYou should see Tempo, Loki, and Mimir configured Restart Your API: The API needs to restart to connect to the OTel Collector:\n# Stop the API (Ctrl+C) go run . The API will now send traces, logs, and metrics to the OTel Collector.\nWhat’s Next Now let’s run comprehensive end-to-end tests and explore the observability features to see your API’s telemetry data in Grafana.\nNext: Comprehensive Testing →\n","categories":"","description":"Add LGTM stack and OpenTelemetry Collector","excerpt":"Add LGTM stack and OpenTelemetry Collector","ref":"/humus/pr-preview/pr-396/walkthroughs/orders-rest/infrastructure/","tags":"","title":"Observability Infrastructure"},{"body":"Now let’s run your instrumented job and explore the telemetry in Grafana!\nRun the Instrumented Job Make sure the infrastructure is still running:\npodman-compose ps All 6 services should be Up. Now run your job:\ngo run . You should now see structured logs with trace IDs:\n{\"time\":\"...\",\"level\":\"INFO\",\"msg\":\"starting 1BRC processing\",\"trace_id\":\"abc123...\",\"bucket\":\"onebrc\",\"input_key\":\"measurements.txt\"} {\"time\":\"...\",\"level\":\"INFO\",\"msg\":\"1BRC processing completed successfully\",\"trace_id\":\"abc123...\",\"cities_processed\":15} Notice the trace_id field - this links logs to traces!\nView Results in MinIO Open http://localhost:9001 Login with minioadmin/minioadmin Browse the onebrc bucket Download results.txt Expected format:\nAbha=-23.0/18.0/59.2 Beijing=-19.5/16.3/49.8 Cairo=-18.2/17.9/48.5 ... Explore Telemetry in Grafana Accessing Grafana Open http://localhost:3000 (anonymous admin access, no login required)\nView Traces Click Explore (compass icon in left sidebar) Select Tempo datasource (dropdown at top) Click Search tab Filter by: Service Name: 1brc-job-walkthrough Span Name: handle Click Run Query Click on a trace to see the waterfall view What you’ll see:\nhandle (2.5s total) ├── parse (2.3s) ├── calculate (0.15s) └── write_results (0.05s) This shows exactly where time is spent:\nMost time in parsing (reading/aggregating data) Quick calculation (sorting and formatting) Fast write (uploading to S3) View Metrics Explore → Select Mimir datasource In the query builder, enter: onebrc_cities_count Note: The metric is defined as onebrc.cities.count but exporters convert dots to underscores Click Run Query You should see a datapoint showing how many cities were processed (e.g., 15 for the test dataset).\nAdd dimensions:\nonebrc_cities_count{bucket=\"onebrc\"} This shows the metric filtered by bucket name.\nView Logs Explore → Select Loki datasource Query: {service_name=\"1brc-job-walkthrough\"} See all structured logs from your job Filter by log level:\n{service_name=\"1brc-job-walkthrough\"} | json | level=\"INFO\" Search for specific messages:\n{service_name=\"1brc-job-walkthrough\"} |= \"completed successfully\" Trace-to-Logs Correlation This is the killer feature of integrated observability:\nIn a trace span, click the Logs tab See all logs emitted during that span Automatically filtered by trace ID! Or go the other way:\nIn Loki logs, find a log line Click the Tempo button next to the trace_id field Jump directly to the correlated trace! This bi-directional linking makes debugging much easier - you can see the full context around any log message.\nRun With Larger Dataset Try processing more data:\ncd tool go run . -count 1000000 # 1 million measurements cd .. go run . Now check the traces again:\nParse time will be significantly longer You can see the performance impact clearly Metrics show the larger city count Understanding Span Duration The trace waterfall shows:\nTotal duration: End-to-end job execution Parse time: Reading and aggregating line-by-line Calculate time: Computing statistics and sorting cities alphabetically Write time: Uploading formatted results to S3 Performance Analysis Use traces to identify bottlenecks:\nIs parsing slow?\nConsider optimizing the parsing loop Try concurrent parsing with goroutines Use faster parsing libraries Is writing slow?\nCheck network latency to MinIO Consider compression Use buffered writes Are spans missing?\nAdd more instrumentation points Instrument the parser at a finer granularity Add spans for bucket operations Clean Up When you’re done exploring:\npodman-compose down # Or remove all data including volumes podman-compose down -v What You Learned ✅ Building a production job with Humus ✅ Starting with minimal setup before adding observability ✅ Config embedding and YAML templates ✅ Streaming large files with MinIO ✅ Retrofitting OpenTelemetry traces, metrics, logs ✅ Viewing distributed traces in Grafana ✅ Trace-to-logs correlation for debugging ✅ Using metrics to track business KPIs Key Takeaways Development Flow:\nBuild and test with minimal infrastructure first Verify business logic works correctly Add observability stack later Instrument code progressively Observability Value:\nTraces show where time is spent (performance) Metrics track business outcomes (cities processed) Logs provide detailed context (errors, events) Correlation enables fast debugging ","categories":"","description":"Execute the instrumented job and view telemetry in Grafana","excerpt":"Execute the instrumented job and view telemetry in Grafana","ref":"/humus/pr-preview/pr-396/walkthroughs/1brc-job/08-running-monitoring/","tags":"","title":"Running and Monitoring"},{"body":"Now that you have the full observability stack running, let’s run comprehensive end-to-end testing scenarios to validate the complete implementation and explore the telemetry data.\nPrerequisites Before running these tests, ensure:\nAll containers are running (from the Infrastructure Setup step): podman ps --format \"table {{.Names}}\\t{{.Status}}\" You should see 6 running containers: wiremock, tempo, loki, mimir, otel-collector, and grafana.\nAPI is running with observability enabled: go run . The API should connect to the OTel Collector at localhost:4317.\nTest Scenarios Scenario 1: List Orders with Pagination # First page curl -s \"http://localhost:8090/v1/orders?accountNumber=ACC-001\" | jq . Expected: Orders list with has_next_page: true and end_cursor.\n# Next page using cursor CURSOR=$(curl -s \"http://localhost:8090/v1/orders?accountNumber=ACC-001\" | jq -r '.page_info.end_cursor') curl -s \"http://localhost:8090/v1/orders?accountNumber=ACC-001\u0026after=$CURSOR\" | jq . Scenario 2: Filter by Status curl -s \"http://localhost:8090/v1/orders?accountNumber=ACC-001\u0026status=completed\" | jq . Expected: Only orders with status “completed”.\nScenario 3: Successful Order Placement curl -s -X POST http://localhost:8090/v1/order \\ -H \"Content-Type: application/json\" \\ -d '{\"customer_id\":\"CUST-001\",\"account_id\":\"ACC-001\"}' | jq . Expected:\n{ \"order_id\": \"uuid-here\" } Scenario 4: Order Blocked by Restrictions curl -s -X POST http://localhost:8090/v1/order \\ -H \"Content-Type: application/json\" \\ -d '{\"customer_id\":\"CUST-001\",\"account_id\":\"ACC-FRAUD\"}' Expected: Error (ACC-FRAUD has fraud restrictions).\nScenario 5: Order Blocked by Eligibility curl -s -X POST http://localhost:8090/v1/order \\ -H \"Content-Type: application/json\" \\ -d '{\"customer_id\":\"CUST-001\",\"account_id\":\"ACC-INELIGIBLE\"}' Expected: Error (ACC-INELIGIBLE is ineligible).\nObservability Validation After running the test scenarios above, validate that telemetry is being captured:\nTraces in Tempo Open Grafana: http://localhost:3000 Go to Explore (compass icon) Select Tempo as data source Search for recent traces You should see:\nTraces for each API request Distributed trace spans showing: Main HTTP request span Child spans for service calls (Restriction, Eligibility, Data) Timing information for each span HTTP status codes and attributes Logs in Loki In Grafana, go to Explore Select Loki as data source Query: {job=\"orders-api\"} You should see:\nApplication logs correlated with trace IDs Request/response logs Error logs (for failed scenarios) Metrics in Mimir In Grafana, go to Explore Select Mimir as data source Query examples: http_server_request_duration_seconds_bucket - Request latency distribution http_server_request_total - Total request count by status code You should see:\nHTTP request metrics by endpoint Request duration histograms Error rates Complete Validation Checklist Code \u0026 Build All code builds: go build ./... No lint errors: go vet ./... Infrastructure All 6 containers running: podman ps Wiremock accessible: http://localhost:8080 Grafana accessible: http://localhost:3000 API Functionality API starts without errors Health checks respond: /health/liveness, /health/readiness OpenAPI spec available: /openapi.json Endpoint Testing GET /v1/orders returns paginated results Pagination cursors work correctly Status filtering works POST /v1/order creates orders successfully Restrictions block orders correctly Eligibility blocks ineligible accounts Observability Traces appear in Tempo for all requests Distributed traces show service call hierarchy Logs appear in Loki with trace IDs Metrics appear in Mimir HTTP request metrics show correct counts Latency histograms are populated Cleanup To stop everything:\n# Stop the API (Ctrl+C in terminal) # Stop infrastructure podman-compose down --volumes Congratulations! You’ve built a production-ready REST API with:\nTwo endpoints with different HTTP methods Service orchestration with proper error handling Cursor-based pagination for scalability Full observability with zero manual instrumentation Auto-generated OpenAPI documentation Next Steps Add unit tests for handlers and services Implement custom error responses Add authentication (JWT) Deploy to Kubernetes with OTel Collector sidecar Connect to real backend services instead of Wiremock Back to Walkthrough Overview →\n","categories":"","description":"Complete end-to-end testing scenarios with full observability","excerpt":"Complete end-to-end testing scenarios with full observability","ref":"/humus/pr-preview/pr-396/walkthroughs/orders-rest/comprehensive-testing/","tags":"","title":"Comprehensive Testing"},{"body":"Humus uses a powerful configuration system built on YAML with Go template support and multi-source composition.\nConfiguration Anatomy Basic Structure A Humus configuration file has three main sections:\n# 1. Service Configuration (REST, gRPC, or omitted for Jobs) rest: port: 8080 host: 0.0.0.0 # 2. OpenTelemetry Configuration (optional but recommended) otel: service: name: my-service version: 1.0.0 # 3. Application-Specific Configuration database: host: localhost port: 5432 Config Struct Mapping The YAML maps to a Go struct:\ntype Config struct { // 1. Service config (embedded with squash) rest.Config `config:\",squash\"` // 2. OTel is embedded in rest.Config/grpc.Config // No need to explicitly include it // 3. Application-specific fields Database struct { Host string `config:\"host\"` Port int `config:\"port\"` } `config:\"database\"` } Template Engine Template Functions Humus supports Go template syntax with two key functions:\nenv - Read Environment Variables otel: service: name: {{env \"SERVICE_NAME\"}} Reads the SERVICE_NAME environment variable.\ndefault - Provide Fallback Values rest: port: {{env \"PORT\" | default \"8080\"}} Uses PORT environment variable, falling back to \"8080\" if not set.\nTemplate Examples Database Configuration:\ndatabase: host: {{env \"DB_HOST\" | default \"localhost\"}} port: {{env \"DB_PORT\" | default \"5432\"}} name: {{env \"DB_NAME\"}} user: {{env \"DB_USER\"}} password: {{env \"DB_PASSWORD\"}} # No default for secrets! Feature Flags:\nfeatures: enable_cache: {{env \"ENABLE_CACHE\" | default \"true\"}} enable_auth: {{env \"ENABLE_AUTH\" | default \"false\"}} Environment-Specific Values:\notel: sdk: disabled: {{env \"OTEL_DISABLED\" | default \"false\"}} traces: exporter: otlp: endpoint: {{env \"OTEL_ENDPOINT\" | default \"http://localhost:4318\"}} Multi-Source Configuration Compose multiple configuration files with config.MultiSource:\nimport ( bedrockcfg \"github.com/z5labs/bedrock/pkg/config\" \"github.com/z5labs/humus/rest\" ) func main() { source := bedrockcfg.MultiSource( bedrockcfg.FromYaml(\"defaults.yaml\"), // Base configuration bedrockcfg.FromYaml(\"config.yaml\"), // Overrides ) rest.Run(source, Init) } Use Cases for Multi-Source 1. Framework Defaults + App Config source := bedrockcfg.MultiSource( bedrockcfg.FromYaml(\"default_config.yaml\"), // Humus framework defaults bedrockcfg.FromYaml(\"config.yaml\"), // Your application config ) default_config.yaml (framework):\notel: service: name: unnamed-service sdk: disabled: false config.yaml (your app):\notel: service: name: my-actual-service # Overrides framework default 2. Environment-Specific Overrides import \"os\" func main() { env := os.Getenv(\"ENV\") if env == \"\" { env = \"dev\" } sources := []bedrockcfg.Source{ bedrockcfg.FromYaml(\"config.base.yaml\"), } envConfig := fmt.Sprintf(\"config.%s.yaml\", env) if _, err := os.Stat(envConfig); err == nil { sources = append(sources, bedrockcfg.FromYaml(envConfig)) } rest.Run(bedrockcfg.MultiSource(sources...), Init) } config.base.yaml:\nrest: port: 8080 otel: service: name: my-service config.prod.yaml:\nrest: host: 0.0.0.0 # Only override what changes otel: traces: exporter: otlp: endpoint: https://otel-collector.prod.example.com 3. Local Development Overrides source := bedrockcfg.MultiSource( bedrockcfg.FromYaml(\"config.yaml\"), bedrockcfg.FromYaml(\"config.local.yaml\"), // Gitignored local overrides ) Add to .gitignore:\nconfig.local.yaml Developers can create config.local.yaml for personal settings without affecting others.\nStruct Tags The squash Tag Embeds fields directly into the parent:\ntype Config struct { rest.Config `config:\",squash\"` // Fields embedded at root level } Without squash:\nrest_config: # Would need this nesting port: 8080 With squash:\nrest: # Direct access port: 8080 Custom Field Names type Config struct { DatabaseURL string `config:\"database_url\"` // Maps to database_url in YAML APIKey string `config:\"api_key\"` // Maps to api_key } Nested Structures type Config struct { rest.Config `config:\",squash\"` Database struct { Primary struct { Host string `config:\"host\"` Port int `config:\"port\"` } `config:\"primary\"` Replica struct { Host string `config:\"host\"` Port int `config:\"port\"` } `config:\"replica\"` } `config:\"database\"` } Corresponding YAML:\ndatabase: primary: host: primary.db.example.com port: 5432 replica: host: replica.db.example.com port: 5432 OpenTelemetry Configuration Full OTel Config Structure otel: service: name: my-service # Required version: 1.0.0 # Optional namespace: production # Optional instance_id: pod-1234 # Optional sdk: disabled: false # Set true to disable all OTel # Resource attributes (optional) resource: attributes: deployment.environment: production service.team: platform # Trace configuration traces: sampler: type: parentbased_traceidratio # or always_on, always_off, etc. arg: 0.1 # Sample 10% of traces exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf # or grpc headers: x-custom-header: value # Metrics configuration metrics: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf # Logs configuration logs: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf Disabling OTel For development or testing:\notel: sdk: disabled: true Or via environment variable:\notel: sdk: disabled: {{env \"OTEL_DISABLED\" | default \"false\"}} Best Practices 1. Secrets Management Never commit secrets:\n# Bad database: password: super-secret-password # Good database: password: {{env \"DB_PASSWORD\"}} 2. Required vs Optional Use templates for optional values:\nrest: port: {{env \"PORT\" | default \"8080\"}} # Optional, has default Validate required values in Init:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { if cfg.Database.Password == \"\" { return nil, fmt.Errorf(\"DB_PASSWORD environment variable required\") } // ... } 3. Environment Variable Naming Use consistent prefixes:\n# Good database: host: {{env \"MYAPP_DB_HOST\"}} port: {{env \"MYAPP_DB_PORT\"}} # Avoids conflicts with other apps 4. Document Your Config Add comments to YAML files:\n# HTTP Server Configuration rest: # Port to listen on. Set via PORT environment variable. # Default: 8080 port: {{env \"PORT\" | default \"8080\"}} # Host to bind to. Use 0.0.0.0 for all interfaces. # Default: localhost (for security) host: {{env \"HOST\" | default \"localhost\"}} 5. Config Validation Validate in Init function:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Validate ranges if cfg.REST.Port \u003c 1024 || cfg.REST.Port \u003e 65535 { return nil, fmt.Errorf(\"port must be between 1024 and 65535\") } // Validate required fields if cfg.Database.Host == \"\" { return nil, fmt.Errorf(\"database host is required\") } // Validate mutually exclusive options if cfg.Features.UseCache \u0026\u0026 cfg.Features.UseMemory { return nil, fmt.Errorf(\"cannot enable both cache and memory mode\") } // Continue with initialization... } Next Steps Learn about Observability to understand OTel configuration Explore Lifecycle Management for runtime behavior See Getting Started - Configuration for basic examples ","categories":"","description":"Deep dive into config composition and templating","excerpt":"Deep dive into config composition and templating","ref":"/humus/pr-preview/pr-396/concepts/configuration-system/","tags":"","title":"Configuration System"},{"body":"Installing Humus Humus is installed as a Go module dependency. Add it to your project using go get:\ngo get github.com/z5labs/humus This will download Humus and all its dependencies, including:\nBedrock - Core application lifecycle framework OpenTelemetry SDK - For observability Service-specific dependencies (chi router for REST, gRPC for gRPC services, etc.) Verifying Installation Create a simple main.go file to verify the installation:\npackage main import ( \"fmt\" \"github.com/z5labs/humus/rest\" ) func main() { fmt.Println(\"Humus installed successfully!\") } Run it:\ngo run main.go If you see “Humus installed successfully!”, you’re ready to go!\nDependency Management Humus follows semantic versioning. To ensure reproducible builds, use Go modules:\n# Initialize a new module (if not already done) go mod init your-module-name # Install Humus go get github.com/z5labs/humus # Tidy up dependencies go mod tidy Version Pinning To pin to a specific version:\n# Install a specific version go get github.com/z5labs/humus@v0.1.0 # Or use the latest patch release go get github.com/z5labs/humus@latest Service-Specific Dependencies Depending on which service type you’re building, you may need additional tools:\nFor REST Services No additional dependencies required - everything is included with Humus.\nFor gRPC Services You’ll need the Protocol Buffers compiler and Go plugins:\n# Install protoc (see https://grpc.io/docs/protoc-installation/) # On macOS: brew install protobuf # On Linux: apt install -y protobuf-compiler # Install Go plugins go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest For Job Services No additional dependencies required.\nDevelopment Tools While not required, these tools are recommended for development:\n# golangci-lint for code quality go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest # Air for hot reloading during development go install github.com/air-verse/air@latest Next Steps Now that Humus is installed, continue to Your First Service to build your first application.\n","categories":"","description":"Installing Humus and dependencies","excerpt":"Installing Humus and dependencies","ref":"/humus/pr-preview/pr-396/getting-started/installation/","tags":"","title":"Installation"},{"body":"The Humus Kafka runtime provides a production-ready integration with Apache Kafka, offering concurrent per-partition processing, automatic OpenTelemetry instrumentation, and flexible delivery semantics.\nOverview The Kafka runtime is built on:\nfranz-go - Modern, high-performance Kafka client Goroutine-per-partition - Automatic concurrency with partition isolation OpenTelemetry Integration - Built-in tracing via franz-go kotel plugin Delivery Semantics - Both at-most-once and at-least-once processing Quick Start package main import ( \"context\" \"encoding/json\" \"github.com/z5labs/humus/queue\" \"github.com/z5labs/humus/queue/kafka\" ) type OrderMessage struct { OrderID string `json:\"order_id\"` Amount float64 `json:\"amount\"` } type OrderProcessor struct{} func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Deserialize var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Process order (should be idempotent) return nil } func Init(ctx context.Context, cfg Config) (*queue.App, error) { processor := \u0026OrderProcessor{} runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), ) return queue.NewApp(runtime), nil } func main() { queue.Run(queue.YamlSource(\"config.yaml\"), Init) } Core Components Runtime The main Kafka runtime that manages the Kafka client, consumer group, and partition processing:\nruntime := kafka.NewRuntime( brokers []string, groupID string, opts ...Option, ) Features:\nConsumer group management Automatic rebalancing Graceful shutdown OpenTelemetry integration Message Represents a Kafka message with all metadata:\ntype Message struct { Key []byte Value []byte Headers []Header Timestamp time.Time Topic string Partition int32 Offset int64 Attrs uint8 } Your processor receives this type and must deserialize Value into your application’s message format.\nDelivery Semantics Configure processing semantics per topic:\nAt-Least-Once:\nkafka.AtLeastOnce(topic, processor) Messages acknowledged after successful processing. Requires idempotent processors.\nAt-Most-Once:\nkafka.AtMostOnce(topic, processor) Messages acknowledged before processing. Fast but may lose messages on failures.\nConfiguration Options Consumer Group Settings SessionTimeout:\nkafka.SessionTimeout(10 * time.Second) Default: 45 seconds. Maximum time between heartbeats before considered dead.\nRebalanceTimeout:\nkafka.RebalanceTimeout(30 * time.Second) Default: 30 seconds. Maximum time for rebalance operations.\nFetch Settings FetchMaxBytes:\nkafka.FetchMaxBytes(100 * 1024 * 1024) // 100 MB Default: 50 MB. Maximum bytes to fetch across all partitions per request.\nMaxConcurrentFetches:\nkafka.MaxConcurrentFetches(5) Default: unlimited. Limit concurrent fetch requests to Kafka.\nMulti-Topic Processing Process multiple topics in a single runtime:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.AtLeastOnce(\"events\", eventsProcessor), kafka.AtMostOnce(\"metrics\", metricsProcessor), ) Each topic can have different processors and delivery semantics.\nConcurrency Model The Kafka runtime uses a goroutine-per-partition architecture:\nTopic \"orders\" with 3 partitions: ├─ Partition 0 → Goroutine 1 ├─ Partition 1 → Goroutine 2 └─ Partition 2 → Goroutine 3 Benefits:\nNatural parallelism scaling with partition count Partition isolation (one slow partition doesn’t block others) Automatic coordination via consumer group Rebalancing:\nAssigned partitions spawn new goroutines Revoked partitions gracefully shut down Context cancellation coordinates all goroutines See Concurrency Model for details.\nBuilt-in Features Every Kafka runtime automatically includes:\nConsumer Group Management - Automatic partition assignment and rebalancing OpenTelemetry Tracing - Spans per message with context propagation Graceful Shutdown - Clean shutdown on SIGTERM/SIGINT with offset commits Error Handling - Structured logging with message metadata What You’ll Learn This section covers:\nQuick Start - Build your first Kafka processor Runtime Configuration - Advanced configuration options Message Structure - Working with Kafka messages Concurrency Model - Understanding partition processing Idempotency - Handling duplicate messages Multi-Topic Processing - Processing multiple topics Observability - OpenTelemetry integration Configuration - Production deployment Next Steps Start with the Quick Start Guide to build your first Kafka message processor.\n","categories":"","description":"Apache Kafka integration for message processing","excerpt":"Apache Kafka integration for message processing","ref":"/humus/pr-preview/pr-396/features/queue/kafka/","tags":"","title":"Kafka Runtime"},{"body":"The Humus queue framework implements a three-phase message processing pattern that separates concerns for consuming, processing, and acknowledging messages from a queue.\nCore Interfaces The queue framework defines four core interfaces that work together to process messages:\nConsumer Retrieves messages from a queue:\ntype Consumer[T any] interface { Consume(ctx context.Context) (T, error) } Responsibilities:\nFetch the next message from the queue Handle connection management and retries Return ErrEndOfQueue when the queue is exhausted (for graceful shutdown) Example:\ntype KafkaConsumer struct { client *kgo.Client } func (c *KafkaConsumer) Consume(ctx context.Context) (*Message, error) { fetches := c.client.PollFetches(ctx) if fetches.Empty() { return nil, queue.ErrEndOfQueue } // Return first message return \u0026Message{...}, nil } Processor Executes business logic on messages:\ntype Processor[T any] interface { Process(ctx context.Context, msg T) error } Responsibilities:\nImplement business logic for message handling Be idempotent (for at-least-once processing) Return errors to trigger acknowledgment logic Example:\ntype OrderProcessor struct { db *sql.DB } func (p *OrderProcessor) Process(ctx context.Context, msg *OrderMessage) error { // Check idempotency exists, err := p.orderExists(ctx, msg.OrderID) if err != nil { return err } if exists { return nil // Already processed } // Process the order return p.createOrder(ctx, msg) } Acknowledger Confirms successful processing back to the queue:\ntype Acknowledger[T any] interface { Acknowledge(ctx context.Context, msg T) error } Responsibilities:\nCommit offsets or delete messages from the queue Ensure the queue knows the message was processed Handle acknowledgment failures Example:\ntype KafkaAcknowledger struct { client *kgo.Client } func (a *KafkaAcknowledger) Acknowledge(ctx context.Context, msg *Message) error { // Commit the offset for this message return a.client.CommitRecords(ctx, msg.record) } Runtime Orchestrates the three phases and manages the application lifecycle:\ntype Runtime interface { ProcessQueue(ctx context.Context) error } Responsibilities:\nCoordinate Consumer, Processor, and Acknowledger Implement the delivery semantics (order of phases) Handle graceful shutdown when context is cancelled Manage concurrency (e.g., goroutines per partition) Example:\ntype MyRuntime struct { consumer queue.Consumer[Message] processor queue.Processor[Message] acknowledger queue.Acknowledger[Message] } func (r *MyRuntime) ProcessQueue(ctx context.Context) error { for { // Phase 1: Consume msg, err := r.consumer.Consume(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil // Graceful shutdown } if err != nil { return err } // Phase 2: Process if err := r.processor.Process(ctx, msg); err != nil { return err } // Phase 3: Acknowledge if err := r.acknowledger.Acknowledge(ctx, msg); err != nil { return err } } } Built-in Item Processors The queue package provides two built-in processors that implement different delivery semantics:\nProcessAtMostOnce At-most-once processing acknowledges messages before processing:\nprocessor := queue.ProcessAtMostOnce(consumer, processor, acknowledger) for { err := processor.ProcessItem(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil } // Continue even on errors - message already acknowledged } Processing Order: Consume → Acknowledge → Process\nGuarantees:\nEach message processed at most once Messages may be lost on processing failures Fast throughput ProcessAtLeastOnce At-least-once processing acknowledges messages after successful processing:\nprocessor := queue.ProcessAtLeastOnce(consumer, processor, acknowledger) for { err := processor.ProcessItem(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil } if err != nil { return err // Message not acknowledged, will be retried } } Processing Order: Consume → Process → Acknowledge\nGuarantees:\nEach message processed at least once Messages may be duplicated on failures Requires idempotent processors See Delivery Semantics for a detailed comparison.\nApp Wrapper The queue.App type wraps a Runtime and integrates it with the Bedrock framework:\nfunc NewApp(runtime Runtime) *App Features:\nCalls runtime.ProcessQueue(ctx) on startup Handles context cancellation Returns errors to the framework for logging Example:\nfunc Init(ctx context.Context, cfg Config) (*queue.App, error) { runtime := \u0026MyRuntime{...} return queue.NewApp(runtime), nil } Builder Pattern The queue.Builder function creates a Bedrock AppBuilder with automatic instrumentation:\nfunc Builder[T any](init func(context.Context, T) (*App, error)) bedrock.AppBuilder[T] Automatic Features:\nOpenTelemetry SDK initialization Panic recovery in handlers OS signal handling (SIGTERM, SIGINT, SIGKILL) Graceful shutdown coordination Usage:\nbuilder := queue.Builder(Init) app, err := builder(ctx, cfg) Run Function The queue.Run function provides a complete entry point for queue services:\nfunc Run[T any]( reader io.Reader, init func(context.Context, T) (*App, error), opts ...RunOption, ) error Workflow:\nRead YAML configuration from reader Parse config into type T Call init function to build App Run app until completion or error Log errors and exit Example:\nfunc main() { queue.Run(queue.YamlSource(\"config.yaml\"), Init) } Graceful Shutdown The framework handles graceful shutdown automatically:\nSignal Handling - SIGTERM/SIGINT triggers context cancellation Consumer Stops - Consumer returns ErrEndOfQueue In-Flight Processing - Completes current messages Final Acknowledgment - Commits final offsets (at-least-once) Cleanup - Closes connections and resources Implementation:\nfunc (r *MyRuntime) ProcessQueue(ctx context.Context) error { for { select { case \u003c-ctx.Done(): // Context cancelled, stop consuming return nil default: } msg, err := r.consumer.Consume(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil } // ... process message } } Error Handling The framework provides structured error handling:\nErrEndOfQueue:\nSpecial error signaling queue exhaustion Triggers graceful shutdown Not treated as a failure Processing Errors:\nReturn errors from Processor for at-least-once retry Log and continue for at-most-once (message lost) Fatal Errors:\nConsumer/Acknowledger errors typically fatal Return from ProcessQueue to shut down OpenTelemetry Integration All queue processing is automatically instrumented:\nAutomatic Tracing:\nSpan per message Processing order visible in traces Context propagation through phases Automatic Logging:\nStructured logs with message metadata Error recording in spans Performance metrics No additional configuration needed in your Processor implementation.\nNext Steps Learn about Delivery Semantics to choose the right processing model Build your first processor with the Kafka Quick Start ","categories":"","description":"Core abstractions and patterns for queue processing","excerpt":"Core abstractions and patterns for queue processing","ref":"/humus/pr-preview/pr-396/features/queue/queue-framework/","tags":"","title":"Queue Framework"},{"body":"This guide walks you through building a complete Kafka message processor with at-least-once delivery semantics.\nPrerequisites Go 1.21 or later Kafka cluster (local or remote) Humus installed (go get github.com/z5labs/humus) Running Kafka Locally If you don’t have Kafka running, start it with Docker:\ndocker run -d \\ --name kafka \\ -p 9092:9092 \\ -e KAFKA_ENABLE_KRAFT=yes \\ -e KAFKA_CFG_NODE_ID=1 \\ -e KAFKA_CFG_PROCESS_ROLES=broker,controller \\ -e KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@127.0.0.1:9093 \\ -e KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 \\ -e KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:9092 \\ -e KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT \\ -e KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER \\ bitnami/kafka:latest Create a test topic:\ndocker exec kafka kafka-topics.sh \\ --create \\ --topic orders \\ --bootstrap-server localhost:9092 \\ --partitions 3 \\ --replication-factor 1 Project Setup mkdir order-processor cd order-processor go mod init order-processor go get github.com/z5labs/humus Configuration Create config.yaml:\nkafka: brokers: - \"localhost:9092\" group_id: \"order-processors\" topic: \"orders\" otel: service: name: order-processor sdk: disabled: true # Disable for this example Define Your Message Create main.go:\npackage main import ( \"context\" \"encoding/json\" \"fmt\" \"sync\" \"github.com/z5labs/humus/queue\" \"github.com/z5labs/humus/queue/kafka\" ) // OrderMessage represents an order from Kafka type OrderMessage struct { OrderID string `json:\"order_id\"` CustomerID string `json:\"customer_id\"` Amount float64 `json:\"amount\"` } // Config holds application configuration type Config struct { queue.Config `config:\",squash\"` Kafka struct { Brokers []string `config:\"brokers\"` GroupID string `config:\"group_id\"` Topic string `config:\"topic\"` } `config:\"kafka\"` } Implement the Processor An idempotent processor that tracks processed orders:\n// OrderProcessor processes order messages type OrderProcessor struct { mu sync.RWMutex processed map[string]bool } func NewOrderProcessor() *OrderProcessor { return \u0026OrderProcessor{ processed: make(map[string]bool), } } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Deserialize the message var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return fmt.Errorf(\"failed to unmarshal order: %w\", err) } // Idempotency check p.mu.RLock() if p.processed[order.OrderID] { p.mu.RUnlock() fmt.Printf(\"Order %s already processed, skipping\\n\", order.OrderID) return nil } p.mu.RUnlock() // Process the order fmt.Printf(\"Processing order: ID=%s, Customer=%s, Amount=$%.2f\\n\", order.OrderID, order.CustomerID, order.Amount, ) // Simulate order processing // In production: save to database, call payment service, etc. // Mark as processed p.mu.Lock() p.processed[order.OrderID] = true p.mu.Unlock() return nil } Initialize the Runtime Configure the Kafka runtime with at-least-once processing:\nfunc Init(ctx context.Context, cfg Config) (*queue.App, error) { processor := NewOrderProcessor() runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), ) return queue.NewApp(runtime), nil } func main() { queue.Run(queue.YamlSource(\"config.yaml\"), Init) } Run the Processor go run main.go You should see output indicating the processor is running:\nINFO Starting order-processor INFO Kafka consumer group initialized group_id=order-processors Test with Messages In another terminal, produce test messages to Kafka:\n# Message 1 echo '{\"order_id\":\"ord-001\",\"customer_id\":\"cust-123\",\"amount\":99.99}' | \\ docker exec -i kafka kafka-console-producer.sh \\ --broker-list localhost:9092 \\ --topic orders # Message 2 echo '{\"order_id\":\"ord-002\",\"customer_id\":\"cust-456\",\"amount\":149.99}' | \\ docker exec -i kafka kafka-console-producer.sh \\ --broker-list localhost:9092 \\ --topic orders # Duplicate of Message 1 (to test idempotency) echo '{\"order_id\":\"ord-001\",\"customer_id\":\"cust-123\",\"amount\":99.99}' | \\ docker exec -i kafka kafka-console-producer.sh \\ --broker-list localhost:9092 \\ --topic orders Your processor should output:\nProcessing order: ID=ord-001, Customer=cust-123, Amount=$99.99 Processing order: ID=ord-002, Customer=cust-456, Amount=$149.99 Order ord-001 already processed, skipping Notice the duplicate message was detected and skipped.\nWhat’s Happening Let’s break down the key components:\n1. Configuration The YAML config provides Kafka connection details and consumer group settings:\nkafka: brokers: [\"localhost:9092\"] # Kafka broker addresses group_id: \"order-processors\" # Consumer group for offset tracking topic: \"orders\" # Topic to consume from 2. Message Deserialization The processor receives kafka.Message with raw bytes:\nfunc (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage json.Unmarshal(msg.Value, \u0026order) // ... } msg.Value contains the JSON bytes, which we deserialize into OrderMessage.\n3. Idempotent Processing The processor tracks processed order IDs to handle duplicates:\nif p.processed[order.OrderID] { return nil // Skip duplicate } // Process... p.processed[order.OrderID] = true This is critical for at-least-once processing where Kafka may redeliver messages.\n4. At-Least-Once Semantics kafka.AtLeastOnce(cfg.Kafka.Topic, processor) This ensures:\nMessages are processed before offsets are committed Failed processing results in message redelivery No messages are lost due to processing failures 5. Graceful Shutdown Press Ctrl+C to stop the processor. You’ll see:\nINFO Shutting down gracefully INFO Committing final offsets INFO Kafka client closed The framework ensures in-flight messages complete before shutdown.\nProduction Considerations This example uses in-memory state. For production:\nDatabase-Backed Idempotency Replace the in-memory map with database storage:\nfunc (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Check database for existing order var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM orders WHERE order_id = $1)\", order.OrderID, ).Scan(\u0026exists) if err != nil { return err } if exists { return nil // Already processed } // Process in transaction tx, err := p.db.BeginTx(ctx, nil) if err != nil { return err } defer tx.Rollback() _, err = tx.ExecContext(ctx, \"INSERT INTO orders (order_id, customer_id, amount) VALUES ($1, $2, $3)\", order.OrderID, order.CustomerID, order.Amount, ) if err != nil { return err } return tx.Commit() } See Idempotency for detailed patterns.\nEnable OpenTelemetry For production observability, enable OTel in config.yaml:\notel: service: name: order-processor sdk: disabled: false exporter: otlp: endpoint: \"localhost:4317\" protocol: grpc See Observability for details.\nTune Performance Adjust fetch settings for your workload:\nruntime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), kafka.FetchMaxBytes(100 * 1024 * 1024), // 100 MB kafka.MaxConcurrentFetches(10), ) See Configuration for tuning guidance.\nComplete Code Full main.go (click to expand) package main import ( \"context\" \"encoding/json\" \"fmt\" \"sync\" \"github.com/z5labs/humus/queue\" \"github.com/z5labs/humus/queue/kafka\" ) type OrderMessage struct { OrderID string `json:\"order_id\"` CustomerID string `json:\"customer_id\"` Amount float64 `json:\"amount\"` } type Config struct { queue.Config `config:\",squash\"` Kafka struct { Brokers []string `config:\"brokers\"` GroupID string `config:\"group_id\"` Topic string `config:\"topic\"` } `config:\"kafka\"` } type OrderProcessor struct { mu sync.RWMutex processed map[string]bool } func NewOrderProcessor() *OrderProcessor { return \u0026OrderProcessor{ processed: make(map[string]bool), } } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return fmt.Errorf(\"failed to unmarshal order: %w\", err) } p.mu.RLock() if p.processed[order.OrderID] { p.mu.RUnlock() fmt.Printf(\"Order %s already processed, skipping\\n\", order.OrderID) return nil } p.mu.RUnlock() fmt.Printf(\"Processing order: ID=%s, Customer=%s, Amount=$%.2f\\n\", order.OrderID, order.CustomerID, order.Amount, ) p.mu.Lock() p.processed[order.OrderID] = true p.mu.Unlock() return nil } func Init(ctx context.Context, cfg Config) (*queue.App, error) { processor := NewOrderProcessor() runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), ) return queue.NewApp(runtime), nil } func main() { queue.Run(queue.YamlSource(\"config.yaml\"), Init) } Next Steps Learn about Message Structure to work with headers and metadata Explore Multi-Topic Processing to handle multiple topics Understand Concurrency Model for partition-level parallelism Implement robust Idempotency patterns for production ","categories":"","description":"Build your first Kafka message processor","excerpt":"Build your first Kafka message processor","ref":"/humus/pr-preview/pr-396/features/queue/kafka/quick-start/","tags":"","title":"Quick Start"},{"body":"This guide walks you through building a complete REST API with CRUD operations.\nPrerequisites Go 1.21 or later Humus installed (go get github.com/z5labs/humus) Project Setup mkdir todo-api cd todo-api go mod init todo-api go get github.com/z5labs/humus Configuration Create config.yaml:\nrest: port: 8080 otel: service: name: todo-api sdk: disabled: true # Disable for this example Define Your Model Create main.go:\npackage main import ( \"context\" \"fmt\" \"net/http\" \"sync\" \"github.com/z5labs/humus/rest\" ) type Todo struct { ID string `json:\"id\"` Title string `json:\"title\"` Completed bool `json:\"completed\"` } // In-memory store type TodoStore struct { mu sync.RWMutex todos map[string]Todo } func NewTodoStore() *TodoStore { return \u0026TodoStore{ todos: make(map[string]Todo), } } func (s *TodoStore) Create(todo Todo) { s.mu.Lock() defer s.mu.Unlock() s.todos[todo.ID] = todo } func (s *TodoStore) Get(id string) (Todo, bool) { s.mu.RLock() defer s.mu.RUnlock() todo, ok := s.todos[id] return todo, ok } func (s *TodoStore) List() []Todo { s.mu.RLock() defer s.mu.RUnlock() todos := make([]Todo, 0, len(s.todos)) for _, todo := range s.todos { todos = append(todos, todo) } return todos } func (s *TodoStore) Update(todo Todo) bool { s.mu.Lock() defer s.mu.Unlock() if _, exists := s.todos[todo.ID]; !exists { return false } s.todos[todo.ID] = todo return true } func (s *TodoStore) Delete(id string) bool { s.mu.Lock() defer s.mu.Unlock() if _, exists := s.todos[id]; !exists { return false } delete(s.todos, id) return true } Configuration Struct type Config struct { rest.Config `config:\",squash\"` } Main Function func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } Initialize API func Init(ctx context.Context, cfg Config) (*rest.Api, error) { store := NewTodoStore() // Create todo handler createHandler := rest.HandlerFunc[Todo, Todo](func(ctx context.Context, req *Todo) (*Todo, error) { if req.ID == \"\" { req.ID = fmt.Sprintf(\"todo-%d\", len(store.todos)+1) } store.Create(*req) return req, nil }) // List todos handler listHandler := rest.ProducerFunc[[]Todo](func(ctx context.Context) (*[]Todo, error) { todos := store.List() return \u0026todos, nil }) // Get todo handler getHandler := rest.ProducerFunc[Todo](func(ctx context.Context) (*Todo, error) { id := rest.PathParamValue(ctx, \"id\") todo, ok := store.Get(id) if !ok { return nil, fmt.Errorf(\"todo not found\") } return \u0026todo, nil }) // Update todo handler updateHandler := rest.HandlerFunc[Todo, Todo](func(ctx context.Context, req *Todo) (*Todo, error) { id := rest.PathParamValue(ctx, \"id\") req.ID = id if !store.Update(*req) { return nil, fmt.Errorf(\"todo not found\") } return req, nil }) // Delete todo handler deleteHandler := rest.ConsumerFunc[struct{}](func(ctx context.Context, req *struct{}) error { id := rest.PathParamValue(ctx, \"id\") if !store.Delete(id) { return fmt.Errorf(\"todo not found\") } return nil }) // Create API with all endpoints api := rest.NewApi( \"Todo API\", \"1.0.0\", rest.Handle(http.MethodPost, rest.BasePath(\"/todos\"), rest.HandleJson(createHandler)), rest.Handle(http.MethodGet, rest.BasePath(\"/todos\"), rest.ProduceJson(listHandler)), rest.Handle(http.MethodGet, rest.BasePath(\"/todos\").Param(\"id\"), rest.ProduceJson(getHandler)), rest.Handle(http.MethodPut, rest.BasePath(\"/todos\").Param(\"id\"), rest.HandleJson(updateHandler)), rest.Handle(http.MethodDelete, rest.BasePath(\"/todos\").Param(\"id\"), rest.ConsumeOnlyJson(deleteHandler)), ) return api, nil } Complete Code Put it all together in main.go:\npackage main import ( \"context\" \"fmt\" \"net/http\" \"sync\" \"github.com/z5labs/humus/rest\" ) type Todo struct { ID string `json:\"id\"` Title string `json:\"title\"` Completed bool `json:\"completed\"` } type TodoStore struct { mu sync.RWMutex todos map[string]Todo } func NewTodoStore() *TodoStore { return \u0026TodoStore{ todos: make(map[string]Todo), } } func (s *TodoStore) Create(todo Todo) { s.mu.Lock() defer s.mu.Unlock() s.todos[todo.ID] = todo } func (s *TodoStore) Get(id string) (Todo, bool) { s.mu.RLock() defer s.mu.RUnlock() todo, ok := s.todos[id] return todo, ok } func (s *TodoStore) List() []Todo { s.mu.RLock() defer s.mu.RUnlock() todos := make([]Todo, 0, len(s.todos)) for _, todo := range s.todos { todos = append(todos, todo) } return todos } func (s *TodoStore) Update(todo Todo) bool { s.mu.Lock() defer s.mu.Unlock() if _, exists := s.todos[todo.ID]; !exists { return false } s.todos[todo.ID] = todo return true } func (s *TodoStore) Delete(id string) bool { s.mu.Lock() defer s.mu.Unlock() if _, exists := s.todos[id]; !exists { return false } delete(s.todos, id) return true } type Config struct { rest.Config `config:\",squash\"` } func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { store := NewTodoStore() // Create todo handler createHandler := rest.HandlerFunc[Todo, Todo](func(ctx context.Context, req *Todo) (*Todo, error) { if req.ID == \"\" { req.ID = fmt.Sprintf(\"todo-%d\", len(store.todos)+1) } store.Create(*req) return req, nil }) // List todos handler listHandler := rest.ProducerFunc[[]Todo](func(ctx context.Context) (*[]Todo, error) { todos := store.List() return \u0026todos, nil }) // Get todo handler getHandler := rest.ProducerFunc[Todo](func(ctx context.Context) (*Todo, error) { id := rest.PathParamValue(ctx, \"id\") todo, ok := store.Get(id) if !ok { return nil, fmt.Errorf(\"todo not found\") } return \u0026todo, nil }) // Update todo handler updateHandler := rest.HandlerFunc[Todo, Todo](func(ctx context.Context, req *Todo) (*Todo, error) { id := rest.PathParamValue(ctx, \"id\") req.ID = id if !store.Update(*req) { return nil, fmt.Errorf(\"todo not found\") } return req, nil }) // Delete todo handler deleteHandler := rest.ConsumerFunc[struct{}](func(ctx context.Context, req *struct{}) error { id := rest.PathParamValue(ctx, \"id\") if !store.Delete(id) { return fmt.Errorf(\"todo not found\") } return nil }) // Create API with all endpoints api := rest.NewApi( \"Todo API\", \"1.0.0\", rest.Handle(http.MethodPost, rest.BasePath(\"/todos\"), rest.HandleJson(createHandler)), rest.Handle(http.MethodGet, rest.BasePath(\"/todos\"), rest.ProduceJson(listHandler)), rest.Handle(http.MethodGet, rest.BasePath(\"/todos\").Param(\"id\"), rest.ProduceJson(getHandler)), rest.Handle(http.MethodPut, rest.BasePath(\"/todos\").Param(\"id\"), rest.HandleJson(updateHandler)), rest.Handle(http.MethodDelete, rest.BasePath(\"/todos\").Param(\"id\"), rest.ConsumeOnlyJson(deleteHandler)), ) return api, nil } Run the Service go run main.go Test the API # Create a todo curl -X POST http://localhost:8080/todos \\ -H \"Content-Type: application/json\" \\ -d '{\"title\": \"Learn Humus\", \"completed\": false}' # List all todos curl http://localhost:8080/todos # Get a specific todo curl http://localhost:8080/todos/todo-1 # Update a todo curl -X PUT http://localhost:8080/todos/todo-1 \\ -H \"Content-Type: application/json\" \\ -d '{\"title\": \"Learn Humus\", \"completed\": true}' # Delete a todo curl -X DELETE http://localhost:8080/todos/todo-1 # View OpenAPI spec curl http://localhost:8080/openapi.json What’s Happening rest.Run() loads config and calls Init rest.NewApi() creates the API with name and version rest.HandleJson/ProduceJson/ConsumeOnlyJson wrap handlers with type-safe serialization rest.Handle() registers handlers at specific paths as API options Automatic instrumentation traces all requests OpenAPI generation creates /openapi.json from your types Securing Your API (Optional) Add JWT authentication to protect write operations:\n1. Create a Simple JWT Verifier import ( \"context\" \"fmt\" ) type SimpleJWTVerifier struct{} func (v *SimpleJWTVerifier) Verify(ctx context.Context, token string) (context.Context, error) { // In production, verify the JWT signature and claims // For this example, we just accept any non-empty token if token == \"\" { return nil, fmt.Errorf(\"empty token\") } // Extract user info (in production, parse from JWT claims) userID := \"user-from-token\" return context.WithValue(ctx, \"user_id\", userID), nil } 2. Protect Create/Update/Delete Operations func registerHandlers(api *rest.Api, store *TodoStore) { verifier := \u0026SimpleJWTVerifier{} // Public endpoint - no auth required listHandler := rest.ProducerFunc[[]Todo](func(ctx context.Context) (*[]Todo, error) { todos := store.List() return \u0026todos, nil }) rest.Handle(http.MethodGet, rest.BasePath(\"/todos\"), rest.ProduceJson(listHandler)) // Protected endpoint - JWT required createHandler := rest.HandlerFunc[Todo, Todo](func(ctx context.Context, req *Todo) (*Todo, error) { if req.ID == \"\" { req.ID = fmt.Sprintf(\"todo-%d\", len(store.todos)+1) } store.Create(*req) return req, nil }) rest.Handle( http.MethodPost, rest.BasePath(\"/todos\"), rest.HandleJson(createHandler), rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ) // Other endpoints... } 3. Test with Authentication # Fails - no Authorization header curl -X POST http://localhost:8080/todos \\ -H \"Content-Type: application/json\" \\ -d '{\"title\": \"Protected todo\"}' # Returns: 401 Unauthorized # Success - with Bearer token curl -X POST http://localhost:8080/todos \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer my-token\" \\ -d '{\"title\": \"Protected todo\"}' # Returns: 200 OK For production JWT implementation with proper signature verification, see Authentication.\nNext Steps Learn about Authentication for production-ready JWT verification Read Handler Helpers for type-safe handlers and serialization See Routing for path parameters and validation Understand Error Handling for custom error responses ","categories":"","description":"Build your first REST API","excerpt":"Build your first REST API","ref":"/humus/pr-preview/pr-396/features/rest/quick-start/","tags":"","title":"Quick Start"},{"body":"Humus REST services provide a complete framework for building OpenAPI-compliant HTTP APIs with automatic schema generation, type-safe handlers, and built-in observability.\nOverview REST services in Humus are built on:\nchi - Fast, lightweight HTTP router OpenAPI 3.0 - Automatic API documentation Type Safety - Compile-time type checking for requests and responses OpenTelemetry - Automatic tracing and metrics Quick Start package main import ( \"context\" \"net/http\" \"github.com/z5labs/humus/rest\" ) type Config struct { rest.Config `config:\",squash\"` } type HelloResponse struct { Message string `json:\"message\"` } func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { handler := rest.ProducerFunc[HelloResponse](func(ctx context.Context) (*HelloResponse, error) { return \u0026HelloResponse{Message: \"Hello, World!\"}, nil }) api := rest.NewApi( \"Hello Service\", \"1.0.0\", rest.Handle( http.MethodGet, rest.BasePath(\"/hello\"), rest.ProduceJson(handler), ), ) return api, nil } Core Components rest.Api The main API object that combines:\nHTTP router (chi) OpenAPI spec generator Health check endpoints Middleware management RPC Pattern Type-safe handler abstraction in the rest package:\nrest.Handler[Req, Resp] - Business logic interface Request deserialization (JSON, XML, etc.) Response serialization OpenAPI schema generation Path Building Flexible path definition:\nStatic paths: /users Path parameters: /users/{id} Nested paths: /users/{id}/posts/{postId} Built-in Endpoints Every REST service automatically includes:\nGET /openapi.json - OpenAPI 3.0 specification GET /health/liveness - Liveness probe GET /health/readiness - Readiness probe What You’ll Learn This section covers:\nQuick Start - Build your first REST API Handler Helpers - Type-safe handlers and serialization Routing - Paths and parameters Authentication - JWT, API keys, and security Error Handling - Custom error responses OpenAPI - Working with generated specs Health Checks - Monitoring service health Next Steps Start with the Quick Start Guide to build your first REST service.\n","categories":"","description":"Building OpenAPI-compliant HTTP APIs","excerpt":"Building OpenAPI-compliant HTTP APIs","ref":"/humus/pr-preview/pr-396/features/rest/","tags":"","title":"REST Services"},{"body":"Complete, hands-on tutorials that guide you through building real-world applications with Humus.\nAvailable Walkthroughs Orders REST API Build a production-ready REST API with service orchestration, cursor-based pagination, and full observability.\nWhat You’ll Build:\nGET /v1/orders endpoint with pagination POST /v1/order endpoint with service orchestration (Restriction → Eligibility → Data) Complete observability stack (Tempo, Loki, Mimir, Grafana) Mock backend services with Wiremock Time Estimate: 45-60 minutes\nPrerequisites:\nGo 1.24+ Podman or Docker Basic familiarity with REST APIs Start the Orders REST Walkthrough →\n1 Billion Row Challenge Job Build a high-performance job application that processes 1 billion temperature measurements using MinIO for storage and OpenTelemetry for observability.\nTopics covered:\nJob framework architecture S3 storage integration Streaming large datasets OpenTelemetry instrumentation (traces, metrics, logs) Viewing telemetry in Grafana Difficulty: Intermediate\nTime: 60 minutes\nPrerequisites:\nGo 1.24+ Podman or Docker About Walkthroughs Each walkthrough is designed to:\nBe Complete - Every step results in runnable code Follow Best Practices - Use production-ready patterns Include Observability - Full OpenTelemetry integration from the start Match Code to Docs - Source code and documentation are always in sync The source code for each walkthrough is available in the example/ directory of the repository. Contributing Have an idea for a walkthrough? See our Contributing Guide.\n","categories":"","description":"Step-by-step guides for building production-ready applications","excerpt":"Step-by-step guides for building production-ready applications","ref":"/humus/pr-preview/pr-396/walkthroughs/","tags":"","title":"Walkthroughs"},{"body":"Queue processing services must choose between two fundamental delivery guarantees: at-most-once and at-least-once. This choice affects how your application handles failures and determines the reliability guarantees you can provide.\nQuick Comparison Aspect At-Most-Once At-Least-Once Processing Order Consume → Acknowledge → Process Consume → Process → Acknowledge On Failure Message lost Message retried Duplicates Never Possible Throughput Higher Lower Processor Requirements None Must be idempotent Use Cases Metrics, logs, caching Transactions, database updates At-Most-Once Processing At-most-once processing acknowledges messages before processing them. This provides fast throughput but means messages can be lost if processing fails.\nProcessing Flow 1. Consume message from queue 2. Acknowledge message (commit offset) 3. Process message If step 3 fails, the message has already been acknowledged and is permanently lost.\nWhen to Use At-most-once is appropriate when:\nPerformance is critical - Lower latency and higher throughput Data loss is acceptable - Occasional message loss won’t impact your application Messages are non-critical - Informational data that can be recreated or ignored Common Use Cases Metrics Collection:\ntype MetricsProcessor struct { client *prometheus.Client } func (p *MetricsProcessor) Process(ctx context.Context, msg *MetricMessage) error { // Send metric to monitoring system // If this fails, we can tolerate losing a few data points return p.client.RecordMetric(ctx, msg.Name, msg.Value) } Log Aggregation:\ntype LogProcessor struct { writer *LogWriter } func (p *LogProcessor) Process(ctx context.Context, msg *LogMessage) error { // Write log to aggregation system // Missing a few log entries is acceptable return p.writer.Write(ctx, msg.Level, msg.Message) } Cache Updates:\ntype CacheProcessor struct { cache *redis.Client } func (p *CacheProcessor) Process(ctx context.Context, msg *CacheUpdate) error { // Update cache entry // Cache can be rebuilt if some updates are lost return p.cache.Set(ctx, msg.Key, msg.Value, msg.TTL) } Advantages Higher throughput - No waiting for processing to complete before acknowledging Lower latency - Messages acknowledged immediately Simpler implementation - No idempotency requirements Faster recovery - Failures don’t block message consumption Disadvantages Data loss - Processing failures result in lost messages No retry logic - Failed messages are not retried Weaker guarantees - Cannot ensure all messages are processed Implementation processor := queue.ProcessAtMostOnce(consumer, processor, acknowledger) for { err := processor.ProcessItem(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil // Graceful shutdown } // Continue processing even on errors // Message already acknowledged and lost } At-Least-Once Processing At-least-once processing acknowledges messages after successful processing. This provides reliable delivery but means messages may be processed multiple times.\nProcessing Flow 1. Consume message from queue 2. Process message 3. Acknowledge message (commit offset) If step 2 fails, the message is not acknowledged and will be redelivered for retry.\nWhen to Use At-least-once is appropriate when:\nReliability is critical - Every message must be processed successfully Data loss is unacceptable - Missing messages would corrupt data or business logic Idempotency is achievable - Your processor can handle duplicate messages safely Common Use Cases Financial Transactions:\ntype PaymentProcessor struct { db *sql.DB } func (p *PaymentProcessor) Process(ctx context.Context, msg *Payment) error { // Check if already processed (idempotency) var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM payments WHERE transaction_id = $1)\", msg.TransactionID, ).Scan(\u0026exists) if err != nil { return err } if exists { return nil // Already processed, skip } // Process payment _, err = p.db.ExecContext(ctx, \"INSERT INTO payments (transaction_id, amount, status) VALUES ($1, $2, 'completed')\", msg.TransactionID, msg.Amount, ) return err } Database Updates:\ntype OrderProcessor struct { db *sql.DB } func (p *OrderProcessor) Process(ctx context.Context, msg *Order) error { // Upsert: idempotent database operation _, err := p.db.ExecContext(ctx, `INSERT INTO orders (order_id, customer_id, total) VALUES ($1, $2, $3) ON CONFLICT (order_id) DO UPDATE SET customer_id = EXCLUDED.customer_id, total = EXCLUDED.total`, msg.OrderID, msg.CustomerID, msg.Total, ) return err } Event Sourcing:\ntype EventProcessor struct { store EventStore } func (p *EventProcessor) Process(ctx context.Context, msg *Event) error { // Event store handles deduplication return p.store.Append(ctx, msg.StreamID, msg) } Advantages Reliable delivery - All messages are processed successfully Automatic retry - Failed messages are retried automatically Stronger guarantees - Can ensure critical operations complete Data integrity - No messages lost or skipped Disadvantages Lower throughput - Must wait for processing to complete before acknowledging Higher latency - Acknowledgment delayed until processing succeeds Duplicate processing - Messages may be processed multiple times Idempotency required - Processors must handle duplicates correctly Implementation processor := queue.ProcessAtLeastOnce(consumer, processor, acknowledger) for { err := processor.ProcessItem(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil // Graceful shutdown } if err != nil { return err // Stop processing on error } } Choosing the Right Semantic Use this decision tree to choose the appropriate semantic:\nCan your application tolerate message loss? ├─ Yes → Is performance critical? │ ├─ Yes → At-Most-Once │ └─ No → Either (prefer At-Most-Once for simplicity) └─ No → Can you implement idempotent processing? ├─ Yes → At-Least-Once └─ No → Redesign to support idempotency or accept data loss Questions to Ask What happens if a message is lost?\nCritical failure → At-Least-Once Acceptable loss → At-Most-Once Can your processor handle duplicate messages?\nYes (idempotent) → At-Least-Once is safe No → At-Most-Once or redesign What are your performance requirements?\nHigh throughput needed → At-Most-Once Reliability more important → At-Least-Once What is the cost of duplicate processing?\nLow (read-only, idempotent) → At-Least-Once is safe High (side effects, non-idempotent) → At-Most-Once or redesign Idempotency Strategies At-least-once processing requires idempotent processors. Common strategies:\nUnique ID Tracking Store processed message IDs in a database:\nfunc (p *Processor) Process(ctx context.Context, msg *Message) error { // Check if already processed var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM processed_messages WHERE message_id = $1)\", msg.ID, ).Scan(\u0026exists) if err != nil { return err } if exists { return nil } // Process and record in same transaction tx, err := p.db.BeginTx(ctx, nil) if err != nil { return err } defer tx.Rollback() // Do work if err := p.doWork(ctx, tx, msg); err != nil { return err } // Record processed _, err = tx.ExecContext(ctx, \"INSERT INTO processed_messages (message_id) VALUES ($1)\", msg.ID, ) if err != nil { return err } return tx.Commit() } Natural Idempotency Design operations to be naturally idempotent:\n// Idempotent: Setting a value UPDATE users SET email = 'new@example.com' WHERE id = 123 // NOT idempotent: Incrementing a value UPDATE accounts SET balance = balance + 100 WHERE id = 456 Upsert Operations Use database upserts for idempotent writes:\n_, err := db.ExecContext(ctx, `INSERT INTO orders (order_id, total) VALUES ($1, $2) ON CONFLICT (order_id) DO UPDATE SET total = EXCLUDED.total`, msg.OrderID, msg.Total, ) See Kafka Idempotency for Kafka-specific patterns.\nMixed Semantics Some applications may need different semantics for different message types:\nfunc Init(ctx context.Context, cfg Config) (*queue.App, error) { // Critical orders: at-least-once ordersRuntime, err := kafka.NewAtLeastOnceRuntime( cfg.Kafka.Brokers, \"orders\", cfg.Kafka.GroupID, ordersProcessor, decodeOrder, ) if err != nil { return nil, err } // Non-critical metrics: at-most-once metricsRuntime, err := kafka.NewAtMostOnceRuntime( cfg.Kafka.Brokers, \"metrics\", cfg.Kafka.GroupID, metricsProcessor, decodeMetric, ) if err != nil { return nil, err } // Combine runtimes (implementation-specific) runtime := newMultiRuntime(ordersRuntime, metricsRuntime) return queue.NewApp(runtime), nil } Next Steps Implement idempotent processors with Kafka Idempotency Learn about Kafka-specific features in Kafka Runtime Build your first processor with Kafka Quick Start ","categories":"","description":"Understanding at-most-once and at-least-once processing","excerpt":"Understanding at-most-once and at-least-once processing","ref":"/humus/pr-preview/pr-396/features/queue/delivery-semantics/","tags":"","title":"Delivery Semantics"},{"body":"Humus gRPC services provide a complete framework for building high-performance microservices with automatic instrumentation, health checks, and seamless Protocol Buffers integration.\nOverview gRPC services in Humus are built on:\ngRPC-Go - Official gRPC implementation Automatic Health Service - gRPC health checking protocol OpenTelemetry Interceptors - Built-in tracing and metrics Service Registration - Simple API for registering services Quick Start package main import ( \"context\" \"github.com/z5labs/humus/grpc\" pb \"your-module/gen/proto/user\" ) type Config struct { grpc.Config `config:\",squash\"` } type userService struct { pb.UnimplementedUserServiceServer } func (s *userService) GetUser(ctx context.Context, req *pb.GetUserRequest) (*pb.User, error) { return \u0026pb.User{ Id: req.Id, Name: \"John Doe\", Email: \"john@example.com\", }, nil } func main() { grpc.Run(grpc.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*grpc.Api, error) { api := grpc.NewApi() // Register your service pb.RegisterUserServiceServer(api, \u0026userService{}) return api, nil } Core Components grpc.Api The main API object that:\nImplements grpc.ServiceRegistrar Manages interceptors for OTel Automatically registers health service Monitors registered services Automatic Features Every gRPC service gets:\nHealth Service - Implements grpc.health.v1.Health protocol Tracing - Automatic span creation for RPCs Metrics - Request count, duration, status Service Monitoring - Health checks for services implementing health.Monitor Built-in Health Service The gRPC health service is automatically registered and provides:\n/grpc.health.v1.Health/Check - Check service health /grpc.health.v1.Health/Watch - Stream health updates No configuration needed - it works out of the box.\nWhat You’ll Learn This section covers:\nQuick Start - Build your first gRPC service gRPC API - Understanding grpc.Api Health Service - Health checking protocol Interceptors - OTel instrumentation Petstore Example - Complete walkthrough Next Steps Start with the Quick Start Guide to build your first gRPC service.\n","categories":"","description":"Building high-performance microservices","excerpt":"Building high-performance microservices","ref":"/humus/pr-preview/pr-396/features/grpc/","tags":"","title":"gRPC Services"},{"body":"Humus provides built-in observability through automatic OpenTelemetry (OTel) integration. Every service gets traces, metrics, and logs out of the box.\nOverview OpenTelemetry is automatically initialized when your Humus service starts. You get:\nDistributed Tracing - Automatic HTTP/gRPC tracing plus manual instrumentation Metrics - Built-in HTTP/gRPC metrics plus custom metrics Structured Logging - Integrated slog logger with trace correlation Automatic Instrumentation REST Services HTTP handlers are automatically instrumented with:\nRequest tracing - Each request creates a span HTTP metrics - Request count, duration, status codes Error tracking - Automatic error recording in spans // No extra code needed - automatic instrumentation! func Init(ctx context.Context, cfg Config) (*rest.Api, error) { handler := rest.ProducerFunc[Response](handleRequest) // Automatically traced api := rest.NewApi( \"My Service\", \"1.0.0\", rest.Handle(http.MethodGet, rest.BasePath(\"/users\"), rest.ProduceJson(handler)), ) return api, nil } gRPC Services gRPC methods are automatically instrumented via interceptors:\nRPC tracing - Each RPC creates a span gRPC metrics - Call count, duration, status Error tracking - Automatic error recording // gRPC instrumentation is automatic via interceptors func Init(ctx context.Context, cfg Config) (*grpc.Api, error) { api := grpc.NewApi() userpb.RegisterUserServiceServer(api, \u0026userService{}) // Automatically traced return api, nil } Job Services Jobs are traced from start to finish:\ntype MyJob struct{} func (j *MyJob) Handle(ctx context.Context) error { // The entire job execution is automatically traced // ctx already contains trace context return processJob(ctx) } Configuration Basic OTel Config Minimal configuration for local development:\notel: service: name: my-service sdk: disabled: false # Enable OTel Production Config Full configuration for production:\notel: service: name: my-service version: 1.0.0 namespace: production instance_id: {{env \"POD_NAME\"}} sdk: disabled: false resource: attributes: deployment.environment: production service.team: platform k8s.cluster.name: prod-cluster traces: sampler: type: parentbased_traceidratio arg: 0.1 # Sample 10% of traces exporter: otlp: endpoint: {{env \"OTEL_EXPORTER_OTLP_ENDPOINT\"}} protocol: grpc headers: api-key: {{env \"OTEL_API_KEY\"}} metrics: exporter: otlp: endpoint: {{env \"OTEL_EXPORTER_OTLP_ENDPOINT\"}} protocol: grpc logs: exporter: otlp: endpoint: {{env \"OTEL_EXPORTER_OTLP_ENDPOINT\"}} protocol: grpc Disabling OTel For development or testing:\notel: sdk: disabled: true # No telemetry overhead Manual Instrumentation Creating Spans Use the standard OTel SDK to create custom spans:\nimport ( \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/attribute\" \"go.opentelemetry.io/otel/codes\" ) func processOrder(ctx context.Context, orderID string) error { // Get a tracer tracer := otel.Tracer(\"my-service\") // Create a span ctx, span := tracer.Start(ctx, \"processOrder\") defer span.End() // Add attributes span.SetAttributes( attribute.String(\"order.id\", orderID), attribute.Int(\"order.items\", 5), ) // Do work... if err := validateOrder(ctx, orderID); err != nil { // Record error span.RecordError(err) span.SetStatus(codes.Error, \"validation failed\") return err } span.SetStatus(codes.Ok, \"order processed\") return nil } Nested Spans Create hierarchical traces:\nfunc processOrder(ctx context.Context, orderID string) error { tracer := otel.Tracer(\"my-service\") ctx, span := tracer.Start(ctx, \"processOrder\") defer span.End() // Child span 1 if err := validateOrder(ctx, orderID); err != nil { return err } // Child span 2 if err := chargePayment(ctx, orderID); err != nil { return err } return nil } func validateOrder(ctx context.Context, orderID string) error { tracer := otel.Tracer(\"my-service\") // This will be a child of \"processOrder\" span ctx, span := tracer.Start(ctx, \"validateOrder\") defer span.End() // Validation logic... return nil } Recording Metrics Create custom metrics:\nimport ( \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/metric\" ) var ( orderCounter metric.Int64Counter orderDuration metric.Float64Histogram ) func init() { meter := otel.Meter(\"my-service\") orderCounter, _ = meter.Int64Counter( \"orders.processed\", metric.WithDescription(\"Number of orders processed\"), ) orderDuration, _ = meter.Float64Histogram( \"orders.duration\", metric.WithDescription(\"Order processing duration\"), metric.WithUnit(\"ms\"), ) } func processOrder(ctx context.Context, orderID string) error { start := time.Now() // Process order... // Record metrics orderCounter.Add(ctx, 1, metric.WithAttributes( attribute.String(\"status\", \"success\"), ), ) duration := time.Since(start).Milliseconds() orderDuration.Record(ctx, float64(duration)) return nil } Structured Logging Using Humus Logger Get an OpenTelemetry-integrated logger:\nimport \"github.com/z5labs/humus\" func processOrder(ctx context.Context, orderID string) error { log := humus.Logger(\"order-processor\") log.InfoContext(ctx, \"processing order\", \"order_id\", orderID, \"user_id\", \"user123\", ) if err := validateOrder(ctx, orderID); err != nil { log.ErrorContext(ctx, \"validation failed\", \"order_id\", orderID, \"error\", err, ) return err } log.InfoContext(ctx, \"order processed successfully\", \"order_id\", orderID, ) return nil } Log-Trace Correlation Logs automatically include trace context when using InfoContext, ErrorContext, etc.:\nfunc handleRequest(ctx context.Context, req Request) (Response, error) { log := humus.Logger(\"handler\") // This log will include trace_id and span_id log.InfoContext(ctx, \"handling request\", \"request_id\", req.ID, ) // Trace and logs are correlated automatically tracer := otel.Tracer(\"my-service\") ctx, span := tracer.Start(ctx, \"processRequest\") defer span.End() // This log also includes the same trace context log.InfoContext(ctx, \"processing\", \"step\", \"validation\", ) return processRequest(ctx, req) } Log Levels Use appropriate log levels:\nlog := humus.Logger(\"my-service\") // Debug - verbose information for debugging log.DebugContext(ctx, \"cache hit\", \"key\", cacheKey) // Info - normal operational messages log.InfoContext(ctx, \"request processed\", \"duration_ms\", duration) // Warn - warning messages log.WarnContext(ctx, \"rate limit approaching\", \"current\", current, \"limit\", limit) // Error - error messages log.ErrorContext(ctx, \"failed to connect\", \"error\", err) Minimum Log Level Filtering You can configure minimum log levels per logger name to filter out verbose logs from specific packages:\notel: log: processor: type: batch batch: export_interval: 1s max_size: 512 exporter: type: otlp otlp: type: grpc target: localhost:4317 minimum_log_level: github.com/z5labs/humus/queue/kafka: info # Filter DEBUG logs github.com/twmb/franz-go/pkg/kgo: warn # Filter DEBUG and INFO logs github.com/some/verbose-lib: error # Only ERROR logs How it works:\nLogger name matching: Uses the instrumentation scope name (package path) from humus.Logger(\"package/name\") Prefix matching: If “github.com/z5labs/humus” is configured, it matches “github.com/z5labs/humus/queue/kafka” and all subpackages Longest prefix wins: More specific configurations take precedence Fail-open: Unconfigured loggers allow all log levels Supported levels:\ndebug - All logs (most verbose) info - INFO and above warn or warning - WARN and above error - Only ERROR logs (least verbose) This is useful for reducing noise from third-party libraries or specific internal packages without affecting other loggers.\nSampling Control trace volume with sampling:\nAlways On (Development) otel: traces: sampler: type: always_on # Capture all traces Ratio-Based (Production) otel: traces: sampler: type: traceidratio arg: 0.1 # Sample 10% of traces Parent-Based (Recommended) otel: traces: sampler: type: parentbased_traceidratio arg: 0.1 # Sample 10%, but respect parent decisions This ensures distributed traces aren’t broken by different sampling decisions.\nExporters OTLP (Recommended) Send to any OTLP-compatible collector:\notel: traces: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf # or grpc Common Backends Jaeger:\notel: traces: exporter: otlp: endpoint: http://jaeger:4318 protocol: http/protobuf Grafana Tempo:\notel: traces: exporter: otlp: endpoint: https://tempo.example.com protocol: grpc headers: authorization: {{env \"GRAFANA_API_KEY\"}} Honeycomb:\notel: traces: exporter: otlp: endpoint: https://api.honeycomb.io protocol: grpc headers: x-honeycomb-team: {{env \"HONEYCOMB_API_KEY\"}} Cloud Providers:\n# AWS X-Ray (via OTLP) otel: traces: exporter: otlp: endpoint: localhost:4317 protocol: grpc # Google Cloud Trace otel: traces: exporter: otlp: endpoint: cloudtrace.googleapis.com:443 protocol: grpc Resource Attributes Add metadata to all telemetry:\notel: service: name: my-service version: 1.0.0 namespace: production resource: attributes: # Deployment info deployment.environment: production deployment.region: us-east-1 # Team info service.team: platform service.owner: team-platform@example.com # Kubernetes info (from env) k8s.pod.name: {{env \"POD_NAME\"}} k8s.namespace.name: {{env \"POD_NAMESPACE\"}} k8s.node.name: {{env \"NODE_NAME\"}} These attributes appear in all traces, metrics, and logs.\nBest Practices 1. Use Context Everywhere Always pass context.Context to propagate traces:\n// Good func processOrder(ctx context.Context, orderID string) error { result := validateOrder(ctx, orderID) // Context propagates trace return saveOrder(ctx, result) } // Bad - traces won't connect func processOrder(ctx context.Context, orderID string) error { result := validateOrder(context.Background(), orderID) // New trace! return saveOrder(ctx, result) } 2. Meaningful Span Names Use clear, hierarchical names:\n// Good tracer.Start(ctx, \"order.validate\") tracer.Start(ctx, \"payment.charge\") tracer.Start(ctx, \"inventory.reserve\") // Less useful tracer.Start(ctx, \"step1\") tracer.Start(ctx, \"process\") 3. Add Relevant Attributes Include contextual information:\nspan.SetAttributes( attribute.String(\"order.id\", orderID), attribute.String(\"user.id\", userID), attribute.Int(\"order.total_cents\", totalCents), attribute.String(\"payment.method\", \"credit_card\"), ) 4. Record Errors Always record errors in spans:\nif err := doWork(ctx); err != nil { span.RecordError(err) span.SetStatus(codes.Error, \"work failed\") return err } 5. Use Structured Logging Prefer structured fields over formatted strings:\n// Good log.InfoContext(ctx, \"order processed\", \"order_id\", orderID, \"duration_ms\", duration, ) // Less useful for querying log.InfoContext(ctx, fmt.Sprintf(\"Order %s processed in %dms\", orderID, duration)) Next Steps Learn about Lifecycle Management for service execution Explore REST Services for HTTP-specific instrumentation See gRPC Services for gRPC-specific instrumentation Read Advanced Topics for advanced OTel patterns ","categories":"","description":"OpenTelemetry integration for traces, metrics, and logs","excerpt":"OpenTelemetry integration for traces, metrics, and logs","ref":"/humus/pr-preview/pr-396/concepts/observability/","tags":"","title":"Observability"},{"body":"The Kafka runtime provides extensive configuration options for controlling consumer behavior, fetch settings, and topic processing.\nCreating a Runtime The basic runtime constructor:\nfunc NewRuntime( brokers []string, groupID string, opts ...Option, ) Runtime Parameters:\nbrokers - List of Kafka broker addresses (e.g., []string{\"localhost:9092\"}) groupID - Consumer group ID for offset management and rebalancing opts - Variadic configuration options Example:\nruntime := kafka.NewRuntime( []string{\"kafka1:9092\", \"kafka2:9092\", \"kafka3:9092\"}, \"my-consumer-group\", kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.SessionTimeout(10 * time.Second), ) Topic Configuration Configure which topics to consume and how to process them:\nAtLeastOnce Reliable processing with message acknowledgment after successful processing:\nfunc AtLeastOnce(topic string, processor queue.Processor[kafka.Message]) Option Example:\ntype OrderProcessor struct{} func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Process message (must be idempotent) return nil } runtime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", \u0026OrderProcessor{}), ) Guarantees:\nMessages acknowledged only after successful processing Failed processing results in redelivery Requires idempotent processors AtMostOnce Fast processing with message acknowledgment before processing:\nfunc AtMostOnce(topic string, processor queue.Processor[kafka.Message]) Option Example:\ntype MetricsProcessor struct{} func (p *MetricsProcessor) Process(ctx context.Context, msg kafka.Message) error { // Process message (may be lost on failure) return nil } runtime := kafka.NewRuntime( brokers, groupID, kafka.AtMostOnce(\"metrics\", \u0026MetricsProcessor{}), ) Guarantees:\nMessages acknowledged immediately after consumption Processing failures result in message loss Higher throughput Multiple Topics Process multiple topics with different semantics:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.AtLeastOnce(\"payments\", paymentsProcessor), kafka.AtMostOnce(\"metrics\", metricsProcessor), kafka.AtMostOnce(\"logs\", logsProcessor), ) Each topic gets its own processor and delivery semantics. See Multi-Topic Processing for details.\nConsumer Group Settings Configure consumer group behavior and rebalancing:\nSessionTimeout Maximum time between heartbeats before a consumer is considered dead:\nfunc SessionTimeout(d time.Duration) Option Default: 45 seconds\nExample:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.SessionTimeout(10 * time.Second), ) Guidelines:\nShort timeout (5-15s): Fast failure detection, but may cause false positives during GC pauses Long timeout (30-60s): Tolerates GC pauses, but slower failure detection Production recommendation: 20-30 seconds RebalanceTimeout Maximum time allowed for rebalance operations:\nfunc RebalanceTimeout(d time.Duration) Option Default: 30 seconds\nExample:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.RebalanceTimeout(60 * time.Second), ) Guidelines:\nShould be longer than session timeout Increase if rebalances frequently timeout Production recommendation: 45-60 seconds Fetch Settings Control how messages are fetched from Kafka:\nFetchMaxBytes Maximum total bytes to buffer from fetch responses across all partitions:\nfunc FetchMaxBytes(bytes int32) Option Default: 50 MB (52,428,800 bytes)\nExample:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.FetchMaxBytes(100 * 1024 * 1024), // 100 MB ) Guidelines:\nSmall messages: Lower value (10-25 MB) reduces memory usage Large messages: Higher value (100+ MB) improves throughput Must be larger than largest single message Production recommendation: 50-100 MB MaxConcurrentFetches Maximum number of concurrent fetch requests to Kafka:\nfunc MaxConcurrentFetches(fetches int) Option Default: Unlimited (0)\nExample:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.MaxConcurrentFetches(10), ) Guidelines:\nUnlimited (0): Maximum throughput, higher network load Limited (5-10): Controlled network load, predictable resource usage Production recommendation: 5-10 for most workloads Configuration Examples High-Throughput Configuration Optimize for maximum message throughput:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtMostOnce(\"events\", processor), kafka.FetchMaxBytes(200 * 1024 * 1024), // 200 MB kafka.MaxConcurrentFetches(0), // Unlimited kafka.SessionTimeout(45 * time.Second), kafka.RebalanceTimeout(60 * time.Second), ) Use cases:\nEvent streaming Log aggregation Metrics collection High-Reliability Configuration Optimize for message reliability and ordered processing:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"transactions\", processor), kafka.FetchMaxBytes(10 * 1024 * 1024), // 10 MB kafka.MaxConcurrentFetches(5), kafka.SessionTimeout(20 * time.Second), kafka.RebalanceTimeout(45 * time.Second), ) Use cases:\nFinancial transactions Database replication Critical event processing Balanced Configuration General-purpose configuration for most workloads:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.FetchMaxBytes(50 * 1024 * 1024), // 50 MB (default) kafka.MaxConcurrentFetches(10), kafka.SessionTimeout(30 * time.Second), kafka.RebalanceTimeout(45 * time.Second), ) Use cases:\nGeneral message processing Microservice communication Event-driven workflows Environment-Based Configuration Use YAML templating for environment-specific settings:\nconfig.yaml:\nkafka: brokers: - \"{{env \"KAFKA_BROKER_1\" | default \"localhost:9092\"}}\" - \"{{env \"KAFKA_BROKER_2\" | default \"localhost:9093\"}}\" group_id: \"{{env \"KAFKA_GROUP_ID\" | default \"my-service\"}}\" topic: \"{{env \"KAFKA_TOPIC\" | default \"events\"}}\" session_timeout: \"{{env \"KAFKA_SESSION_TIMEOUT\" | default \"30s\"}}\" fetch_max_bytes: {{env \"KAFKA_FETCH_MAX_BYTES\" | default \"52428800\"}} otel: service: name: \"{{env \"SERVICE_NAME\" | default \"queue-processor\"}}\" Parsing in code:\ntype Config struct { queue.Config `config:\",squash\"` Kafka struct { Brokers []string `config:\"brokers\"` GroupID string `config:\"group_id\"` Topic string `config:\"topic\"` SessionTimeout time.Duration `config:\"session_timeout\"` FetchMaxBytes int32 `config:\"fetch_max_bytes\"` } `config:\"kafka\"` } func Init(ctx context.Context, cfg Config) (*queue.App, error) { runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), kafka.SessionTimeout(cfg.Kafka.SessionTimeout), kafka.FetchMaxBytes(cfg.Kafka.FetchMaxBytes), ) return queue.NewApp(runtime), nil } Monitoring Configuration Check runtime behavior through logs and metrics:\nConsumer Group Lag:\nkafka-consumer-groups.sh \\ --bootstrap-server localhost:9092 \\ --group my-consumer-group \\ --describe Key metrics to monitor:\nConsumer lag per partition Messages processed per second Processing errors Rebalance frequency Session timeout violations See Observability for OpenTelemetry integration.\nCommon Configuration Issues Frequent Rebalances Symptoms: Consumer group frequently rebalancing\nSolutions:\n// Increase session and rebalance timeouts kafka.SessionTimeout(45 * time.Second), kafka.RebalanceTimeout(90 * time.Second), High Memory Usage Symptoms: Application consuming excessive memory\nSolutions:\n// Reduce fetch buffer size kafka.FetchMaxBytes(25 * 1024 * 1024), // 25 MB kafka.MaxConcurrentFetches(5), Slow Processing Symptoms: Consumer lag growing, messages processed slowly\nSolutions:\nCheck processor logic for inefficiencies Increase partition count for more concurrency Scale horizontally (more consumer instances) Consider at-most-once for non-critical messages Messages Larger Than FetchMaxBytes Symptoms: Errors fetching messages\nSolutions:\n// Increase fetch buffer kafka.FetchMaxBytes(200 * 1024 * 1024), // 200 MB // Or reduce message size at producer Next Steps Learn about Message Structure for working with message metadata Understand Concurrency Model for partition processing Explore Configuration for production deployment patterns ","categories":"","description":"Advanced Kafka runtime configuration","excerpt":"Advanced Kafka runtime configuration","ref":"/humus/pr-preview/pr-396/features/queue/kafka/runtime/","tags":"","title":"Runtime Configuration"},{"body":"In this guide, you’ll build a simple REST service that responds with “Hello, World!”. This will introduce you to the core concepts of Humus.\nProject Setup Create a new directory and initialize a Go module:\nmkdir hello-humus cd hello-humus go mod init hello-humus go get github.com/z5labs/humus Configuration File Create a config.yaml file in your project root:\nrest: port: 8080 otel: service: name: hello-humus sdk: disabled: true # Disable for this simple example This configuration:\nSets the HTTP server port to 8080 Names the service “hello-humus” Disables OpenTelemetry for simplicity (you’ll enable this in production) Application Code Create a main.go file:\npackage main import ( \"context\" \"net/http\" \"github.com/z5labs/humus/rest\" ) // Config embeds rest.Config to get HTTP server configuration type Config struct { rest.Config `config:\",squash\"` } func main() { // rest.Run handles configuration loading, app initialization, and execution rest.Run(rest.YamlSource(\"config.yaml\"), Init) } // Init is called with the loaded configuration and returns the API func Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Create a simple handler that returns \"Hello, World!\" handler := rest.ProducerFunc[string](func(ctx context.Context) (*string, error) { msg := \"Hello, World!\" return \u0026msg, nil }) // Create a new API with name and version and register the handler at GET /hello api := rest.NewApi( \"Hello Service\", \"1.0.0\", rest.Handle( http.MethodGet, rest.BasePath(\"/hello\"), rest.ProduceJson(handler), ), ) return api, nil } Running the Service Run your service:\ngo run main.go You should see output indicating the server has started. The service is now running on http://localhost:8080.\nTesting the Endpoint In another terminal, test your endpoint:\ncurl http://localhost:8080/hello You should see:\n\"Hello, World!\" Exploring Built-in Endpoints Humus automatically provides several endpoints:\nOpenAPI Specification curl http://localhost:8080/openapi.json This returns the OpenAPI 3.0 specification for your API, automatically generated from your code.\nHealth Checks # Liveness probe curl http://localhost:8080/health/liveness # Readiness probe curl http://localhost:8080/health/readiness Both should return 200 OK with {\"healthy\":true}.\nUnderstanding the Code Let’s break down what’s happening:\nConfiguration: The Config struct embeds rest.Config, which provides HTTP server configuration fields that are automatically populated from config.yaml.\nrest.Run(): This function orchestrates the entire application lifecycle:\nReads configuration from the YAML file Calls Init() with the parsed configuration Starts the HTTP server Handles graceful shutdown on OS signals Init Function: This is where you build your API:\nCreate an Api instance with a name and version Define handlers for your endpoints Register handlers with HTTP methods and paths Return the configured API Handler Pattern: The rest.ProducerFunc creates a type-safe handler. In this example, it produces a string response with no request body.\nNext Steps Now that you have a working service, you can:\nLearn about Configuration to customize your service Explore Project Structure for organizing larger applications Read about REST Services for more advanced HTTP patterns Understand Core Concepts for a deeper dive into Humus architecture Complete Example The complete code for this example is available in the Humus repository.\n","categories":"","description":"Build a Hello World REST service with Humus","excerpt":"Build a Hello World REST service with Humus","ref":"/humus/pr-preview/pr-396/getting-started/first-service/","tags":"","title":"Your First Service"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/advanced/builder-runner-pattern/","tags":"","title":"Builder Runner Pattern"},{"body":"Humus uses YAML-based configuration with Go template support. This provides a flexible, environment-aware configuration system.\nBasic Configuration A minimal configuration file looks like this:\nrest: port: 8080 otel: service: name: my-service Configuration Structure Service Type Sections Each service type has its own configuration section:\nREST Services:\nrest: port: 8080 host: localhost # optional, defaults to all interfaces gRPC Services:\ngrpc: port: 9090 host: localhost # optional Job Services:\n# Jobs don't need server configuration # Just OTel and app-specific config OpenTelemetry Configuration The otel section configures observability:\notel: service: name: my-service version: 1.0.0 # optional sdk: disabled: false # Set to true to disable OTel entirely traces: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf metrics: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf logs: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf Go Template Support Configuration files support Go template syntax for dynamic values:\nEnvironment Variables Use the env function to read environment variables:\notel: service: name: {{env \"SERVICE_NAME\"}} rest: port: {{env \"PORT\"}} Default Values Use the default function to provide fallbacks:\notel: service: name: {{env \"SERVICE_NAME\" | default \"my-service\"}} rest: port: {{env \"PORT\" | default \"8080\"}} Complete Example rest: port: {{env \"HTTP_PORT\" | default \"8080\"}} host: {{env \"HTTP_HOST\" | default \"0.0.0.0\"}} otel: service: name: {{env \"OTEL_SERVICE_NAME\" | default \"my-service\"}} version: {{env \"APP_VERSION\" | default \"dev\"}} sdk: disabled: {{env \"OTEL_DISABLED\" | default \"false\"}} traces: exporter: otlp: endpoint: {{env \"OTEL_EXPORTER_OTLP_ENDPOINT\" | default \"http://localhost:4318\"}} Configuration in Code Basic Config Struct Embed the appropriate config type for your service:\ntype Config struct { rest.Config `config:\",squash\"` // For REST services // Add your custom config fields here } For gRPC:\ntype Config struct { grpc.Config `config:\",squash\"` } For Jobs:\ntype Config struct { humus.Config `config:\",squash\"` // Base OTel config only } Custom Configuration Fields Add your own configuration fields using struct tags:\ntype Config struct { rest.Config `config:\",squash\"` Database struct { Host string `config:\"host\"` Port int `config:\"port\"` Name string `config:\"name\"` } `config:\"database\"` Features struct { EnableCache bool `config:\"enable_cache\"` } `config:\"features\"` } Corresponding YAML:\nrest: port: 8080 database: host: localhost port: 5432 name: mydb features: enable_cache: true Configuration Sources YAML File The most common source:\nrest.Run(rest.YamlSource(\"config.yaml\"), Init) Multiple Sources Use bedrockcfg.MultiSource to compose configurations:\nimport ( \"github.com/z5labs/bedrock/pkg/config\" bedrockcfg \"github.com/z5labs/bedrock/pkg/config\" ) func main() { source := bedrockcfg.MultiSource( bedrockcfg.FromYaml(\"default_config.yaml\"), // Defaults bedrockcfg.FromYaml(\"config.yaml\"), // Overrides ) rest.Run(source, Init) } Environment-Specific Configs import \"os\" func main() { env := os.Getenv(\"ENV\") if env == \"\" { env = \"dev\" } configFile := fmt.Sprintf(\"config.%s.yaml\", env) rest.Run(rest.YamlSource(configFile), Init) } This allows you to have:\nconfig.dev.yaml config.staging.yaml config.prod.yaml Default Configuration Humus includes a default_config.yaml with sensible defaults for OpenTelemetry. You can compose this with your config:\nsource := bedrockcfg.MultiSource( bedrockcfg.FromYaml(\"default_config.yaml\"), // Framework defaults bedrockcfg.FromYaml(\"config.yaml\"), // Your overrides ) Best Practices Use Environment Variables for Secrets: Never commit credentials to YAML files. Use env function:\ndatabase: password: {{env \"DB_PASSWORD\"}} Provide Defaults: Always use default with env for non-secret values:\nport: {{env \"PORT\" | default \"8080\"}} Separate Environments: Use different config files or environment variables for dev/staging/prod.\nDocument Your Config: Add comments to your YAML files explaining each section.\nValidate Early: Use the Init function to validate configuration:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { if cfg.Database.Host == \"\" { return nil, fmt.Errorf(\"database host is required\") } // ... } Next Steps Learn about Project Structure for organizing your config files Explore Core Concepts for advanced configuration patterns See Observability for OTel configuration details ","categories":"","description":"Understanding the YAML configuration system","excerpt":"Understanding the YAML configuration system","ref":"/humus/pr-preview/pr-396/getting-started/configuration/","tags":"","title":"Configuration"},{"body":"Documentation coming soon.\nSee Contributing Overview for general contribution guidelines.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Contributing Overview for general …","ref":"/humus/pr-preview/pr-396/contributing/development-setup/","tags":"","title":"Development Setup"},{"body":"Documentation coming soon.\nSee GitHub Discussions for community support.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee GitHub Discussions for community …","ref":"/humus/pr-preview/pr-396/faq/faq/","tags":"","title":"Faq"},{"body":"Documentation coming soon.\nSee Getting Started for building new Humus services.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Getting Started for building new Humus …","ref":"/humus/pr-preview/pr-396/integration/from-vanilla-go/","tags":"","title":"From Vanilla Go"},{"body":"Humus provides handler helpers that simplify common REST API patterns by combining type-safe request/response handling with automatic serialization and OpenAPI schema generation.\nOverview The REST package provides three categories of handler helpers:\nJSON Handlers - For endpoints that consume and/or produce JSON Producer/Consumer Patterns - For endpoints with only request or only response Low-level Wrappers - For building custom serialization formats Core Interfaces All handler helpers build on these core interfaces:\n// Handler processes a request and returns a response type Handler[Req, Resp any] interface { Handle(context.Context, *Req) (*Resp, error) } // Producer returns a response without consuming a request type Producer[T any] interface { Produce(context.Context) (*T, error) } // Consumer consumes a request without returning a response type Consumer[T any] interface { Consume(context.Context, *T) error } JSON Handlers The most common handlers work with JSON payloads. These provide automatic serialization, content-type validation, and OpenAPI schema generation.\nHandleJson - Full Request and Response Use HandleJson when your endpoint consumes and produces JSON:\n// Define your handler logic handler := rest.HandlerFunc[CreateUserRequest, User]( func(ctx context.Context, req *CreateUserRequest) (*User, error) { user := \u0026User{ ID: generateID(), Name: req.Name, Email: req.Email, } return user, nil }, ) // Wrap with JSON serialization rest.Handle( http.MethodPost, rest.BasePath(\"/users\"), rest.HandleJson(handler), ) What happens automatically:\nRequest body parsed as JSON Content-Type validation (requires application/json) Response serialized as JSON with Content-Type: application/json OpenAPI request/response schemas generated from types OpenAPI Output:\n{ \"requestBody\": { \"required\": true, \"content\": { \"application/json\": { \"schema\": { \"$ref\": \"#/components/schemas/CreateUserRequest\" } } } }, \"responses\": { \"200\": { \"content\": { \"application/json\": { \"schema\": { \"$ref\": \"#/components/schemas/User\" } } } } } } ProduceJson - GET Endpoints Use ProduceJson for GET endpoints that return data without consuming a request body:\n// Define your producer producer := rest.ProducerFunc[[]User]( func(ctx context.Context) (*[]User, error) { users := getUsersFromDB(ctx) return \u0026users, nil }, ) // Wrap with JSON serialization rest.Handle( http.MethodGet, rest.BasePath(\"/users\"), rest.ProduceJson(producer), ) What happens automatically:\nNo request body parsing Response serialized as JSON OpenAPI response schema generated Accessing Path/Query Parameters:\nproducer := rest.ProducerFunc[User]( func(ctx context.Context) (*User, error) { // Extract parameters from context userID := rest.PathParamValue(ctx, \"id\") include := rest.QueryParamValue(ctx, \"include\") user := getUserByID(ctx, userID, include) return user, nil }, ) rest.Handle( http.MethodGet, rest.BasePath(\"/users\").Param(\"id\"), rest.ProduceJson(producer), rest.QueryParam(\"include\"), ) ConsumeOnlyJson - Webhook Endpoints Use ConsumeOnlyJson for POST/PUT webhooks that process data but don’t return content:\n// Define your consumer consumer := rest.ConsumerFunc[WebhookPayload]( func(ctx context.Context, payload *WebhookPayload) error { processWebhook(ctx, payload) return nil }, ) // Wrap with JSON deserialization rest.Handle( http.MethodPost, rest.BasePath(\"/webhooks/github\"), rest.ConsumeOnlyJson(consumer), ) What happens automatically:\nRequest body parsed as JSON Content-Type validation Returns 200 OK with empty body on success Returns appropriate error status on failure Response behavior:\nPOST /webhooks/github HTTP/1.1 Content-Type: application/json {\"event\": \"push\", \"repository\": \"myrepo\"} HTTP/1.1 200 OK Content-Length: 0 Function Adapters Handler helpers provide function adapters for inline handler definitions:\nHandlerFunc Convert a function to a Handler:\nhandler := rest.HandlerFunc[Request, Response]( func(ctx context.Context, req *Request) (*Response, error) { // Process request and return response return \u0026Response{}, nil }, ) ProducerFunc Convert a function to a Producer:\nproducer := rest.ProducerFunc[Response]( func(ctx context.Context) (*Response, error) { // Generate and return response return \u0026Response{}, nil }, ) ConsumerFunc Convert a function to a Consumer:\nconsumer := rest.ConsumerFunc[Request]( func(ctx context.Context, req *Request) error { // Process request return nil }, ) Composition Patterns Handler helpers are designed to compose together, allowing you to build complex handlers from simple pieces.\nAdding Custom Middleware Wrap handlers with additional behavior:\n// Base handler baseHandler := rest.HandlerFunc[Request, Response](businessLogic) // Add validation layer validatingHandler := rest.HandlerFunc[Request, Response]( func(ctx context.Context, req *Request) (*Response, error) { if err := validateRequest(req); err != nil { return nil, err } return baseHandler.Handle(ctx, req) }, ) // Wrap with JSON serialization rest.Handle( http.MethodPost, rest.BasePath(\"/api\"), rest.HandleJson(validatingHandler), ) Transforming Responses Chain transformations before serialization:\n// Handler returns internal type handler := rest.HandlerFunc[Request, InternalResponse](getInternalData) // Transform to API response transformer := rest.HandlerFunc[Request, ApiResponse]( func(ctx context.Context, req *Request) (*ApiResponse, error) { internal, err := handler.Handle(ctx, req) if err != nil { return nil, err } return toApiResponse(internal), nil }, ) rest.Handle( http.MethodPost, rest.BasePath(\"/api\"), rest.HandleJson(transformer), ) Low-Level Building Blocks For custom serialization formats, use the underlying wrappers directly.\nConsumeJson - Custom Request Deserialization Wrap any handler to consume JSON requests:\nhandler := rest.HandlerFunc[MyRequest, MyResponse](businessLogic) // Add JSON request deserialization jsonHandler := rest.ConsumeJson(handler) rest.Handle( http.MethodPost, rest.BasePath(\"/api\"), jsonHandler, ) ReturnJson - Custom Response Serialization Wrap any handler to return JSON responses:\nhandler := rest.HandlerFunc[MyRequest, MyResponse](businessLogic) // Add JSON response serialization jsonHandler := rest.ReturnJson(handler) rest.Handle( http.MethodPost, rest.BasePath(\"/api\"), jsonHandler, ) ConsumeNothing and ProduceNothing Build handlers without request or response bodies:\n// Producer - generates response without request body producer := rest.ProducerFunc[Response](generateData) handler := rest.ConsumeNothing(producer) // Consumer - processes request without response body consumer := rest.ConsumerFunc[Request](processData) handler := rest.ProduceNothing(consumer) Advanced Patterns Conditional Response Types Return different response types based on business logic:\nhandler := rest.HandlerFunc[Request, Response]( func(ctx context.Context, req *Request) (*Response, error) { // Return different status codes via custom error types if !isAuthorized(ctx) { return nil, rest.UnauthorizedError{ Cause: errors.New(\"invalid credentials\"), } } if !exists(req.ID) { return nil, rest.NotFoundError{ Cause: errors.New(\"resource not found\"), } } return \u0026Response{Data: getData(req.ID)}, nil }, ) See Error Handling for complete error handling patterns.\nStreaming Responses For streaming responses, implement custom TypedResponse:\ntype StreamingResponse struct { data chan []byte } func (sr *StreamingResponse) WriteResponse(ctx context.Context, w http.ResponseWriter) error { w.Header().Set(\"Content-Type\", \"application/x-ndjson\") w.WriteHeader(http.StatusOK) for data := range sr.data { if _, err := w.Write(data); err != nil { return err } if f, ok := w.(http.Flusher); ok { f.Flush() } } return nil } func (sr *StreamingResponse) Spec() (int, openapi3.ResponseOrRef, error) { // Define OpenAPI spec for streaming response return http.StatusOK, openapi3.ResponseOrRef{}, nil } Custom Content Types Implement handlers for other content types:\n// XML request type type XMLRequest[T any] struct { inner T } func (xr *XMLRequest[T]) ReadRequest(ctx context.Context, r *http.Request) error { contentType := r.Header.Get(\"Content-Type\") if contentType != \"application/xml\" { return rest.BadRequestError{ Cause: rest.InvalidContentTypeError{ ContentType: contentType, }, } } dec := xml.NewDecoder(r.Body) return dec.Decode(\u0026xr.inner) } func (xr *XMLRequest[T]) Spec() (openapi3.RequestBodyOrRef, error) { // Define OpenAPI spec for XML request return openapi3.RequestBodyOrRef{}, nil } Complete Example Putting it all together in a CRUD API:\ntype UserStore interface { Create(ctx context.Context, user User) error Get(ctx context.Context, id string) (*User, error) List(ctx context.Context) ([]User, error) Update(ctx context.Context, user User) error Delete(ctx context.Context, id string) error } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { store := NewUserStore() // POST /users - Create user createHandler := rest.HandlerFunc[CreateUserRequest, User]( func(ctx context.Context, req *CreateUserRequest) (*User, error) { user := User{ ID: generateID(), Name: req.Name, Email: req.Email, } if err := store.Create(ctx, user); err != nil { return nil, err } return \u0026user, nil }, ) // GET /users - List all users listProducer := rest.ProducerFunc[[]User]( func(ctx context.Context) (*[]User, error) { users, err := store.List(ctx) return \u0026users, err }, ) // GET /users/{id} - Get single user getProducer := rest.ProducerFunc[User]( func(ctx context.Context) (*User, error) { id := rest.PathParamValue(ctx, \"id\") return store.Get(ctx, id) }, ) // PUT /users/{id} - Update user updateHandler := rest.HandlerFunc[UpdateUserRequest, User]( func(ctx context.Context, req *UpdateUserRequest) (*User, error) { id := rest.PathParamValue(ctx, \"id\") user := User{ ID: id, Name: req.Name, Email: req.Email, } if err := store.Update(ctx, user); err != nil { return nil, err } return \u0026user, nil }, ) // DELETE /users/{id} - Delete user deleteConsumer := rest.ConsumerFunc[struct{}]( func(ctx context.Context, _ *struct{}) error { id := rest.PathParamValue(ctx, \"id\") return store.Delete(ctx, id) }, ) api := rest.NewApi( \"User API\", \"1.0.0\", rest.Handle(http.MethodPost, rest.BasePath(\"/users\"), rest.HandleJson(createHandler)), rest.Handle(http.MethodGet, rest.BasePath(\"/users\"), rest.ProduceJson(listProducer)), rest.Handle(http.MethodGet, rest.BasePath(\"/users\").Param(\"id\"), rest.ProduceJson(getProducer)), rest.Handle(http.MethodPut, rest.BasePath(\"/users\").Param(\"id\"), rest.HandleJson(updateHandler)), rest.Handle(http.MethodDelete, rest.BasePath(\"/users\").Param(\"id\"), rest.ConsumeOnlyJson(deleteConsumer)), ) return api, nil } Best Practices 1. Choose the Right Helper Match the helper to your endpoint’s behavior:\n// GET endpoints - ProduceJson rest.ProduceJson(producer) // Webhooks - ConsumeOnlyJson rest.ConsumeOnlyJson(consumer) // Full CRUD operations - HandleJson rest.HandleJson(handler) 2. Keep Handlers Focused Each handler should have a single responsibility:\n// Good - focused handler handler := rest.HandlerFunc[CreateUserRequest, User](createUser) // Avoid - handler doing too much handler := rest.HandlerFunc[Request, Response]( func(ctx context.Context, req *Request) (*Response, error) { validate(req) // Should be middleware log(req) // Should be middleware transform(req) // Should be separate transformer return process(req), nil }, ) 3. Use Type Parameters Effectively Define clear request/response types:\n// Good - explicit types type CreateUserRequest struct { Name string `json:\"name\"` Email string `json:\"email\"` } type UserResponse struct { ID string `json:\"id\"` Name string `json:\"name\"` Email string `json:\"email\"` CreatedAt time.Time `json:\"created_at\"` } // Avoid - using map[string]interface{} handler := rest.HandlerFunc[map[string]interface{}, map[string]interface{}](...) 4. Leverage Function Adapters Use function adapters for inline handlers:\n// Good - inline with adapter rest.ProduceJson(rest.ProducerFunc[Response]( func(ctx context.Context) (*Response, error) { return \u0026Response{}, nil }, )) // Verbose - defining separate type type MyProducer struct{} func (p *MyProducer) Produce(ctx context.Context) (*Response, error) { return \u0026Response{}, nil } rest.ProduceJson(\u0026MyProducer{}) 5. Document with JSON Tags Use JSON tags to control serialization and OpenAPI schema generation:\ntype User struct { ID string `json:\"id\"` // Required field Name string `json:\"name\"` // Required field Email string `json:\"email,omitempty\"` // Optional field Internal string `json:\"-\"` // Excluded from JSON CreatedAt time.Time `json:\"created_at\"` // Snake case in JSON } Performance Considerations Request Body Size JSON handlers buffer the entire request body in memory. For large uploads, consider:\n// Set max request size at the HTTP server level config := rest.Config{ HTTP: rest.HTTPConfig{ MaxRequestBodySize: 10 * 1024 * 1024, // 10 MB }, } Response Streaming For large responses, use custom streaming responses instead of buffering:\n// Avoid - buffers entire response type LargeResponse struct { Data []LargeItem `json:\"data\"` // Could be GBs } // Prefer - streams data type StreamingHandler struct{} func (h *StreamingHandler) Handle(ctx context.Context, req *Request) (*StreamingResponse, error) { return \u0026StreamingResponse{ data: generateDataStream(ctx), }, nil } Troubleshooting Content-Type Errors If you see 400 Bad Request with “invalid content type”:\n{ \"error\": \"invalid content type: text/plain\" } Ensure your client sends Content-Type: application/json:\ncurl -X POST http://localhost:8080/api \\ -H \"Content-Type: application/json\" \\ -d '{\"key\": \"value\"}' JSON Parsing Errors If you see JSON unmarshal errors, verify your request structure matches the type:\n// Handler expects type Request struct { Name string `json:\"name\"` } // Client must send {\"name\": \"value\"} // Not {\"Name\": \"value\"} // Wrong - Go field name instead of JSON tag Empty Response Body ConsumeOnlyJson returns an empty response body by design:\n// This is correct behavior rest.ConsumeOnlyJson(consumer) // Returns: 200 OK with empty body // To return data, use HandleJson instead rest.HandleJson(handler) // Returns: 200 OK with JSON response Next Steps Learn about Error Handling for custom error responses Explore Routing for path parameters and query strings See OpenAPI for customizing generated schemas Read Authentication for securing endpoints ","categories":"","description":"Type-safe handler creation with built-in serialization","excerpt":"Type-safe handler creation with built-in serialization","ref":"/humus/pr-preview/pr-396/features/rest/handler-helpers/","tags":"","title":"Handler Helpers"},{"body":"Humus Job services provide a framework for building one-off executors, batch processors, database migrations, and scheduled tasks with the same observability and lifecycle management as long-running services.\nOverview Job services in Humus are built on:\nSimple Handler Interface - Single Handle(context.Context) error method Full Observability - Same OpenTelemetry support as REST/gRPC Lifecycle Management - Graceful shutdown and context cancellation Exit Code Handling - Proper success/failure signaling Quick Start package main import ( \"context\" \"fmt\" \"github.com/z5labs/humus\" \"github.com/z5labs/humus/job\" ) type Config struct { humus.Config `config:\",squash\"` Database struct { Host string `config:\"host\"` Name string `config:\"name\"` } `config:\"database\"` } type MigrationJob struct { dbHost string dbName string } func (j *MigrationJob) Handle(ctx context.Context) error { log := humus.Logger(\"migration\") log.InfoContext(ctx, \"starting migration\", \"host\", j.dbHost, \"database\", j.dbName, ) // Run your migration logic if err := runMigrations(ctx, j.dbHost, j.dbName); err != nil { log.ErrorContext(ctx, \"migration failed\", \"error\", err) return err } log.InfoContext(ctx, \"migration completed successfully\") return nil } func main() { job.Run(job.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (job.Handler, error) { return \u0026MigrationJob{ dbHost: cfg.Database.Host, dbName: cfg.Database.Name, }, nil } func runMigrations(ctx context.Context, host, name string) error { // Your migration logic here return nil } The Handler Interface type Handler interface { Handle(context.Context) error } That’s it! Just implement one method.\nLifecycle Jobs have a simple lifecycle:\nConfiguration Loading - Config file is parsed Initialization - Init function creates the handler Execution - Handle method is called Exit: Returns nil → Exit code 0 (success) Returns error → Exit code 1 (failure) Receives SIGTERM/SIGINT → Context cancelled, job should return Use Cases Database Migrations type MigrationJob struct { db *sql.DB } func (j *MigrationJob) Handle(ctx context.Context) error { migrations := []string{ \"CREATE TABLE users...\", \"CREATE INDEX idx_users_email...\", } for i, migration := range migrations { select { case \u003c-ctx.Done(): return ctx.Err() default: log.InfoContext(ctx, \"running migration\", \"step\", i+1) if _, err := j.db.ExecContext(ctx, migration); err != nil { return fmt.Errorf(\"migration %d failed: %w\", i+1, err) } } } return nil } Batch Processing type BatchProcessor struct { source Source dest Destination } func (j *BatchProcessor) Handle(ctx context.Context) error { items, err := j.source.FetchAll(ctx) if err != nil { return err } for i, item := range items { select { case \u003c-ctx.Done(): log.InfoContext(ctx, \"shutdown requested\", \"processed\", i) return ctx.Err() default: if err := j.dest.Write(ctx, item); err != nil { return fmt.Errorf(\"failed at item %d: %w\", i, err) } } } return nil } Data Import type ImportJob struct { filePath string db *Database } func (j *ImportJob) Handle(ctx context.Context) error { file, err := os.Open(j.filePath) if err != nil { return err } defer file.Close() scanner := bufio.NewScanner(file) lineNum := 0 for scanner.Scan() { lineNum++ select { case \u003c-ctx.Done(): log.InfoContext(ctx, \"import cancelled\", \"lines_processed\", lineNum) return ctx.Err() default: if err := j.db.Insert(ctx, scanner.Text()); err != nil { return fmt.Errorf(\"line %d: %w\", lineNum, err) } } } return scanner.Err() } Scheduled Task (with external scheduler) Jobs are designed to run once. Use an external scheduler (cron, Kubernetes CronJob, etc.) to run them periodically:\n# Kubernetes CronJob example apiVersion: batch/v1 kind: CronJob metadata: name: daily-report spec: schedule: \"0 2 * * *\" # 2 AM daily jobTemplate: spec: template: spec: containers: - name: report-job image: my-report-job:latest restartPolicy: OnFailure Context Handling Always respect context cancellation:\nfunc (j *MyJob) Handle(ctx context.Context) error { for i := 0; i \u003c 1000; i++ { // Check for shutdown before each iteration select { case \u003c-ctx.Done(): log.InfoContext(ctx, \"graceful shutdown\", \"progress\", i) return ctx.Err() default: processItem(ctx, i) } } return nil } Error Handling Return errors for failures:\nfunc (j *MyJob) Handle(ctx context.Context) error { if err := validateInput(); err != nil { return fmt.Errorf(\"validation failed: %w\", err) } if err := processData(ctx); err != nil { return fmt.Errorf(\"processing failed: %w\", err) } return nil // Success } The job framework will:\nLog the error Exit with code 1 Ensure proper cleanup Configuration Minimal config for jobs:\notel: service: name: my-job sdk: disabled: false # Enable for production # Add your job-specific config database: host: localhost port: 5432 What You’ll Learn This section covers:\nQuick Start - Build your first job Job Handler - Implementing the interface Use Cases - Common patterns and examples Next Steps Start with the Quick Start Guide to build your first job service.\n","categories":"","description":"Building batch processors and one-off tasks","excerpt":"Building batch processors and one-off tasks","ref":"/humus/pr-preview/pr-396/features/job/","tags":"","title":"Job Services"},{"body":"Humus provides automatic lifecycle management for all service types, including graceful shutdown, panic recovery, and OS signal handling.\nService Lifecycle Every Humus service follows this lifecycle:\n1. Configuration Loading ↓ 2. Initialization (Init function) ↓ 3. Service Startup ↓ 4. Running (handling requests/jobs) ↓ 5. Shutdown Signal Received ↓ 6. Graceful Shutdown ↓ 7. Cleanup \u0026 Exit Lifecycle Phases 1. Configuration Loading Before your code runs, Humus loads configuration from the specified source:\nfunc main() { // Configuration is loaded here rest.Run(rest.YamlSource(\"config.yaml\"), Init) } If configuration loading fails, the service exits with an error.\n2. Initialization Your Init function is called with the loaded configuration:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Set up resources db, err := connectDatabase(ctx, cfg.Database) if err != nil { return nil, fmt.Errorf(\"failed to connect to database: %w\", err) } // Build the API api := rest.NewApi(\"My Service\", \"1.0.0\") // Register handlers rest.Handle(http.MethodGet, rest.BasePath(\"/users\"), userHandler) return api, nil } If Init returns an error, the service exits without starting.\nThe context passed to Init:\nContains trace context for instrumentation Is NOT cancelled when shutdown begins Should be used for initialization operations that need context 3. Service Startup After successful initialization, Humus starts the service:\nREST: HTTP server starts listening on configured port gRPC: gRPC server starts listening on configured port Job: Job handler begins execution 4. Running The service handles requests or executes jobs:\nREST/gRPC: Servers handle incoming requests Job: Handler executes once, then service waits for shutdown signal 5. Shutdown Signal Humus listens for OS signals:\nSIGINT (Ctrl+C) SIGTERM (Docker/Kubernetes termination) When received, graceful shutdown begins.\n6. Graceful Shutdown Humus gracefully shuts down the service:\nFor REST/gRPC:\nStop accepting new connections Wait for in-flight requests to complete (with timeout) Close the server For Jobs:\nCancel the job context Wait for job to return (handler should respect context cancellation) 7. Cleanup \u0026 Exit After shutdown completes, the service exits with appropriate status code.\nShutdown Handling Automatic Graceful Shutdown Shutdown is automatic - no code needed:\nfunc main() { // Graceful shutdown is built-in rest.Run(rest.YamlSource(\"config.yaml\"), Init) } When SIGTERM/SIGINT is received:\nServer stops accepting new connections Existing requests are allowed to complete Server shuts down after all requests finish (or timeout) Context Cancellation For long-running operations, respect context cancellation:\nfunc processJob(ctx context.Context) error { for { select { case \u003c-ctx.Done(): // Context was cancelled (shutdown signal received) log.Info(\"shutting down gracefully\") return ctx.Err() default: // Do work if err := processNextItem(ctx); err != nil { return err } } } } Cleanup Resources Clean up in your handlers when context is cancelled:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { db, err := connectDatabase(ctx, cfg.Database) if err != nil { return nil, err } handler := rest.HandlerFunc[Request, Response](func(ctx context.Context, req *Request) (*Response, error) { // Handler automatically respects context cancellation resp, err := processRequest(ctx, db, req) return \u0026resp, err }) api := rest.NewApi( \"My Service\", \"1.0.0\", rest.Handle(http.MethodPost, rest.BasePath(\"/process\"), rest.HandleJson(handler)), ) return api, nil } // No explicit cleanup needed - Humus handles server shutdown For resources that need explicit cleanup, use the handler’s context:\ntype JobHandler struct { db *sql.DB } func (h *JobHandler) Handle(ctx context.Context) error { // Job will be cancelled when shutdown signal received defer h.db.Close() return processWithDatabase(ctx, h.db) } Panic Recovery Humus automatically recovers from panics in handlers:\nfunc handleRequest(ctx context.Context, req Request) (Response, error) { // If this panics, Humus recovers and returns 500 panic(\"something went wrong\") } What happens:\nPanic is caught Error is logged with stack trace For REST: HTTP 500 response sent For gRPC: Internal error status returned Service continues running (not crashed) Don’t rely on panic recovery:\nUse proper error handling with error returns Panic recovery is a safety net, not a pattern Job Lifecycle Jobs have a simpler lifecycle:\ntype MyJob struct{} func (j *MyJob) Handle(ctx context.Context) error { // Job starts executing immediately log.InfoContext(ctx, \"job started\") // Do work... if err := doWork(ctx); err != nil { return err // Job fails, service exits with error } log.InfoContext(ctx, \"job completed\") return nil // Job succeeds, service exits cleanly } func main() { job.Run(job.YamlSource(\"config.yaml\"), func(ctx context.Context, cfg Config) (job.Handler, error) { return \u0026MyJob{}, nil }) } Job execution:\nJob starts immediately after initialization Context is valid until job returns OR shutdown signal received If job returns nil, service exits with code 0 If job returns error, service exits with code 1 If shutdown signal received, context is cancelled Respecting shutdown in jobs:\nfunc (j *MyJob) Handle(ctx context.Context) error { items, err := fetchItems(ctx) if err != nil { return err } for _, item := range items { // Check if shutdown was requested select { case \u003c-ctx.Done(): log.InfoContext(ctx, \"shutdown requested, stopping job\") return ctx.Err() default: // Process item if err := processItem(ctx, item); err != nil { return err } } } return nil } Timeouts Shutdown Timeout REST and gRPC servers have default shutdown timeouts. If requests don’t complete in time, the server forcefully shuts down.\nThis is managed by Bedrock and typically doesn’t need configuration.\nRequest Timeouts For long-running requests, implement your own timeouts:\nfunc handleRequest(ctx context.Context, req Request) (Response, error) { // Create timeout context ctx, cancel := context.WithTimeout(ctx, 30*time.Second) defer cancel() // This will fail if it takes \u003e 30 seconds return processRequest(ctx, req) } Job Timeouts Jobs can implement their own timeouts:\nfunc (j *MyJob) Handle(ctx context.Context) error { // Set maximum job duration ctx, cancel := context.WithTimeout(ctx, 1*time.Hour) defer cancel() return processJob(ctx) } Health During Lifecycle REST services provide health endpoints that reflect lifecycle state:\nDuring Initialization:\nLiveness: Not ready (server hasn’t started) Readiness: Not ready During Normal Operation:\nLiveness: Healthy Readiness: Healthy (unless custom health check fails) During Shutdown:\nLiveness: Healthy (but server is shutting down) Readiness: Unhealthy (stops receiving traffic) See REST Health Checks for details.\nBest Practices 1. Fast Initialization Keep Init function fast:\n// Good - quick setup func Init(ctx context.Context, cfg Config) (*rest.Api, error) { db := newDatabaseClient(cfg.Database) // Just create client api := rest.NewApi(\"My Service\", \"1.0.0\") // Register handlers... return api, nil } // Avoid - slow startup func Init(ctx context.Context, cfg Config) (*rest.Api, error) { db := newDatabaseClient(cfg.Database) if err := db.Migrate(); err != nil { // Don't run migrations here! return nil, err } // ... } Run migrations as a separate job service.\n2. Respect Context Cancellation Always check context in loops:\n// Good func processItems(ctx context.Context, items []Item) error { for _, item := range items { select { case \u003c-ctx.Done(): return ctx.Err() default: process(item) } } return nil } // Bad - ignores shutdown func processItems(ctx context.Context, items []Item) error { for _, item := range items { process(item) // Won't stop on shutdown! } return nil } 3. Proper Error Handling Return errors from Init for startup failures:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { db, err := connectDatabase(ctx, cfg.Database) if err != nil { return nil, fmt.Errorf(\"database connection failed: %w\", err) } api := rest.NewApi(\"My Service\", \"1.0.0\") // ... return api, nil } This ensures the service doesn’t start in a broken state.\n4. Resource Cleanup For most resources, cleanup is automatic:\nHTTP/gRPC servers are closed by Humus Contexts are cancelled on shutdown For resources that need explicit cleanup (database connections, file handles), either:\nOption 1: Defer in handlers\nfunc (h *Handler) Handle(ctx context.Context) error { defer h.cleanup() return h.process(ctx) } Option 2: Use finalizers (advanced)\nFor advanced lifecycle hooks, use Bedrock’s lifecycle management directly. See Advanced Topics.\n5. Don’t Block Shutdown Avoid operations that might block shutdown:\n// Bad - might block shutdown indefinitely func processJob(ctx context.Context) error { for { item := blockingQueue.Get() // Blocks forever! process(item) } } // Good - respects cancellation func processJob(ctx context.Context) error { for { select { case \u003c-ctx.Done(): return ctx.Err() case item := \u003c-queue: process(item) } } } Next Steps Learn about REST Health Checks for monitoring service health Explore Advanced Topics for custom lifecycle hooks See Job Services for job-specific lifecycle patterns ","categories":"","description":"Graceful shutdown and signal handling","excerpt":"Graceful shutdown and signal handling","ref":"/humus/pr-preview/pr-396/concepts/lifecycle-management/","tags":"","title":"Lifecycle Management"},{"body":"Kafka messages in Humus are represented by the kafka.Message type, which provides access to all message data and metadata.\nMessage Type type Message struct { Key []byte Value []byte Headers []Header Timestamp time.Time Topic string Partition int32 Offset int64 Attrs uint8 } Message Fields Value The message payload as raw bytes:\ntype Message struct { Value []byte // The message content } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Deserialize JSON var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Process the order return p.processOrder(ctx, order) } Key Optional message key used for partitioning and compaction:\ntype Message struct { Key []byte // Optional partition key } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { if msg.Key != nil { customerID := string(msg.Key) // All messages for this customer go to same partition } // Process message return nil } Key purposes:\nPartition assignment (same key → same partition) Log compaction (retain latest message per key) Message ordering (within partition) Headers Key-value metadata attached to the message:\ntype Header struct { Key string Value []byte } type Message struct { Headers []Header } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Find specific header for _, header := range msg.Headers { if header.Key == \"trace-id\" { traceID := string(header.Value) // Use for distributed tracing } if header.Key == \"content-type\" { contentType := string(header.Value) // Determine deserialization format } } return nil } Common header uses:\nDistributed tracing IDs Content type/encoding Source application Schema version Correlation IDs Timestamp When the message was produced:\ntype Message struct { Timestamp time.Time } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { age := time.Since(msg.Timestamp) if age \u003e 5*time.Minute { // Handle stale message log.Warn(\"Processing stale message\", \"age\", age) } return nil } Timestamp types:\nCreateTime: When producer sent the message (default) LogAppendTime: When broker received the message (if configured) Topic The Kafka topic this message came from:\ntype Message struct { Topic string } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { switch msg.Topic { case \"orders\": return p.processOrder(ctx, msg) case \"payments\": return p.processPayment(ctx, msg) default: return fmt.Errorf(\"unknown topic: %s\", msg.Topic) } } Useful when processing multiple topics with the same processor.\nPartition The partition this message came from:\ntype Message struct { Partition int32 } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { log.Info(\"Processing message\", \"topic\", msg.Topic, \"partition\", msg.Partition, \"offset\", msg.Offset, ) return nil } Partition guarantees:\nMessages in a partition are ordered Each partition processed by one consumer at a time Same key always goes to same partition Offset The message’s position within its partition:\ntype Message struct { Offset int64 } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Save offset for manual checkpoint recovery if err := p.processMessage(ctx, msg); err != nil { return err } // Record last processed offset p.recordCheckpoint(msg.Topic, msg.Partition, msg.Offset) return nil } Offset characteristics:\nMonotonically increasing within partition Unique identifier for message position Used for offset commits (acknowledgments) Attrs Message attributes (advanced):\ntype Message struct { Attrs uint8 } Bitmap of message flags (compression, transaction markers, etc.). Rarely used directly in application code.\nDeserialization Patterns JSON Deserialization Most common pattern for JSON messages:\ntype OrderMessage struct { OrderID string `json:\"order_id\"` Amount float64 `json:\"amount\"` } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return fmt.Errorf(\"invalid JSON: %w\", err) } return p.processOrder(ctx, order) } Protobuf Deserialization For protobuf-encoded messages:\nimport \"google.golang.org/protobuf/proto\" func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order orderpb.Order if err := proto.Unmarshal(msg.Value, \u0026order); err != nil { return fmt.Errorf(\"invalid protobuf: %w\", err) } return p.processOrder(ctx, \u0026order) } Avro Deserialization With schema registry:\nimport \"github.com/linkedin/goavro/v2\" type AvroProcessor struct { codec *goavro.Codec } func (p *AvroProcessor) Process(ctx context.Context, msg kafka.Message) error { native, _, err := p.codec.NativeFromBinary(msg.Value) if err != nil { return fmt.Errorf(\"invalid avro: %w\", err) } record := native.(map[string]interface{}) return p.processRecord(ctx, record) } Content-Type Routing Use headers to determine deserialization format:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { contentType := p.getHeader(msg, \"content-type\") switch contentType { case \"application/json\": return p.processJSON(ctx, msg.Value) case \"application/protobuf\": return p.processProtobuf(ctx, msg.Value) case \"application/avro\": return p.processAvro(ctx, msg.Value) default: return fmt.Errorf(\"unsupported content-type: %s\", contentType) } } func (p *Processor) getHeader(msg kafka.Message, key string) string { for _, h := range msg.Headers { if h.Key == key { return string(h.Value) } } return \"\" } Working with Headers Reading Headers Helper function for header access:\nfunc getHeader(msg kafka.Message, key string) (string, bool) { for _, h := range msg.Headers { if h.Key == key { return string(h.Value), true } } return \"\", false } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { if traceID, ok := getHeader(msg, \"trace-id\"); ok { // Use trace ID ctx = context.WithValue(ctx, \"trace-id\", traceID) } return nil } Header-Based Filtering Skip messages based on headers:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Skip test messages if env, ok := getHeader(msg, \"environment\"); ok \u0026\u0026 env == \"test\" { return nil } // Skip old schema versions if version, ok := getHeader(msg, \"schema-version\"); ok \u0026\u0026 version != \"v2\" { log.Warn(\"Skipping old schema version\", \"version\", version) return nil } return p.processMessage(ctx, msg) } Trace Context Propagation Extract distributed tracing context from headers:\nimport ( \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/propagation\" ) func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Extract trace context from headers carrier := \u0026headerCarrier{headers: msg.Headers} ctx = otel.GetTextMapPropagator().Extract(ctx, carrier) // Now ctx contains the parent span context _, span := tracer.Start(ctx, \"process-order\") defer span.End() return p.processOrder(ctx, msg) } type headerCarrier struct { headers []kafka.Header } func (c *headerCarrier) Get(key string) string { for _, h := range c.headers { if h.Key == key { return string(h.Value) } } return \"\" } func (c *headerCarrier) Set(key, value string) { // Not needed for extraction } func (c *headerCarrier) Keys() []string { keys := make([]string, len(c.headers)) for i, h := range c.headers { keys[i] = h.Key } return keys } Note: The Kafka runtime automatically handles OTel trace propagation, so this is usually not needed.\nError Handling Validation Errors Validate messages and decide how to handle invalid data:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { // Log and skip invalid JSON log.Error(\"Invalid JSON message\", \"topic\", msg.Topic, \"partition\", msg.Partition, \"offset\", msg.Offset, \"error\", err, ) return nil // Skip message, don't retry } // Validate business rules if order.Amount \u003c 0 { log.Error(\"Invalid order amount\", \"order_id\", order.OrderID, \"amount\", order.Amount, ) return nil // Skip invalid message } return p.processOrder(ctx, order) } Dead Letter Queue Route failed messages to a DLQ:\ntype Processor struct { producer *kgo.Client // For DLQ publishing } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { // Send to DLQ return p.sendToDLQ(ctx, msg, err) } if err := p.processOrder(ctx, order); err != nil { // Send to DLQ on processing error return p.sendToDLQ(ctx, msg, err) } return nil } func (p *Processor) sendToDLQ(ctx context.Context, msg kafka.Message, err error) error { dlqRecord := \u0026kgo.Record{ Topic: msg.Topic + \".dlq\", Key: msg.Key, Value: msg.Value, Headers: append(msg.Headers, kafka.Header{ Key: \"error\", Value: []byte(err.Error()), }), } p.producer.Produce(ctx, dlqRecord, nil) return nil // Don't return error, message handled } Message Logging Log messages for debugging:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { log.Info(\"Processing message\", \"topic\", msg.Topic, \"partition\", msg.Partition, \"offset\", msg.Offset, \"key\", string(msg.Key), \"timestamp\", msg.Timestamp, \"headers\", len(msg.Headers), ) // Don't log msg.Value in production (may contain PII) // Instead log a hash or truncated version valueHash := fmt.Sprintf(\"%x\", sha256.Sum256(msg.Value)) log.Debug(\"Message content hash\", \"hash\", valueHash) return p.processMessage(ctx, msg) } Next Steps Learn about Concurrency Model for partition processing Implement Idempotency patterns Explore Observability for message tracing ","categories":"","description":"Working with Kafka messages and metadata","excerpt":"Working with Kafka messages and metadata","ref":"/humus/pr-preview/pr-396/features/queue/kafka/message/","tags":"","title":"Message Structure"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/features/grpc/quick-start/","tags":"","title":"Quick Start"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/features/job/quick-start/","tags":"","title":"Quick Start"},{"body":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","ref":"/humus/pr-preview/pr-396/reference/rest-package/","tags":"","title":"Rest Package"},{"body":"Humus REST services provide built-in support for multiple authentication schemes with automatic OpenAPI security specification generation.\nOverview Authentication in Humus is handled through parameter validation options that you apply to headers, query parameters, or cookies. The framework:\nExtracts authentication credentials from requests Validates credentials using your custom logic Injects verified data into the request context Generates OpenAPI security schemes automatically Returns appropriate error responses (401 Unauthorized, 400 Bad Request) Parameter Validation Basics Before diving into authentication, understand parameter validation:\nRequired Parameters Mark parameters as required to ensure they’re present:\nrest.Handle( http.MethodGet, rest.BasePath(\"/protected\"), handler, rest.Header(\"Authorization\", rest.Required()), ) Missing required parameters return 400 Bad Request with a descriptive error.\nRegular Expression Validation Validate parameter format with regex:\nrest.Handle( http.MethodGet, rest.BasePath(\"/api/v1/users\"), handler, rest.QueryParam(\"api_version\", rest.Regex(regexp.MustCompile(`^v\\d+$`))), ) Invalid formats return 400 Bad Request.\nCombining Validators Chain multiple validators together:\nrest.Header( \"X-API-Key\", rest.Required(), rest.Regex(regexp.MustCompile(`^[a-f0-9]{32}$`)), rest.APIKey(\"api-key\"), ) Validators run in order. The first failure stops validation and returns an error.\nAuthentication Schemes Humus supports five authentication schemes, each adding appropriate OpenAPI security documentation.\nAPI Key Authentication API keys can be passed in headers, query parameters, or cookies:\n// Header-based API key rest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.Header(\"X-API-Key\", rest.Required(), rest.APIKey(\"api-key\")), ) // Query parameter API key rest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.QueryParam(\"key\", rest.Required(), rest.APIKey(\"api-key\")), ) // Cookie-based API key rest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.Cookie(\"api_key\", rest.Required(), rest.APIKey(\"api-key\")), ) Access the API key in your handler:\nhandler := rest.ProducerFunc[Response](func(ctx context.Context) (*Response, error) { apiKey := rest.HeaderValue(ctx, \"X-API-Key\") // Validate API key against your database if !isValidAPIKey(apiKey[0]) { return nil, fmt.Errorf(\"invalid API key\") } return processRequest(ctx) }) Basic Authentication HTTP Basic authentication (username:password encoded in Base64):\nrest.Handle( http.MethodGet, rest.BasePath(\"/admin\"), handler, rest.Header(\"Authorization\", rest.Required(), rest.BasicAuth(\"basic\")), ) Parse Basic auth credentials in your handler:\nhandler := rest.ProducerFunc[Response](func(ctx context.Context) (*Response, error) { authHeader := rest.HeaderValue(ctx, \"Authorization\")[0] // Parse \"Basic \u003cbase64\u003e\" format if !strings.HasPrefix(authHeader, \"Basic \") { return nil, fmt.Errorf(\"invalid authorization header\") } encoded := strings.TrimPrefix(authHeader, \"Basic \") decoded, err := base64.StdEncoding.DecodeString(encoded) if err != nil { return nil, fmt.Errorf(\"invalid base64 encoding\") } // Split username:password credentials := strings.SplitN(string(decoded), \":\", 2) if len(credentials) != 2 { return nil, fmt.Errorf(\"invalid credentials format\") } username, password := credentials[0], credentials[1] if !validateCredentials(username, password) { return nil, fmt.Errorf(\"invalid credentials\") } return processRequest(ctx) }) JWT Authentication JWT (JSON Web Token) Bearer authentication provides the most comprehensive solution with automatic token extraction and verification.\nHow It Works The framework:\nExtracts the Authorization header Validates the “Bearer ” format Strips the “Bearer \" prefix Calls your JWTVerifier.Verify() method with the clean token Returns 401 Unauthorized if verification fails Continues processing with the updated context if successful JWTVerifier Interface Implement the JWTVerifier interface to handle token verification:\ntype JWTVerifier interface { Verify(ctx context.Context, token string) (context.Context, error) } The token parameter is the JWT without the “Bearer \" prefix. Your implementation should:\nVerify the token’s signature Validate claims (expiration, issuer, audience, etc.) Extract relevant claims Inject claims into the context Return error if verification fails Complete Example with golang-jwt/jwt package main import ( \"context\" \"crypto/rsa\" \"fmt\" \"time\" \"github.com/golang-jwt/jwt/v5\" \"github.com/z5labs/humus/rest\" ) // Custom claims structure type Claims struct { UserID string `json:\"user_id\"` Email string `json:\"email\"` Roles []string `json:\"roles\"` jwt.RegisteredClaims } // JWTVerifier implementation type MyJWTVerifier struct { publicKey *rsa.PublicKey } func NewJWTVerifier(publicKey *rsa.PublicKey) *MyJWTVerifier { return \u0026MyJWTVerifier{publicKey: publicKey} } func (v *MyJWTVerifier) Verify(ctx context.Context, tokenString string) (context.Context, error) { // Parse and verify the token token, err := jwt.ParseWithClaims(tokenString, \u0026Claims{}, func(token *jwt.Token) (interface{}, error) { // Verify signing method if _, ok := token.Method.(*jwt.SigningMethodRSA); !ok { return nil, fmt.Errorf(\"unexpected signing method: %v\", token.Header[\"alg\"]) } return v.publicKey, nil }) if err != nil { return nil, fmt.Errorf(\"failed to parse token: %w\", err) } // Extract claims claims, ok := token.Claims.(*Claims) if !ok || !token.Valid { return nil, fmt.Errorf(\"invalid token claims\") } // Additional validation if claims.ExpiresAt.Before(time.Now()) { return nil, fmt.Errorf(\"token expired\") } // Inject claims into context ctx = context.WithValue(ctx, \"user_id\", claims.UserID) ctx = context.WithValue(ctx, \"email\", claims.Email) ctx = context.WithValue(ctx, \"roles\", claims.Roles) return ctx, nil } // Context keys for type-safe access type contextKey string const ( userIDKey contextKey = \"user_id\" emailKey contextKey = \"email\" rolesKey contextKey = \"roles\" ) // Helper functions to extract claims func GetUserID(ctx context.Context) (string, bool) { userID, ok := ctx.Value(userIDKey).(string) return userID, ok } func GetEmail(ctx context.Context) (string, bool) { email, ok := ctx.Value(emailKey).(string) return email, ok } func GetRoles(ctx context.Context) ([]string, bool) { roles, ok := ctx.Value(rolesKey).([]string) return roles, ok } Registering with JWT Auth func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"Secure API\", \"1.0.0\") // Load your public key (example) publicKey, err := loadPublicKey(\"public.pem\") if err != nil { return nil, err } verifier := NewJWTVerifier(publicKey) // Protected endpoint handler := rest.ProducerFunc[UserProfile](func(ctx context.Context) (*UserProfile, error) { // Extract user info from context userID, ok := GetUserID(ctx) if !ok { return nil, fmt.Errorf(\"user ID not found in context\") } email, _ := GetEmail(ctx) roles, _ := GetRoles(ctx) return \u0026UserProfile{ ID: userID, Email: email, Roles: roles, }, nil }) rest.Handle( http.MethodGet, rest.BasePath(\"/profile\"), rest.ProduceJson(handler), rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ) return api, nil } Testing JWT Authentication func TestJWTAuthentication(t *testing.T) { // Create test verifier privateKey, _ := rsa.GenerateKey(rand.Reader, 2048) verifier := NewJWTVerifier(\u0026privateKey.PublicKey) // Create valid token claims := \u0026Claims{ UserID: \"user-123\", Email: \"user@example.com\", Roles: []string{\"admin\"}, RegisteredClaims: jwt.RegisteredClaims{ ExpiresAt: jwt.NewNumericDate(time.Now().Add(time.Hour)), }, } token := jwt.NewWithClaims(jwt.SigningMethodRS256, claims) tokenString, _ := token.SignedString(privateKey) // Test request req := httptest.NewRequest(http.MethodGet, \"/profile\", nil) req.Header.Set(\"Authorization\", \"Bearer \"+tokenString) // Make request and verify // ... (standard HTTP testing) } Error Handling JWT authentication returns different HTTP status codes depending on the type of error:\n400 Bad Request - Token extraction failures (malformed request):\nMissing Authorization header (when JWTAuth used without Required()) Malformed header (not “Bearer ” format) Empty token after “Bearer \" prefix 401 Unauthorized - Token verification failures (authentication failed):\nInvalid JWT signature Expired token Invalid claims (issuer, audience, etc.) Any error returned by your JWTVerifier.Verify() method Example error scenarios:\n# Missing header - 400 Bad Request curl http://localhost:8080/profile # Returns: 400 Bad Request # Malformed header (missing \"Bearer\") - 400 Bad Request curl -H \"Authorization: invalid-token\" http://localhost:8080/profile # Returns: 400 Bad Request # Empty token - 400 Bad Request curl -H \"Authorization: Bearer \" http://localhost:8080/profile # Returns: 400 Bad Request # Invalid token (verification fails) - 401 Unauthorized curl -H \"Authorization: Bearer invalid.jwt.token\" http://localhost:8080/profile # Returns: 401 Unauthorized # Expired token (verification fails) - 401 Unauthorized curl -H \"Authorization: Bearer expired.jwt.token\" http://localhost:8080/profile # Returns: 401 Unauthorized Note: When combined with Required(), missing headers return 400 Bad Request from the Required() validator, which runs before JWT verification.\nOAuth 2.0 OAuth 2.0 authentication scheme:\nrest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.Header(\"Authorization\", rest.Required(), rest.OAuth2(\"oauth2\")), ) Note: OAuth 2.0 flows are not yet fully configured in the OpenAPI spec. You’ll need to implement the OAuth flow manually in your handler.\nOpenID Connect OpenID Connect authentication with discovery URL:\nrest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.Header( \"Authorization\", rest.Required(), rest.OpenIDConnect(\"oidc\", \"https://accounts.example.com/.well-known/openid-configuration\"), ), ) The discovery URL should point to your OpenID Connect provider’s configuration endpoint.\nOpenAPI Security Schemes All authentication options automatically add security schemes to your OpenAPI specification:\napi := rest.NewApi( \"Secure API\", \"1.0.0\", rest.Handle( http.MethodPost, rest.BasePath(\"/users\"), createUserHandler, rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ), ) The generated /openapi.json includes:\n{ \"components\": { \"securitySchemes\": { \"jwt\": { \"type\": \"http\", \"scheme\": \"bearer\", \"bearerFormat\": \"JWT\" } } }, \"paths\": { \"/users\": { \"post\": { \"security\": [ {\"jwt\": []} ] } } } } This integrates with Swagger UI, Postman, and other OpenAPI tools.\nSecurity Best Practices 1. Always Use HTTPS in Production Never transmit authentication credentials over unencrypted HTTP:\n// In production configuration rest.Config{ Port: 8443, TLS: \u0026rest.TLSConfig{ CertFile: \"/path/to/cert.pem\", KeyFile: \"/path/to/key.pem\", }, } 2. Validate Token Expiration Always check token expiration in your verifier:\nif claims.ExpiresAt.Before(time.Now()) { return nil, fmt.Errorf(\"token expired\") } 3. Use Strong Signing Algorithms Prefer RS256 (RSA) or ES256 (ECDSA) over HS256 (HMAC):\n// Verify signing method if _, ok := token.Method.(*jwt.SigningMethodRSA); !ok { return nil, fmt.Errorf(\"unexpected signing method\") } 4. Implement Rate Limiting Protect authentication endpoints from brute force attacks (use middleware or external service).\n5. Rotate Keys Regularly Implement key rotation for JWT signing keys and API keys.\n6. Use Context Keys with Types Avoid string collisions by using typed context keys:\ntype contextKey string const userIDKey contextKey = \"user_id\" // Set ctx = context.WithValue(ctx, userIDKey, \"user-123\") // Get with type safety userID, ok := ctx.Value(userIDKey).(string) 7. Validate All Claims Check audience, issuer, and other relevant claims:\nif claims.Issuer != \"https://auth.yourservice.com\" { return nil, fmt.Errorf(\"invalid issuer\") } if !claims.VerifyAudience(\"your-service\", true) { return nil, fmt.Errorf(\"invalid audience\") } 8. Log Authentication Failures Monitor for suspicious activity:\nfunc (v *MyJWTVerifier) Verify(ctx context.Context, token string) (context.Context, error) { // ... verification logic if err != nil { logger.WarnContext(ctx, \"JWT verification failed\", \"error\", err) return nil, err } return ctx, nil } Common Patterns Role-Based Access Control Combine JWT authentication with role checking:\nfunc requireRole(requiredRole string) rest.OperationOption { return func(oo *rest.OperationOptions) { oo.transforms = append(oo.transforms, func(r *http.Request) (*http.Request, error) { roles, ok := GetRoles(r.Context()) if !ok { return nil, fmt.Errorf(\"roles not found in context\") } hasRole := false for _, role := range roles { if role == requiredRole { hasRole = true break } } if !hasRole { return nil, fmt.Errorf(\"insufficient permissions\") } return r, nil }) } } // Usage rest.Handle( http.MethodDelete, rest.BasePath(\"/users\").Param(\"id\"), deleteHandler, rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), requireRole(\"admin\"), ) Optional Authentication Make authentication optional by omitting Required():\nrest.Handle( http.MethodGet, rest.BasePath(\"/public-or-private\"), handler, rest.Header(\"Authorization\", rest.JWTAuth(\"jwt\", verifier)), // No Required() ) Check in handler:\nhandler := rest.ProducerFunc[Response](func(ctx context.Context) (*Response, error) { if userID, ok := GetUserID(ctx); ok { // Authenticated user - return personalized response return getPersonalizedResponse(ctx, userID) } // Anonymous user - return generic response return getPublicResponse(ctx) }) Multiple Authentication Schemes Support multiple authentication methods:\n// Create verifier that handles multiple schemes type MultiAuthVerifier struct { jwtVerifier *JWTVerifier apiKeyStore map[string]string } func (v *MultiAuthVerifier) Verify(ctx context.Context, token string) (context.Context, error) { // Try JWT first ctx, err := v.jwtVerifier.Verify(ctx, token) if err == nil { return ctx, nil } // Fall back to API key if userID, ok := v.apiKeyStore[token]; ok { return context.WithValue(ctx, userIDKey, userID), nil } return nil, fmt.Errorf(\"authentication failed\") } Next Steps Learn about Error Handling for custom authentication error responses Explore OpenAPI to customize security documentation Read Handler Helpers for implementing authenticated handlers See Testing for authentication test patterns ","categories":"","description":"JWT, API keys, and security","excerpt":"JWT, API keys, and security","ref":"/humus/pr-preview/pr-396/features/rest/authentication/","tags":"","title":"Authentication"},{"body":"The Kafka runtime uses a goroutine-per-partition architecture that provides automatic parallelism and partition isolation.\nArchitecture Overview Each Kafka partition is processed in its own goroutine:\nTopic \"orders\" (3 partitions): ┌─────────────┐ │ Partition 0 │──────\u003e Goroutine 1 ──\u003e Processor ├─────────────┤ │ Partition 1 │──────\u003e Goroutine 2 ──\u003e Processor ├─────────────┤ │ Partition 2 │──────\u003e Goroutine 3 ──\u003e Processor └─────────────┘ Key characteristics:\nOne goroutine per assigned partition Partitions processed independently No locks needed between partitions Scales with partition count Consumer Group Coordination The runtime coordinates with other consumers in the group:\nConsumer Group \"order-processors\" with 3 instances: Instance 1 Instance 2 Instance 3 ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ │ Partition 0 │ │ Partition 1 │ │ Partition 2 │ │ Partition 3 │ │ Partition 4 │ │ Partition 5 │ └──────────────┘ └──────────────┘ └──────────────┘ Guarantees:\nEach partition assigned to exactly one consumer Automatic rebalancing when consumers join/leave Partition reassignment coordinates gracefully Partition Lifecycle Assignment When a partition is assigned to this consumer:\nReceive assignment from consumer group coordinator Spawn goroutine for the partition Start processing messages from the partition Continue until partition is revoked or app shuts down Code flow:\n// Internal runtime behavior (conceptual) func (r *Runtime) onPartitionAssigned(topic string, partition int32) { ctx, cancel := context.WithCancel(r.ctx) // Spawn goroutine for this partition go r.processPartition(ctx, topic, partition) // Store cancel function for revocation r.partitionCancels[topicPartition{topic, partition}] = cancel } Revocation When a partition is revoked (rebalance):\nReceive revocation signal from coordinator Cancel context for partition’s goroutine Wait for goroutine to finish current message Complete gracefully before rebalance proceeds Code flow:\n// Internal runtime behavior (conceptual) func (r *Runtime) onPartitionRevoked(topic string, partition int32) { // Cancel the goroutine's context cancel := r.partitionCancels[topicPartition{topic, partition}] cancel() // Wait for goroutine to finish r.wait.Wait() // Partition now free for reassignment } Concurrency Guarantees Within a Partition Messages in a partition are processed serially (one at a time):\n// For Partition 0: Message 1 ──\u003e Process ──\u003e Complete Message 2 ──\u003e Process ──\u003e Complete Message 3 ──\u003e Process ──\u003e Complete Guarantees:\nMessages processed in order No concurrent processing within partition Simple reasoning about state Across Partitions Messages in different partitions are processed concurrently:\n// Concurrent processing: Partition 0: Message 1 ──\u003e Process ──┐ Partition 1: Message 1 ──\u003e Process ──┼──\u003e All concurrent Partition 2: Message 1 ──\u003e Process ──┘ Implications:\nNo ordering guarantees across partitions Shared state needs synchronization Independent failure isolation Scaling Patterns Vertical Scaling (More Partitions) Increase partition count to enable more parallelism:\n# Create topic with 12 partitions kafka-topics.sh --create \\ --topic orders \\ --partitions 12 \\ --bootstrap-server localhost:9092 One consumer instance can process 12 partitions concurrently:\nConsumer Instance 1 (12 goroutines): Partition 0 ──\u003e Goroutine 1 Partition 1 ──\u003e Goroutine 2 ... Partition 11 ──\u003e Goroutine 12 Limits:\nMaximum parallelism = number of partitions Cannot exceed partition count with consumers Horizontal Scaling (More Consumers) Add consumer instances to distribute partitions:\n3 consumers, 12 partitions (4 partitions each): Consumer 1: Consumer 2: Consumer 3: Partition 0 Partition 4 Partition 8 Partition 1 Partition 5 Partition 9 Partition 2 Partition 6 Partition 10 Partition 3 Partition 7 Partition 11 Recommendations:\nStart with partitions = 2-4× expected consumers Allows room for scaling Example: 12 partitions supports 1-12 consumers Shared State Patterns When processors need shared state:\nThread-Safe Data Structures Use concurrent-safe types:\nimport \"sync\" type Processor struct { mu sync.RWMutex cache map[string]string } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Read from cache p.mu.RLock() value := p.cache[string(msg.Key)] p.mu.RUnlock() // Process message result := p.compute(value) // Write to cache p.mu.Lock() p.cache[string(msg.Key)] = result p.mu.Unlock() return nil } Partition-Local State Maintain separate state per partition:\ntype Processor struct { // Map from partition to its state partitionState map[int32]*PartitionState mu sync.RWMutex } type PartitionState struct { // No locks needed - only accessed by one goroutine counter int lastSeen time.Time } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Get state for this partition (no lock needed within partition) p.mu.RLock() state := p.partitionState[msg.Partition] p.mu.RUnlock() // Update state without locks state.counter++ state.lastSeen = time.Now() return nil } Atomic Operations Use atomic types for counters:\nimport \"sync/atomic\" type Processor struct { messagesProcessed atomic.Int64 } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Atomic increment - safe across goroutines p.messagesProcessed.Add(1) return nil } Rebalancing Triggering Rebalances Rebalances occur when:\nConsumer joins the group (new instance starts) Consumer leaves the group (instance stops/crashes) Topic partitions are added Session timeout expires (consumer considered dead) Rebalance Process 1. Coordinator initiates rebalance ↓ 2. All consumers stop fetching ↓ 3. Current partitions revoked ├─\u003e Cancel partition goroutines └─\u003e Wait for completion ↓ 4. New partitions assigned ├─\u003e Spawn new goroutines └─\u003e Resume processing ↓ 5. Normal processing resumes Minimizing Rebalance Impact Fast Rebalancing:\n// Use shorter timeouts for faster rebalancing runtime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.SessionTimeout(10 * time.Second), kafka.RebalanceTimeout(20 * time.Second), ) Graceful Shutdown:\n// Ensure clean shutdown to avoid forced rebalancing // The framework handles this automatically via context cancellation Performance Considerations Partition Count Too few partitions:\nLimited parallelism One slow processor blocks others Difficult to scale horizontally Too many partitions:\nHigher overhead per partition More goroutines More memory usage Recommendation:\nStart with 2-4× expected consumer count Example: 3 consumers → 12 partitions Allows scaling to 12 consumers without repartitioning Message Distribution Even distribution across partitions:\n// Producer uses key to distribute evenly key := fmt.Sprintf(\"customer-%d\", customerID % numPartitions) Avoid hot partitions:\nDon’t route all traffic to one partition Use random or round-robin for keyless messages Monitor partition lag for imbalance Processing Time Fast processors:\nCan handle more partitions per consumer Higher throughput Lower latency Slow processors:\nLimit partitions per consumer Scale horizontally Consider async I/O Monitoring Concurrency Consumer Lag per Partition kafka-consumer-groups.sh \\ --bootstrap-server localhost:9092 \\ --group my-group \\ --describe Output shows lag per partition:\nTOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG orders 0 1000 1000 0 orders 1 950 1020 70 # Lagging! orders 2 1000 1000 0 Hot partition detected: Partition 1 is lagging.\nGoroutine Count Monitor active goroutines:\nimport \"runtime\" func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Periodically log goroutine count if msg.Offset % 1000 == 0 { log.Info(\"Runtime stats\", \"goroutines\", runtime.NumGoroutine(), \"partition\", msg.Partition, ) } return nil } Error Handling Partition-Level Errors Errors in one partition don’t affect others:\nPartition 0: Processing ✓ Partition 1: Error! ← Partition 1 stops Partition 2: Processing ✓ ← Continues normally At-least-once behavior:\nFailed partition stops processing Offset not committed Messages will be redelivered after rebalance At-most-once behavior:\nFailed partition stops processing Offset already committed Messages lost Context Cancellation All partition goroutines respect context cancellation:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Check context before expensive operation select { case \u003c-ctx.Done(): return ctx.Err() default: } // Process message return p.doWork(ctx, msg) } This ensures clean shutdown and rebalancing.\nNext Steps Learn Idempotency patterns for at-least-once processing Explore Multi-Topic Processing patterns Configure Production Settings for optimal performance ","categories":"","description":"Understanding goroutine-per-partition processing","excerpt":"Understanding goroutine-per-partition processing","ref":"/humus/pr-preview/pr-396/features/queue/kafka/concurrency/","tags":"","title":"Concurrency Model"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/advanced/custom-health-monitors/","tags":"","title":"Custom Health Monitors"},{"body":"Documentation coming soon.\nSee Getting Started for building new Humus services.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Getting Started for building new Humus …","ref":"/humus/pr-preview/pr-396/integration/from-chi/","tags":"","title":"From Chi"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/features/grpc/grpc-api/","tags":"","title":"Grpc Api"},{"body":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","ref":"/humus/pr-preview/pr-396/reference/grpc-package/","tags":"","title":"Grpc Package"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/features/job/job-handler/","tags":"","title":"Job Handler"},{"body":"This guide provides recommended patterns for organizing Humus applications.\nSimple Service For small services with a single purpose:\nmy-service/ ├── main.go # Entry point with Run() call ├── config.yaml # Configuration ├── go.mod ├── go.sum └── README.md main.go:\npackage main import ( \"context\" \"github.com/z5labs/humus/rest\" ) type Config struct { rest.Config `config:\",squash\"` } func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"My Service\", \"1.0.0\") // Register handlers... return api, nil } Organized Service For services with multiple handlers or business logic:\nmy-service/ ├── cmd/ │ └── server/ │ └── main.go # Entry point ├── internal/ │ ├── app/ │ │ └── app.go # Init function │ ├── handlers/ │ │ ├── users.go # User handlers │ │ └── posts.go # Post handlers │ └── models/ │ └── user.go # Domain models ├── config.yaml ├── go.mod └── go.sum cmd/server/main.go:\npackage main import ( \"my-service/internal/app\" \"github.com/z5labs/humus/rest\" ) func main() { rest.Run(rest.YamlSource(\"config.yaml\"), app.Init) } internal/app/app.go:\npackage app import ( \"context\" \"my-service/internal/handlers\" \"github.com/z5labs/humus/rest\" ) type Config struct { rest.Config `config:\",squash\"` // Additional config... } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"My Service\", \"1.0.0\") // Register handlers from different packages handlers.RegisterUserHandlers(api) handlers.RegisterPostHandlers(api) return api, nil } Large Application For larger applications with multiple domains:\nmy-service/ ├── cmd/ │ └── server/ │ └── main.go ├── internal/ │ ├── app/ │ │ ├── app.go │ │ └── config.go │ ├── user/ │ │ ├── handler.go # HTTP handlers │ │ ├── service.go # Business logic │ │ └── store.go # Data access │ ├── post/ │ │ ├── handler.go │ │ ├── service.go │ │ └── store.go │ └── common/ │ └── middleware.go ├── pkg/ │ └── client/ # Public client library (optional) │ └── client.go ├── configs/ │ ├── config.dev.yaml │ ├── config.staging.yaml │ └── config.prod.yaml ├── go.mod └── go.sum gRPC Service For gRPC services with Protocol Buffers:\nmy-grpc-service/ ├── cmd/ │ └── server/ │ └── main.go ├── internal/ │ ├── app/ │ │ └── app.go │ └── user/ │ └── service.go # gRPC service implementation ├── proto/ │ ├── user/ │ │ └── user.proto # Proto definitions │ └── common/ │ └── types.proto ├── gen/ # Generated code │ └── proto/ │ └── user/ │ ├── user.pb.go │ └── user_grpc.pb.go ├── Makefile # For proto generation ├── config.yaml ├── go.mod └── go.sum Example Makefile for proto generation:\n.PHONY: proto proto: protoc --go_out=gen --go_opt=paths=source_relative \\ --go-grpc_out=gen --go-grpc_opt=paths=source_relative \\ proto/**/*.proto Job Service For batch processing or one-off jobs:\nmy-job/ ├── cmd/ │ └── job/ │ └── main.go ├── internal/ │ ├── app/ │ │ └── app.go │ ├── processor/ │ │ └── processor.go # Job logic │ └── store/ │ └── database.go # Data access ├── config.yaml ├── go.mod └── go.sum internal/app/app.go:\npackage app import ( \"context\" \"my-job/internal/processor\" \"github.com/z5labs/humus/job\" ) type Config struct { humus.Config `config:\",squash\"` // Job-specific config } func Init(ctx context.Context, cfg Config) (job.Handler, error) { proc := processor.New(cfg) return proc, nil } Monorepo with Multiple Services For projects with multiple related services:\nmy-project/ ├── services/ │ ├── api/ # REST API │ │ ├── cmd/ │ │ ├── internal/ │ │ └── go.mod │ ├── worker/ # gRPC service │ │ ├── cmd/ │ │ ├── internal/ │ │ └── go.mod │ └── jobs/ # Background jobs │ ├── cmd/ │ ├── internal/ │ └── go.mod ├── pkg/ # Shared packages │ ├── models/ │ └── common/ └── proto/ # Shared proto files └── common/ Configuration Files Multiple Environments my-service/ ├── configs/ │ ├── base.yaml # Shared config │ ├── dev.yaml # Development │ ├── staging.yaml # Staging │ └── prod.yaml # Production └── cmd/server/main.go Loading environment-specific config:\nimport ( \"os\" \"github.com/z5labs/bedrock/pkg/config\" ) func main() { env := os.Getenv(\"ENV\") if env == \"\" { env = \"dev\" } source := config.MultiSource( config.FromYaml(\"configs/base.yaml\"), config.FromYaml(fmt.Sprintf(\"configs/%s.yaml\", env)), ) rest.Run(source, app.Init) } Package Organization Best Practices Use internal/ for Private Code Place code that shouldn’t be imported by other projects in internal/:\nmy-service/ ├── internal/ # Cannot be imported by external projects │ ├── app/ │ └── handlers/ └── pkg/ # Can be imported by others └── client/ Domain-Driven Structure Group by domain/feature rather than technical layer:\nGood:\ninternal/ ├── user/ │ ├── handler.go # HTTP layer │ ├── service.go # Business logic │ └── store.go # Data layer └── post/ ├── handler.go ├── service.go └── store.go Less Ideal:\ninternal/ ├── handlers/ # All HTTP handlers ├── services/ # All business logic └── stores/ # All data access Separate Main Package Keep main.go minimal - just wiring, not logic:\n// Good: main.go just calls Run func main() { rest.Run(rest.YamlSource(\"config.yaml\"), app.Init) } // Less ideal: main.go contains business logic func main() { // Lots of setup code... // Handler definitions... // Database initialization... } Testing Structure Place tests alongside the code:\ninternal/ ├── user/ │ ├── handler.go │ ├── handler_test.go │ ├── service.go │ └── service_test.go For integration tests:\nmy-service/ ├── internal/ └── test/ ├── integration/ │ └── api_test.go └── testdata/ └── fixtures.json Next Steps Now that you understand project structure:\nExplore REST Services to build HTTP APIs Learn about gRPC Services for microservices Read Job Services for batch processing Review Core Concepts for deeper understanding ","categories":"","description":"Recommended project layout patterns","excerpt":"Recommended project layout patterns","ref":"/humus/pr-preview/pr-396/getting-started/project-structure/","tags":"","title":"Project Structure"},{"body":"Humus queue services provide a complete framework for processing messages from message queues with configurable delivery semantics, automatic concurrency management, and built-in observability.\nOverview Queue services in Humus are built on:\nPluggable Runtimes - Support for different message queue systems (Kafka, and more) Delivery Semantics - Choose between at-most-once and at-least-once processing Type Safety - Compile-time type checking for message processors OpenTelemetry - Automatic tracing and metrics for message processing Quick Start package main import ( \"context\" \"encoding/json\" \"github.com/z5labs/humus/queue\" \"github.com/z5labs/humus/queue/kafka\" ) type Config struct { queue.Config `config:\",squash\"` Kafka struct { Brokers []string `config:\"brokers\"` GroupID string `config:\"group_id\"` Topic string `config:\"topic\"` } `config:\"kafka\"` } type OrderMessage struct { OrderID string `json:\"order_id\"` Amount float64 `json:\"amount\"` } type OrderProcessor struct{} func (p *OrderProcessor) Process(ctx context.Context, msg *OrderMessage) error { // Process the order // This should be idempotent for at-least-once processing return nil } func decodeOrder(data []byte) (*OrderMessage, error) { var msg OrderMessage err := json.Unmarshal(data, \u0026msg) return \u0026msg, err } func main() { queue.Run(queue.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*queue.App, error) { processor := \u0026OrderProcessor{} runtime, err := kafka.NewAtLeastOnceRuntime( cfg.Kafka.Brokers, cfg.Kafka.Topic, cfg.Kafka.GroupID, processor, decodeOrder, ) if err != nil { return nil, err } return queue.NewApp(runtime), nil } Core Concepts Three-Phase Processing Pattern Queue processing follows a three-phase pattern:\nConsumer - Retrieves messages from the queue Processor - Executes business logic on messages Acknowledger - Confirms successful processing The order of these phases determines the delivery semantics.\nDelivery Semantics At-Most-Once (Consume → Acknowledge → Process):\nMessages acknowledged before processing Fast throughput, but processing failures lose messages Suitable for non-critical data (metrics, logs, caching) At-Least-Once (Consume → Process → Acknowledge):\nMessages acknowledged after successful processing Reliable delivery, but may deliver duplicates Requires idempotent processors Suitable for critical operations (financial, database updates) Runtime Interface All queue implementations provide a Runtime that orchestrates the processing phases:\ntype Runtime interface { ProcessQueue(ctx context.Context) error } Available Runtimes Currently supported:\nKafka - Apache Kafka via franz-go client Built-in Features Every queue service automatically includes:\nGraceful Shutdown - Clean shutdown on SIGTERM/SIGINT OpenTelemetry Tracing - Automatic spans for each message Context Propagation - Distributed tracing across services Lifecycle Management - Managed by Bedrock framework What You’ll Learn This section covers:\nQueue Framework - Core abstractions and patterns Delivery Semantics - At-most-once vs at-least-once Kafka Runtime - Apache Kafka integration Next Steps Start with the Kafka Quick Start Guide to build your first queue processor.\n","categories":"","description":"Processing messages from queues with flexible delivery semantics","excerpt":"Processing messages from queues with flexible delivery semantics","ref":"/humus/pr-preview/pr-396/features/queue/","tags":"","title":"Queue Services"},{"body":"Humus REST routing provides flexible path building and comprehensive parameter validation with automatic OpenAPI documentation generation.\nPath Building Paths are constructed using rest.BasePath() and chained parameter methods.\nStatic Paths Simple static routes:\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\"), listUsersHandler, ) // Matches: GET /users Path Parameters Add dynamic segments using .Param():\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\").Param(\"id\"), getUserHandler, ) // Matches: GET /users/{id} // Examples: /users/123, /users/abc Access path parameters in your handler:\nhandler := rest.ProducerFunc[User](func(ctx context.Context) (*User, error) { userID := rest.PathParamValue(ctx, \"id\") return getUserByID(ctx, userID) }) Nested Paths Chain multiple path segments:\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\").Param(\"id\").Path(\"posts\").Param(\"postId\"), getPostHandler, ) // Matches: GET /users/{id}/posts/{postId} // Example: /users/123/posts/456 Path Options Path parameters support validation options:\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\").Param(\"id\", rest.Regex(regexp.MustCompile(`^\\d+$`))), getUserHandler, ) // Only matches numeric IDs: /users/123 // Rejects: /users/abc (returns 400 Bad Request) Parameter Validation Parameters can be validated using various options that apply to headers, query parameters, cookies, and path parameters.\nRequired Parameters Ensure parameters are present:\nrest.Handle( http.MethodGet, rest.BasePath(\"/search\"), searchHandler, rest.QueryParam(\"q\", rest.Required()), ) Missing required parameters return 400 Bad Request:\n{ \"error\": \"missing required request parameter in query: q\" } Regular Expression Validation Validate parameter format:\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\"), handler, rest.QueryParam(\"page\", rest.Regex(regexp.MustCompile(`^\\d+$`))), rest.QueryParam(\"email\", rest.Regex(regexp.MustCompile(`^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$`))), ) Invalid formats return 400 Bad Request:\n{ \"error\": \"invalid parameter value in query: page\" } Combining Validators Chain multiple validators:\nrest.QueryParam( \"api_key\", rest.Required(), rest.Regex(regexp.MustCompile(`^[a-f0-9]{32}$`)), ) Validators run in order. The first failure stops validation and returns an error.\nQuery Parameters Query parameters are extracted from the URL query string.\nBasic Usage rest.Handle( http.MethodGet, rest.BasePath(\"/search\"), handler, rest.QueryParam(\"q\"), rest.QueryParam(\"limit\"), rest.QueryParam(\"offset\"), ) Access in handler:\nhandler := rest.ProducerFunc[Results](func(ctx context.Context) (*Results, error) { query := rest.QueryParamValue(ctx, \"q\")[0] limit := rest.QueryParamValue(ctx, \"limit\")[0] return search(ctx, query, limit) }) Multiple Values Query parameters can have multiple values:\nrest.Handle( http.MethodGet, rest.BasePath(\"/filter\"), handler, rest.QueryParam(\"tag\"), // Allows multiple values ) Request: GET /filter?tag=go\u0026tag=rest\u0026tag=api\nhandler := rest.ProducerFunc[Results](func(ctx context.Context) (*Results, error) { tags := rest.QueryParamValue(ctx, \"tag\") // tags = []string{\"go\", \"rest\", \"api\"} return filterByTags(ctx, tags) }) With Validation rest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.QueryParam(\"page\", rest.Required(), rest.Regex(regexp.MustCompile(`^\\d+$`))), rest.QueryParam(\"limit\", rest.Regex(regexp.MustCompile(`^\\d+$`))), ) Headers Headers are used for metadata, authentication, and content negotiation.\nBasic Usage rest.Handle( http.MethodGet, rest.BasePath(\"/data\"), handler, rest.Header(\"Accept-Language\"), rest.Header(\"X-Request-ID\", rest.Required()), ) Access in handler:\nhandler := rest.ProducerFunc[Data](func(ctx context.Context) (*Data, error) { language := rest.HeaderValue(ctx, \"Accept-Language\")[0] requestID := rest.HeaderValue(ctx, \"X-Request-ID\")[0] return getData(ctx, language, requestID) }) Authentication Headers For authentication, use dedicated auth options instead of manual validation:\nrest.Handle( http.MethodPost, rest.BasePath(\"/users\"), handler, rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ) See Authentication for complete authentication examples.\nCustom Validation rest.Handle( http.MethodPost, rest.BasePath(\"/webhooks\"), handler, rest.Header(\"X-Signature\", rest.Required(), rest.Regex(regexp.MustCompile(`^sha256=[a-f0-9]{64}$`))), ) Cookies Cookies are extracted from the Cookie header.\nBasic Usage rest.Handle( http.MethodGet, rest.BasePath(\"/dashboard\"), handler, rest.Cookie(\"session\", rest.Required()), ) Access in handler:\nhandler := rest.ProducerFunc[Dashboard](func(ctx context.Context) (*Dashboard, error) { cookies := rest.CookieValue(ctx, \"session\") sessionID := cookies[0].Value return getDashboard(ctx, sessionID) }) With Validation rest.Handle( http.MethodGet, rest.BasePath(\"/app\"), handler, rest.Cookie(\"session_id\", rest.Required(), rest.Regex(regexp.MustCompile(`^[a-f0-9]{64}$`))), ) Cookie Authentication Cookies can be used for API key authentication:\nrest.Handle( http.MethodGet, rest.BasePath(\"/api\"), handler, rest.Cookie(\"api_key\", rest.Required(), rest.APIKey(\"cookie-auth\")), ) OpenAPI Integration All parameters are automatically documented in the OpenAPI specification:\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\").Param(\"id\"), handler, rest.QueryParam(\"include\", rest.Regex(regexp.MustCompile(`^(profile|posts|comments)$`))), rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ) Generates OpenAPI spec:\n{ \"paths\": { \"/users/{id}\": { \"get\": { \"parameters\": [ { \"name\": \"id\", \"in\": \"path\", \"required\": true }, { \"name\": \"include\", \"in\": \"query\", \"schema\": { \"type\": \"string\", \"pattern\": \"^(profile|posts|comments)$\" } }, { \"name\": \"Authorization\", \"in\": \"header\", \"required\": true } ], \"security\": [ {\"jwt\": []} ] } } } } Best Practices 1. Use Path Parameters for Resources // Good - resource identifiers in path rest.BasePath(\"/users\").Param(\"id\") // Avoid - identifiers in query string rest.BasePath(\"/users\") + rest.QueryParam(\"id\") 2. Use Query Parameters for Filtering and Options // Good - filtering/pagination via query rest.QueryParam(\"page\") rest.QueryParam(\"filter\") rest.QueryParam(\"sort\") 3. Validate Early Apply validation at the parameter level, not in business logic:\n// Good rest.QueryParam(\"limit\", rest.Required(), rest.Regex(regexp.MustCompile(`^\\d+$`))) // Avoid - validation in handler handler := func(ctx context.Context) (*Results, error) { limit := rest.QueryParamValue(ctx, \"limit\")[0] if !isNumeric(limit) { return nil, errors.New(\"invalid limit\") } // ... } 4. Use Typed Context Keys Avoid string collisions when storing values in context:\ntype contextKey string const userIDKey contextKey = \"user_id\" // Set ctx = context.WithValue(ctx, userIDKey, \"123\") // Get with type safety userID, ok := ctx.Value(userIDKey).(string) 5. Document with Examples Use descriptive parameter names and add OpenAPI descriptions:\n// Clear parameter names rest.QueryParam(\"items_per_page\") // Not just \"limit\" rest.QueryParam(\"sort_order\") // Not just \"order\" 6. Use Authentication Options For authentication, always use the built-in auth options:\n// Good rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)) // Avoid - manual token parsing rest.Header(\"Authorization\", rest.Required()) // ... then parse JWT in handler See Authentication for proper authentication patterns.\nCommon Patterns Pagination rest.Handle( http.MethodGet, rest.BasePath(\"/items\"), handler, rest.QueryParam(\"page\", rest.Regex(regexp.MustCompile(`^\\d+$`))), rest.QueryParam(\"limit\", rest.Regex(regexp.MustCompile(`^\\d+$`))), ) Filtering rest.Handle( http.MethodGet, rest.BasePath(\"/products\"), handler, rest.QueryParam(\"category\"), rest.QueryParam(\"min_price\", rest.Regex(regexp.MustCompile(`^\\d+(\\.\\d{2})?$`))), rest.QueryParam(\"max_price\", rest.Regex(regexp.MustCompile(`^\\d+(\\.\\d{2})?$`))), ) Sorting rest.Handle( http.MethodGet, rest.BasePath(\"/users\"), handler, rest.QueryParam(\"sort\", rest.Regex(regexp.MustCompile(`^(name|email|created_at)$`))), rest.QueryParam(\"order\", rest.Regex(regexp.MustCompile(`^(asc|desc)$`))), ) Nested Resources // GET /organizations/{orgId}/teams/{teamId}/members rest.Handle( http.MethodGet, rest.BasePath(\"/organizations\"). Param(\"orgId\"). Path(\"teams\"). Param(\"teamId\"). Path(\"members\"), handler, ) Versioned APIs // URL versioning rest.BasePath(\"/api/v1/users\") rest.BasePath(\"/api/v2/users\") // Or header versioning rest.Handle( http.MethodGet, rest.BasePath(\"/api/users\"), handler, rest.Header(\"API-Version\", rest.Required(), rest.Regex(regexp.MustCompile(`^v\\d+$`))), ) Next Steps Learn about Authentication for securing your routes Explore Handler Helpers for type-safe handlers See OpenAPI for customizing generated documentation Read Error Handling for parameter validation errors ","categories":"","description":"Paths and parameters","excerpt":"Paths and parameters","ref":"/humus/pr-preview/pr-396/features/rest/routing/","tags":"","title":"Routing"},{"body":"Documentation coming soon.\nSee Contributing Overview for general contribution guidelines.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Contributing Overview for general …","ref":"/humus/pr-preview/pr-396/contributing/testing-guide/","tags":"","title":"Testing Guide"},{"body":"Documentation coming soon.\nSee GitHub Discussions for community support.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee GitHub Discussions for community …","ref":"/humus/pr-preview/pr-396/faq/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"AI Coding Agent Instructions For developers using AI coding agents (GitHub Copilot, Cursor, Codeium, etc.), Humus provides modular instruction files that you can copy to your project repository.\n📋 Browse the instructions directory\nCopy the relevant instruction files to your project’s .github/ directory (or similar location):\nAvailable instruction files:\nhumus-common.instructions.md - Common patterns for all service types (required) humus-rest.instructions.md - REST API specific patterns humus-grpc.instructions.md - gRPC service specific patterns humus-kafka.instructions.md - Kafka queue processor specific patterns humus-job.instructions.md - Job executor specific patterns What’s included:\nProject structure patterns (simple, organized, production-ready) Service-specific patterns (REST handlers, gRPC services, queue processors, jobs) Configuration best practices Error handling strategies Common pitfalls and anti-patterns Testing guidelines Usage: Copy humus-common.instructions.md along with the file(s) specific to your application type. For example, a REST API would use humus-common.instructions.md + humus-rest.instructions.md.\nThis helps your AI coding agent generate code that follows Humus conventions and best practices.\nGeneral Best Practices For detailed best practices and patterns, see the Project Structure guide and review the example projects.\nCommunity Support See GitHub Discussions for community support.\n","categories":"","description":"","excerpt":"AI Coding Agent Instructions For developers using AI coding agents …","ref":"/humus/pr-preview/pr-396/faq/best-practices/","tags":"","title":"Best Practices"},{"body":"Documentation coming soon.\nSee Contributing Overview for general contribution guidelines.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Contributing Overview for general …","ref":"/humus/pr-preview/pr-396/contributing/documentation/","tags":"","title":"Documentation"},{"body":"Documentation coming soon.\nSee Getting Started for building new Humus services.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Getting Started for building new Humus …","ref":"/humus/pr-preview/pr-396/integration/from-grpc-go/","tags":"","title":"From Grpc Go"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/features/grpc/health-service/","tags":"","title":"Health Service"},{"body":"At-least-once processing guarantees message delivery but may deliver duplicates. Your processor must be idempotent - processing the same message multiple times produces the same result.\nWhy Idempotency Matters At-Least-Once Delivers Duplicates Common scenarios that cause duplicates:\nProcessing completes but offset commit fails\nMessage 100 → Process ✓ → Commit ✗ [Rebalance or restart] Message 100 → Process ✓ → Commit ✓ (duplicate!) Consumer crashes after processing\nMessage 100 → Process ✓ → [Crash before commit] [Restart] Message 100 → Process ✓ → Commit ✓ (duplicate!) Network partition during commit\nMessage 100 → Process ✓ → Commit [timeout] [Retry] Message 100 → Process ✓ → Commit ✓ (duplicate!) Without Idempotency Non-idempotent processors corrupt data:\n// NON-IDEMPOTENT: Incrementing a counter func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // First processing: balance = 100 + 50 = 150 // Duplicate: balance = 150 + 50 = 200 ← Wrong! _, err := p.db.Exec( \"UPDATE accounts SET balance = balance + $1 WHERE id = $2\", msg.Amount, msg.AccountID, ) return err } Idempotency Patterns Pattern 1: Unique ID Tracking Store processed message IDs in a table:\ntype Processor struct { db *sql.DB } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Check if already processed var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM processed_orders WHERE order_id = $1)\", order.OrderID, ).Scan(\u0026exists) if err != nil { return err } if exists { // Already processed, skip return nil } // Process in transaction tx, err := p.db.BeginTx(ctx, nil) if err != nil { return err } defer tx.Rollback() // Insert order _, err = tx.ExecContext(ctx, \"INSERT INTO orders (order_id, customer_id, total) VALUES ($1, $2, $3)\", order.OrderID, order.CustomerID, order.Total, ) if err != nil { return err } // Record as processed _, err = tx.ExecContext(ctx, \"INSERT INTO processed_orders (order_id, processed_at) VALUES ($1, NOW())\", order.OrderID, ) if err != nil { return err } return tx.Commit() } Schema:\nCREATE TABLE processed_orders ( order_id VARCHAR(255) PRIMARY KEY, processed_at TIMESTAMP NOT NULL ); CREATE INDEX idx_processed_at ON processed_orders(processed_at); Cleanup old entries:\nDELETE FROM processed_orders WHERE processed_at \u003c NOW() - INTERVAL '7 days'; Pattern 2: Upsert Operations Use database upserts for natural idempotency:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Upsert: idempotent operation _, err := p.db.ExecContext(ctx, `INSERT INTO orders (order_id, customer_id, total, updated_at) VALUES ($1, $2, $3, NOW()) ON CONFLICT (order_id) DO UPDATE SET customer_id = EXCLUDED.customer_id, total = EXCLUDED.total, updated_at = NOW()`, order.OrderID, order.CustomerID, order.Total, ) return err } Result: Processing the same message multiple times produces the same database state.\nPattern 3: SET Operations Use idempotent SET operations instead of increments:\n// IDEMPOTENT: Setting absolute values func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var update AccountUpdate if err := json.Unmarshal(msg.Value, \u0026update); err != nil { return err } // Set absolute value (idempotent) _, err := p.db.ExecContext(ctx, \"UPDATE accounts SET balance = $1, updated_at = $2 WHERE id = $3\", update.NewBalance, // Absolute value update.Timestamp, // Version/timestamp update.AccountID, ) return err } Pattern 4: Unique Constraints Let the database enforce uniqueness:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var payment Payment if err := json.Unmarshal(msg.Value, \u0026payment); err != nil { return err } // Insert with unique constraint _, err := p.db.ExecContext(ctx, \"INSERT INTO payments (transaction_id, amount, status) VALUES ($1, $2, 'completed')\", payment.TransactionID, payment.Amount, ) // Handle duplicate key error if isDuplicateKeyError(err) { // Already processed, not an error return nil } return err } func isDuplicateKeyError(err error) bool { if err == nil { return false } // PostgreSQL duplicate key error code return strings.Contains(err.Error(), \"duplicate key value\") } Schema:\nCREATE TABLE payments ( transaction_id VARCHAR(255) PRIMARY KEY, amount DECIMAL(10,2) NOT NULL, status VARCHAR(50) NOT NULL, created_at TIMESTAMP DEFAULT NOW() ); Pattern 5: Offset as Idempotency Key Use Kafka’s natural ordering:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var event Event if err := json.Unmarshal(msg.Value, \u0026event); err != nil { return err } // Composite key: topic + partition + offset idempotencyKey := fmt.Sprintf(\"%s-%d-%d\", msg.Topic, msg.Partition, msg.Offset) var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM processed_events WHERE idempotency_key = $1)\", idempotencyKey, ).Scan(\u0026exists) if err != nil { return err } if exists { return nil } // Process and record tx, err := p.db.BeginTx(ctx, nil) if err != nil { return err } defer tx.Rollback() // Process event if err := p.processEvent(ctx, tx, event); err != nil { return err } // Record offset _, err = tx.ExecContext(ctx, \"INSERT INTO processed_events (idempotency_key) VALUES ($1)\", idempotencyKey, ) if err != nil { return err } return tx.Commit() } Message-Level Idempotency Producer-Assigned IDs Ensure messages have unique IDs from the producer:\n// Producer side type OrderMessage struct { OrderID string `json:\"order_id\"` // Unique ID CustomerID string `json:\"customer_id\"` Amount float64 `json:\"amount\"` } // Consumer side func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Use order.OrderID as idempotency key return p.processOrder(ctx, order.OrderID, order) } UUID Generation Generate UUIDs at the producer:\nimport \"github.com/google/uuid\" // Producer order := OrderMessage{ OrderID: uuid.New().String(), // Globally unique CustomerID: \"cust-123\", Amount: 99.99, } Advanced Patterns Timestamp-Based Deduplication Accept only newer messages:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var update Update if err := json.Unmarshal(msg.Value, \u0026update); err != nil { return err } // Only apply if newer than current result, err := p.db.ExecContext(ctx, `UPDATE entities SET value = $1, updated_at = $2 WHERE id = $3 AND (updated_at IS NULL OR updated_at \u003c $2)`, update.Value, update.Timestamp, // Must be set by producer update.EntityID, ) if err != nil { return err } rows, _ := result.RowsAffected() if rows == 0 { // Stale update, skip (already have newer data) return nil } return nil } Event Sourcing Natural idempotency through event deduplication:\ntype EventStore interface { Append(ctx context.Context, streamID string, event Event) error } type Processor struct { eventStore EventStore } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var event Event if err := json.Unmarshal(msg.Value, \u0026event); err != nil { return err } // Event store handles deduplication by event ID return p.eventStore.Append(ctx, event.StreamID, event) } Distributed Locks Use distributed locks for complex operations:\nimport \"github.com/go-redsync/redsync/v4\" type Processor struct { rs *redsync.Redsync db *sql.DB } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Acquire distributed lock mutex := p.rs.NewMutex( fmt.Sprintf(\"order-lock-%s\", order.OrderID), redsync.WithExpiry(10*time.Second), ) if err := mutex.LockContext(ctx); err != nil { return err } defer mutex.UnlockContext(ctx) // Check if processed var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM orders WHERE order_id = $1)\", order.OrderID, ).Scan(\u0026exists) if err != nil { return err } if exists { return nil } // Process order return p.processOrder(ctx, order) } Testing Idempotency Duplicate Message Test func TestProcessor_Idempotency(t *testing.T) { db := setupTestDB(t) processor := \u0026Processor{db: db} message := kafka.Message{ Value: []byte(`{\"order_id\":\"ord-123\",\"customer_id\":\"cust-456\",\"total\":99.99}`), } // Process first time err := processor.Process(context.Background(), message) require.NoError(t, err) // Verify inserted var count int db.QueryRow(\"SELECT COUNT(*) FROM orders WHERE order_id = 'ord-123'\").Scan(\u0026count) assert.Equal(t, 1, count) // Process duplicate err = processor.Process(context.Background(), message) require.NoError(t, err) // Verify still only one record db.QueryRow(\"SELECT COUNT(*) FROM orders WHERE order_id = 'ord-123'\").Scan(\u0026count) assert.Equal(t, 1, count, \"Duplicate message should not create new record\") } Concurrent Duplicate Test func TestProcessor_ConcurrentDuplicates(t *testing.T) { db := setupTestDB(t) processor := \u0026Processor{db: db} message := kafka.Message{ Value: []byte(`{\"order_id\":\"ord-123\",\"customer_id\":\"cust-456\",\"total\":99.99}`), } // Process same message concurrently var wg sync.WaitGroup errors := make([]error, 10) for i := 0; i \u003c 10; i++ { wg.Add(1) go func(idx int) { defer wg.Done() errors[idx] = processor.Process(context.Background(), message) }(i) } wg.Wait() // All should succeed (or fail with duplicate key) for _, err := range errors { if err != nil { assert.True(t, isDuplicateKeyError(err), \"Unexpected error: %v\", err) } } // Verify exactly one record var count int db.QueryRow(\"SELECT COUNT(*) FROM orders WHERE order_id = 'ord-123'\").Scan(\u0026count) assert.Equal(t, 1, count, \"Concurrent processing should create exactly one record\") } Performance Considerations Index Idempotency Keys CREATE INDEX idx_processed_orders_id ON processed_orders(order_id); CREATE INDEX idx_processed_events_key ON processed_events(idempotency_key); Cleanup Old Records Prevent unbounded growth:\n// Run cleanup periodically func (p *Processor) cleanupOldRecords(ctx context.Context) error { _, err := p.db.ExecContext(ctx, \"DELETE FROM processed_orders WHERE processed_at \u003c NOW() - INTERVAL '7 days'\", ) return err } Batch Lookups For high throughput, batch idempotency checks:\nfunc (p *Processor) ProcessBatch(ctx context.Context, messages []kafka.Message) error { // Extract all order IDs orderIDs := make([]string, len(messages)) for i, msg := range messages { var order Order json.Unmarshal(msg.Value, \u0026order) orderIDs[i] = order.OrderID } // Batch lookup rows, err := p.db.QueryContext(ctx, \"SELECT order_id FROM processed_orders WHERE order_id = ANY($1)\", pq.Array(orderIDs), ) if err != nil { return err } defer rows.Close() processed := make(map[string]bool) for rows.Next() { var id string rows.Scan(\u0026id) processed[id] = true } // Process only unprocessed messages for i, msg := range messages { if !processed[orderIDs[i]] { if err := p.processSingle(ctx, msg); err != nil { return err } } } return nil } Next Steps Explore Multi-Topic Processing patterns Learn about Observability for message tracing Configure Production Settings for deployment ","categories":"","description":"Handling duplicate messages in at-least-once processing","excerpt":"Handling duplicate messages in at-least-once processing","ref":"/humus/pr-preview/pr-396/features/queue/kafka/idempotency/","tags":"","title":"Idempotency"},{"body":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","ref":"/humus/pr-preview/pr-396/reference/job-package/","tags":"","title":"Job Package"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/advanced/otel-integration/","tags":"","title":"Otel Integration"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/features/job/use-cases/","tags":"","title":"Use Cases"},{"body":"Documentation coming soon.\nSee Getting Started for building new Humus services.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Getting Started for building new Humus …","ref":"/humus/pr-preview/pr-396/integration/bedrock-integration/","tags":"","title":"Bedrock Integration"},{"body":"Humus provides flexible error handling for REST APIs with support for custom error responses and RFC 7807 Problem Details.\nOverview Error handling in Humus follows a hierarchical approach:\nDefault Behavior - Logs errors and returns appropriate HTTP status codes Custom Error Handlers - Implement rest.ErrorHandler for custom error responses RFC 7807 Problem Details - Standardized error format with extension fields Default Error Handling By default, Humus logs all errors and returns HTTP status codes without response bodies:\nrest.Operation( http.MethodPost, rest.BasePath(\"/users\"), rest.HandleJson(handler), // No OnError option = uses default error handler ) Default behavior:\nErrors implementing rest.HttpResponseWriter control their own HTTP response Framework errors (rest.BadRequestError, rest.UnauthorizedError) return appropriate status codes All other errors return 500 Internal Server Error All errors are logged using the configured logger Custom Error Handlers Implement the rest.ErrorHandler interface to customize error responses:\ntype ErrorHandler interface { OnError(ctx context.Context, w http.ResponseWriter, err error) } Example: JSON Error Response type jsonErrorHandler struct { includeDetails bool } func (h *jsonErrorHandler) OnError(ctx context.Context, w http.ResponseWriter, err error) { response := map[string]string{ \"error\": \"An error occurred\", } if h.includeDetails { response[\"detail\"] = err.Error() } w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusInternalServerError) json.NewEncoder(w).Encode(response) } // Use the custom error handler rest.Operation( http.MethodPost, rest.BasePath(\"/users\"), rest.HandleJson(handler), rest.OnError(\u0026jsonErrorHandler{includeDetails: true}), ) ErrorHandler Function Adapter Use rest.ErrorHandlerFunc to create error handlers from functions:\nrest.Operation( http.MethodPost, rest.BasePath(\"/users\"), rest.HandleJson(handler), rest.OnError(rest.ErrorHandlerFunc(func(ctx context.Context, w http.ResponseWriter, err error) { w.Header().Set(\"Content-Type\", \"application/json\") w.WriteHeader(http.StatusInternalServerError) json.NewEncoder(w).Encode(map[string]string{\"error\": err.Error()}) })), ) RFC 7807 Problem Details Humus provides built-in support for RFC 7807 Problem Details, a standardized format for HTTP API error responses.\nBasic Usage handler := rest.NewProblemDetailsErrorHandler(rest.ProblemDetailsConfig{ DefaultType: \"https://api.example.com/errors\", IncludeDetails: nil, // nil = include details (default) }) rest.Operation( http.MethodPost, rest.BasePath(\"/users\"), rest.HandleJson(createUserHandler), rest.OnError(handler), ) Example error response:\n{ \"type\": \"https://api.example.com/errors/400\", \"title\": \"Bad Request\", \"status\": 400, \"detail\": \"Invalid email format\" } Custom Errors with Extension Fields Create type-safe custom errors by embedding rest.ProblemDetail:\ntype ValidationError struct { rest.ProblemDetail ValidationErrors []FieldError `json:\"validation_errors\"` } type FieldError struct { Field string `json:\"field\"` Message string `json:\"message\"` } func (e ValidationError) Error() string { return e.Detail } // Return from handler func createUser(ctx context.Context, req *CreateUserRequest) (*User, error) { if req.Email == \"\" { return nil, ValidationError{ ProblemDetail: rest.ProblemDetail{ Type: \"https://api.example.com/errors/validation\", Title: \"Validation Failed\", Status: http.StatusBadRequest, Detail: \"Request validation failed\", Instance: \"/users\", }, ValidationErrors: []FieldError{ {Field: \"email\", Message: \"Email is required\"}, {Field: \"name\", Message: \"Name must be at least 3 characters\"}, }, } } return \u0026User{}, nil } Response:\n{ \"type\": \"https://api.example.com/errors/validation\", \"title\": \"Validation Failed\", \"status\": 400, \"detail\": \"Request validation failed\", \"instance\": \"/users\", \"validation_errors\": [ {\"field\": \"email\", \"message\": \"Email is required\"}, {\"field\": \"name\", \"message\": \"Name must be at least 3 characters\"} ] } Configuration Options DefaultType Base URI for error types. Appended to the status code if the error doesn’t set a Type:\nconfig := rest.ProblemDetailsConfig{ DefaultType: \"https://api.example.com/errors\", } // Error without Type field will become: \"https://api.example.com/errors/500\" IncludeDetails Control whether to include error details in the response. Use nil (default true) for development, \u0026false for production:\n// Development - include error details devConfig := rest.ProblemDetailsConfig{ DefaultType: \"https://api.example.com/errors\", IncludeDetails: nil, // nil defaults to true } // Production - hide internal error details includeDetails := false prodConfig := rest.ProblemDetailsConfig{ DefaultType: \"https://api.example.com/errors\", IncludeDetails: \u0026includeDetails, } Logger Custom logger for error logging:\nlogger := slog.New(slog.NewJSONHandler(os.Stderr, \u0026slog.HandlerOptions{ Level: slog.LevelError, })) config := rest.ProblemDetailsConfig{ DefaultType: \"https://api.example.com/errors\", Logger: logger, } Error Detection Hierarchy The ProblemDetailsErrorHandler detects errors in this order:\nCustom errors embedding ProblemDetail - Serialized directly with all fields Framework errors (rest.BadRequestError, rest.UnauthorizedError, etc.) - Converted to standard Problem Details Generic errors - Wrapped as 500 Internal Server Error // Priority 1: Custom error with ProblemDetail (returns full object with extensions) return nil, ValidationError{ ProblemDetail: rest.ProblemDetail{...}, ValidationErrors: []FieldError{...}, } // Priority 2: Framework error (converted to Problem Details) return nil, rest.BadRequestError{Message: \"Invalid input\"} // Returns: {\"type\":\"about:blank\",\"title\":\"Bad Request\",\"status\":400,\"detail\":\"Invalid input\"} // Priority 3: Generic error (wrapped as 500) return nil, errors.New(\"database connection failed\") // Returns: {\"type\":\"about:blank\",\"title\":\"Internal Server Error\",\"status\":500} Framework Error Types Humus provides built-in error types that implement rest.HttpResponseWriter:\nBadRequestError Returns HTTP 400 Bad Request:\nreturn nil, rest.BadRequestError{Message: \"Invalid email format\"} Specialized bad request errors:\nrest.MissingRequiredParameterError - Missing required parameter (query, path, header) rest.InvalidParameterValueError - Invalid parameter value rest.InvalidContentTypeError - Unsupported Content-Type rest.InvalidJWTError - Invalid JWT token UnauthorizedError Returns HTTP 401 Unauthorized:\nreturn nil, rest.UnauthorizedError{Message: \"Invalid credentials\"} Common Error Patterns Validation Errors type ValidationError struct { rest.ProblemDetail Errors map[string][]string `json:\"errors\"` } func newValidationError(errors map[string][]string) ValidationError { return ValidationError{ ProblemDetail: rest.ProblemDetail{ Type: \"https://api.example.com/errors/validation\", Title: \"Validation Failed\", Status: http.StatusBadRequest, Detail: \"One or more validation errors occurred\", }, Errors: errors, } } // Usage if len(validationErrors) \u003e 0 { return nil, newValidationError(validationErrors) } Not Found Errors type NotFoundError struct { rest.ProblemDetail ResourceType string `json:\"resource_type\"` ResourceID string `json:\"resource_id\"` } func newNotFoundError(resourceType, resourceID string) NotFoundError { return NotFoundError{ ProblemDetail: rest.ProblemDetail{ Type: \"https://api.example.com/errors/not-found\", Title: \"Resource Not Found\", Status: http.StatusNotFound, Detail: fmt.Sprintf(\"%s with ID %s not found\", resourceType, resourceID), }, ResourceType: resourceType, ResourceID: resourceID, } } Rate Limiting Errors type RateLimitError struct { rest.ProblemDetail RetryAfter int `json:\"retry_after\"` Limit int `json:\"limit\"` Window string `json:\"window\"` } func newRateLimitError(retryAfter, limit int, window string) RateLimitError { return RateLimitError{ ProblemDetail: rest.ProblemDetail{ Type: \"https://api.example.com/errors/rate-limit\", Title: \"Rate Limit Exceeded\", Status: http.StatusTooManyRequests, Detail: fmt.Sprintf(\"Rate limit of %d requests per %s exceeded\", limit, window), }, RetryAfter: retryAfter, Limit: limit, Window: window, } } Conflict Errors type ConflictError struct { rest.ProblemDetail ConflictingField string `json:\"conflicting_field\"` ExistingValue string `json:\"existing_value\"` } func newConflictError(field, value string) ConflictError { return ConflictError{ ProblemDetail: rest.ProblemDetail{ Type: \"https://api.example.com/errors/conflict\", Title: \"Resource Conflict\", Status: http.StatusConflict, Detail: fmt.Sprintf(\"A resource with %s=%s already exists\", field, value), }, ConflictingField: field, ExistingValue: value, } } Best Practices Define Error Type Constants Define error type URIs as constants for consistency:\nconst ( ErrTypeValidation = \"https://api.example.com/errors/validation\" ErrTypeNotFound = \"https://api.example.com/errors/not-found\" ErrTypeRateLimit = \"https://api.example.com/errors/rate-limit\" ErrTypeConflict = \"https://api.example.com/errors/conflict\" ) type ValidationError struct { rest.ProblemDetail Errors map[string][]string `json:\"errors\"` } func newValidationError(errors map[string][]string) ValidationError { return ValidationError{ ProblemDetail: rest.ProblemDetail{ Type: ErrTypeValidation, // Use constant Title: \"Validation Failed\", Status: http.StatusBadRequest, }, Errors: errors, } } Use Constructor Functions Encapsulate error creation logic in constructor functions:\nfunc newNotFoundError(resourceType, resourceID string) NotFoundError { return NotFoundError{ ProblemDetail: rest.ProblemDetail{ Type: ErrTypeNotFound, Title: \"Resource Not Found\", Status: http.StatusNotFound, Detail: fmt.Sprintf(\"%s with ID %s not found\", resourceType, resourceID), }, ResourceType: resourceType, ResourceID: resourceID, } } // Usage user, err := db.GetUser(userID) if err != nil { return nil, newNotFoundError(\"User\", userID) } Implement Error() Method Always implement the Error() method for custom error types:\ntype ValidationError struct { rest.ProblemDetail Errors map[string][]string `json:\"errors\"` } func (e ValidationError) Error() string { return e.Detail // Or construct custom message } Hide Internal Errors in Production Use IncludeDetails: \u0026false in production to prevent leaking internal error messages:\n// Development devHandler := rest.NewProblemDetailsErrorHandler(rest.ProblemDetailsConfig{ DefaultType: \"https://api.example.com/errors\", IncludeDetails: nil, // Include details }) // Production includeDetails := false prodHandler := rest.NewProblemDetailsErrorHandler(rest.ProblemDetailsConfig{ DefaultType: \"https://api.example.com/errors\", IncludeDetails: \u0026includeDetails, // Hide internal details }) Use Extension Fields for Structured Data Add extension fields beyond the RFC 7807 standard fields for rich error information:\ntype ValidationError struct { rest.ProblemDetail ValidationErrors []FieldError `json:\"validation_errors\"` // Extension field Timestamp time.Time `json:\"timestamp\"` // Extension field RequestID string `json:\"request_id\"` // Extension field } Complete Example package endpoint import ( \"context\" \"net/http\" \"github.com/z5labs/humus/rest\" ) const ( ErrTypeValidation = \"https://api.example.com/errors/validation\" ErrTypeNotFound = \"https://api.example.com/errors/not-found\" ) type ValidationError struct { rest.ProblemDetail Errors map[string][]string `json:\"errors\"` } func (e ValidationError) Error() string { return e.Detail } type CreateUserRequest struct { Name string `json:\"name\"` Email string `json:\"email\"` } type CreateUserResponse struct { ID string `json:\"id\"` } type createUserHandler struct { // dependencies } func CreateUser(ctx context.Context) rest.ApiOption { handler := \u0026createUserHandler{} // Configure Problem Details error handler includeDetails := false // Production mode errorHandler := rest.NewProblemDetailsErrorHandler(rest.ProblemDetailsConfig{ DefaultType: \"https://api.example.com/errors\", IncludeDetails: \u0026includeDetails, }) return rest.Operation( http.MethodPost, rest.BasePath(\"/users\"), rest.HandleJson(handler), rest.OnError(errorHandler), ) } func (h *createUserHandler) Handle(ctx context.Context, req *CreateUserRequest) (*CreateUserResponse, error) { // Validate request validationErrors := make(map[string][]string) if req.Name == \"\" { validationErrors[\"name\"] = []string{\"Name is required\"} } if req.Email == \"\" { validationErrors[\"email\"] = []string{\"Email is required\"} } if len(validationErrors) \u003e 0 { return nil, ValidationError{ ProblemDetail: rest.ProblemDetail{ Type: ErrTypeValidation, Title: \"Validation Failed\", Status: http.StatusBadRequest, Detail: \"Request validation failed\", Instance: \"/users\", }, Errors: validationErrors, } } // Create user... return \u0026CreateUserResponse{ID: \"123\"}, nil } See Also RFC 7807 Problem Details Specification API Reference Problem Details Example ","categories":"","description":"Custom error responses and RFC 7807 Problem Details","excerpt":"Custom error responses and RFC 7807 Problem Details","ref":"/humus/pr-preview/pr-396/features/rest/error-handling/","tags":"","title":"Error Handling"},{"body":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","ref":"/humus/pr-preview/pr-396/reference/health-package/","tags":"","title":"Health Package"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/features/grpc/interceptors/","tags":"","title":"Interceptors"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/advanced/middleware/","tags":"","title":"Middleware"},{"body":"The Kafka runtime supports consuming and processing multiple topics simultaneously, each with its own processor and delivery semantics.\nBasic Multi-Topic Configuration Configure multiple topics in a single runtime:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.AtLeastOnce(\"payments\", paymentsProcessor), kafka.AtMostOnce(\"metrics\", metricsProcessor), ) Key features:\nEach topic has its own processor Different delivery semantics per topic All topics share the same consumer group Partitions from all topics processed concurrently Processor per Topic Define separate processors for each topic:\ntype OrdersProcessor struct { db *sql.DB } func (p *OrdersProcessor) Process(ctx context.Context, msg kafka.Message) error { var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } return p.processOrder(ctx, order) } type PaymentsProcessor struct { db *sql.DB } func (p *PaymentsProcessor) Process(ctx context.Context, msg kafka.Message) error { var payment Payment if err := json.Unmarshal(msg.Value, \u0026payment); err != nil { return err } return p.processPayment(ctx, payment) } func Init(ctx context.Context, cfg Config) (*queue.App, error) { ordersProc := \u0026OrdersProcessor{db: cfg.DB} paymentsProc := \u0026PaymentsProcessor{db: cfg.DB} runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(\"orders\", ordersProc), kafka.AtLeastOnce(\"payments\", paymentsProc), ) return queue.NewApp(runtime), nil } Shared State Between Topics Processors can share state:\ntype SharedProcessor struct { mu sync.RWMutex cache map[string]string db *sql.DB } type OrdersProcessor struct { *SharedProcessor } func (p *OrdersProcessor) Process(ctx context.Context, msg kafka.Message) error { var order Order json.Unmarshal(msg.Value, \u0026order) // Access shared cache p.mu.RLock() customerData := p.cache[order.CustomerID] p.mu.RUnlock() return p.processOrder(ctx, order, customerData) } type PaymentsProcessor struct { *SharedProcessor } func (p *PaymentsProcessor) Process(ctx context.Context, msg kafka.Message) error { var payment Payment json.Unmarshal(msg.Value, \u0026payment) // Update shared cache p.mu.Lock() p.cache[payment.CustomerID] = payment.Status p.mu.Unlock() return p.processPayment(ctx, payment) } func Init(ctx context.Context, cfg Config) (*queue.App, error) { shared := \u0026SharedProcessor{ cache: make(map[string]string), db: cfg.DB, } runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(\"orders\", \u0026OrdersProcessor{shared}), kafka.AtLeastOnce(\"payments\", \u0026PaymentsProcessor{shared}), ) return queue.NewApp(runtime), nil } Topic-Based Routing Route messages by topic to a unified handler:\ntype UnifiedProcessor struct { db *sql.DB } func (p *UnifiedProcessor) Process(ctx context.Context, msg kafka.Message) error { switch msg.Topic { case \"orders\": return p.processOrder(ctx, msg) case \"payments\": return p.processPayment(ctx, msg) case \"shipments\": return p.processShipment(ctx, msg) default: return fmt.Errorf(\"unknown topic: %s\", msg.Topic) } } func (p *UnifiedProcessor) processOrder(ctx context.Context, msg kafka.Message) error { var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Process order return nil } func (p *UnifiedProcessor) processPayment(ctx context.Context, msg kafka.Message) error { var payment Payment if err := json.Unmarshal(msg.Value, \u0026payment); err != nil { return err } // Process payment return nil } func Init(ctx context.Context, cfg Config) (*queue.App, error) { processor := \u0026UnifiedProcessor{db: cfg.DB} runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(\"orders\", processor), kafka.AtLeastOnce(\"payments\", processor), kafka.AtLeastOnce(\"shipments\", processor), ) return queue.NewApp(runtime), nil } Mixed Delivery Semantics Use different semantics for different topics:\nruntime := kafka.NewRuntime( brokers, groupID, // Critical topics: at-least-once kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.AtLeastOnce(\"payments\", paymentsProcessor), kafka.AtLeastOnce(\"inventory\", inventoryProcessor), // Non-critical topics: at-most-once kafka.AtMostOnce(\"metrics\", metricsProcessor), kafka.AtMostOnce(\"logs\", logsProcessor), kafka.AtMostOnce(\"analytics\", analyticsProcessor), ) Rationale:\nOrders, payments, inventory: Cannot lose data → at-least-once Metrics, logs, analytics: Can tolerate loss → at-most-once Configuration YAML Configuration kafka: brokers: - \"localhost:9092\" group_id: \"multi-topic-processor\" topics: orders: semantic: \"at-least-once\" payments: semantic: \"at-least-once\" metrics: semantic: \"at-most-once\" Dynamic Topic Configuration type TopicConfig struct { Name string Semantic string } type Config struct { queue.Config `config:\",squash\"` Kafka struct { Brokers []string `config:\"brokers\"` GroupID string `config:\"group_id\"` Topics []TopicConfig `config:\"topics\"` } `config:\"kafka\"` } func Init(ctx context.Context, cfg Config) (*queue.App, error) { opts := make([]kafka.Option, 0, len(cfg.Kafka.Topics)) for _, topic := range cfg.Kafka.Topics { switch topic.Semantic { case \"at-least-once\": processor := newProcessor(topic.Name) opts = append(opts, kafka.AtLeastOnce(topic.Name, processor)) case \"at-most-once\": processor := newProcessor(topic.Name) opts = append(opts, kafka.AtMostOnce(topic.Name, processor)) } } runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, opts..., ) return queue.NewApp(runtime), nil } Workflow Patterns Sequential Processing Process related messages across topics:\ntype WorkflowProcessor struct { db *sql.DB } // Orders topic func (p *WorkflowProcessor) ProcessOrder(ctx context.Context, msg kafka.Message) error { var order Order json.Unmarshal(msg.Value, \u0026order) // 1. Save order if err := p.saveOrder(ctx, order); err != nil { return err } // 2. Check payment (from payments topic) // This will be processed by ProcessPayment when payment arrives return nil } // Payments topic func (p *WorkflowProcessor) ProcessPayment(ctx context.Context, msg kafka.Message) error { var payment Payment json.Unmarshal(msg.Value, \u0026payment) // 1. Save payment if err := p.savePayment(ctx, payment); err != nil { return err } // 2. Update order status return p.updateOrderStatus(ctx, payment.OrderID, \"paid\") } Event Aggregation Aggregate events from multiple topics:\ntype AggregationProcessor struct { mu sync.RWMutex aggregates map[string]*Aggregate } type Aggregate struct { OrderReceived bool PaymentReceived bool ShipmentReceived bool } func (p *AggregationProcessor) ProcessOrder(ctx context.Context, msg kafka.Message) error { var order Order json.Unmarshal(msg.Value, \u0026order) p.mu.Lock() defer p.mu.Unlock() agg := p.getAggregate(order.OrderID) agg.OrderReceived = true if p.isComplete(agg) { return p.finalizeOrder(ctx, order.OrderID) } return nil } func (p *AggregationProcessor) ProcessPayment(ctx context.Context, msg kafka.Message) error { var payment Payment json.Unmarshal(msg.Value, \u0026payment) p.mu.Lock() defer p.mu.Unlock() agg := p.getAggregate(payment.OrderID) agg.PaymentReceived = true if p.isComplete(agg) { return p.finalizeOrder(ctx, payment.OrderID) } return nil } func (p *AggregationProcessor) isComplete(agg *Aggregate) bool { return agg.OrderReceived \u0026\u0026 agg.PaymentReceived \u0026\u0026 agg.ShipmentReceived } Topic Fan-In Multiple topics feed into one aggregator:\ntype EventAggregator struct { db *sql.DB } func (p *EventAggregator) Process(ctx context.Context, msg kafka.Message) error { // Common event envelope type Event struct { Type string `json:\"type\"` Timestamp time.Time `json:\"timestamp\"` Data json.RawMessage `json:\"data\"` } var event Event if err := json.Unmarshal(msg.Value, \u0026event); err != nil { return err } // Store in unified events table _, err := p.db.ExecContext(ctx, `INSERT INTO events (topic, type, timestamp, data) VALUES ($1, $2, $3, $4)`, msg.Topic, event.Type, event.Timestamp, event.Data, ) return err } func Init(ctx context.Context, cfg Config) (*queue.App, error) { aggregator := \u0026EventAggregator{db: cfg.DB} runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, // All topics use the same aggregator kafka.AtLeastOnce(\"user-events\", aggregator), kafka.AtLeastOnce(\"order-events\", aggregator), kafka.AtLeastOnce(\"payment-events\", aggregator), ) return queue.NewApp(runtime), nil } Partition Distribution How Partitions Are Assigned With multiple topics, partitions from all topics are distributed:\nConsumer Group \"processors\" with 2 consumers: Consumer 1: Consumer 2: orders (partition 0) orders (partition 1) orders (partition 2) payments (partition 0) payments (partition 1) payments (partition 2) Key points:\nPartitions assigned across all topics Load balanced across consumers No guarantee which consumer gets which topic Topic Affinity To ensure a consumer processes specific topics, use separate consumer groups:\n// Separate runtimes for different topics func Init(ctx context.Context, cfg Config) ([]*queue.App, error) { // Runtime 1: Critical topics runtime1 := kafka.NewRuntime( cfg.Kafka.Brokers, \"critical-processor\", // Different group ID kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.AtLeastOnce(\"payments\", paymentsProcessor), ) // Runtime 2: Analytics topics runtime2 := kafka.NewRuntime( cfg.Kafka.Brokers, \"analytics-processor\", // Different group ID kafka.AtMostOnce(\"metrics\", metricsProcessor), kafka.AtMostOnce(\"logs\", logsProcessor), ) return []*queue.App{ queue.NewApp(runtime1), queue.NewApp(runtime2), }, nil } Note: This requires running multiple apps, which is outside the standard pattern. Consider separate deployments instead.\nMonitoring Multi-Topic Processing Topic-Level Metrics Monitor lag per topic:\nkafka-consumer-groups.sh \\ --bootstrap-server localhost:9092 \\ --group multi-topic-processor \\ --describe Output:\nTOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG orders 0 1000 1000 0 orders 1 1050 1050 0 payments 0 500 520 20 # Lagging payments 1 510 510 0 metrics 0 5000 5000 0 Processor-Level Logging Log which processor handles each message:\nfunc (p *OrdersProcessor) Process(ctx context.Context, msg kafka.Message) error { log.Info(\"Processing order\", \"topic\", msg.Topic, \"partition\", msg.Partition, \"offset\", msg.Offset, \"processor\", \"orders\", ) return nil } Best Practices Separate Concerns Use different processors for different business domains:\n// Good: Separate processors kafka.AtLeastOnce(\"orders\", ordersProcessor) kafka.AtLeastOnce(\"payments\", paymentsProcessor) // Avoid: One processor for everything kafka.AtLeastOnce(\"orders\", genericProcessor) kafka.AtLeastOnce(\"payments\", genericProcessor) Match Semantics to Criticality // Critical: At-least-once kafka.AtLeastOnce(\"financial-transactions\", processor) // Non-critical: At-most-once kafka.AtMostOnce(\"usage-metrics\", processor) Shared State Needs Locks type Processor struct { mu sync.RWMutex state map[string]int } // Safe: Uses locks func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { p.mu.Lock() p.state[string(msg.Key)]++ p.mu.Unlock() return nil } Independent Topic Processing Avoid dependencies between topics when possible:\n// Good: Independent processing func (p *OrdersProcessor) Process(ctx context.Context, msg kafka.Message) error { // Only processes orders, no dependencies return p.saveOrder(ctx, msg) } // Avoid: Cross-topic dependencies func (p *OrdersProcessor) Process(ctx context.Context, msg kafka.Message) error { // Waiting for payment from another topic creates coupling payment := p.waitForPayment(msg.OrderID) // Anti-pattern return p.saveOrder(ctx, msg, payment) } Next Steps Learn about Observability for multi-topic tracing Configure Production Settings for optimal performance Review Idempotency patterns for each topic ","categories":"","description":"Processing multiple Kafka topics in one runtime","excerpt":"Processing multiple Kafka topics in one runtime","ref":"/humus/pr-preview/pr-396/features/queue/kafka/multi-topic/","tags":"","title":"Multi-Topic Processing"},{"body":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","ref":"/humus/pr-preview/pr-396/reference/config-package/","tags":"","title":"Config Package"},{"body":"The Kafka runtime provides comprehensive OpenTelemetry integration with automatic tracing, metrics, and structured logging for message processing.\nOverview Observability is built-in at every level:\nAutomatic Tracing - Spans created for each message via franz-go kotel plugin Context Propagation - Distributed tracing across services Structured Logging - Message metadata in log entries Metrics - Consumer lag, processing rates, errors (via OTel SDK) No manual instrumentation needed in your processor code.\nTracing Automatic Span Creation Every message gets a processing span automatically:\nSpan: kafka.process ├─ topic: \"orders\" ├─ partition: 0 ├─ offset: 12345 ├─ group_id: \"order-processors\" └─ duration: 45ms Trace Propagation Trace context is automatically extracted from Kafka headers:\nProducer (orders-api): HTTP Request → Span A (trace-id: abc123) └─\u003e Publish to Kafka (inject trace-id into headers) Consumer (order-processor): Consume from Kafka → Extract trace-id from headers └─\u003e Span B (trace-id: abc123, parent: Span A) This creates a distributed trace across services.\nCustom Spans in Processor Add child spans for detailed tracing:\nimport ( \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/trace\" ) var tracer = otel.Tracer(\"order-processor\") type OrderProcessor struct { db *sql.DB } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Parent span already created by kafka runtime // Add custom child span ctx, span := tracer.Start(ctx, \"deserialize-order\") var order Order err := json.Unmarshal(msg.Value, \u0026order) span.End() if err != nil { return err } // Another span for database operation ctx, span = tracer.Start(ctx, \"save-order\") defer span.End() _, err = p.db.ExecContext(ctx, \"INSERT INTO orders (order_id, total) VALUES ($1, $2)\", order.OrderID, order.Total, ) if err != nil { span.RecordError(err) } return err } Span Attributes Add custom attributes to spans:\nimport \"go.opentelemetry.io/otel/attribute\" func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { ctx, span := tracer.Start(ctx, \"process-order\") defer span.End() var order Order json.Unmarshal(msg.Value, \u0026order) // Add business context to span span.SetAttributes( attribute.String(\"order.id\", order.OrderID), attribute.String(\"customer.id\", order.CustomerID), attribute.Float64(\"order.total\", order.Total), attribute.String(\"order.status\", order.Status), ) return p.processOrder(ctx, order) } Logging Structured Logging with slog Use the built-in logger with Kafka attributes:\nimport ( \"log/slog\" \"github.com/z5labs/humus/queue/kafka\" ) func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order Order json.Unmarshal(msg.Value, \u0026order) log.InfoContext(ctx, \"Processing order\", kafka.TopicAttr(msg.Topic), kafka.PartitionAttr(msg.Partition), kafka.OffsetAttr(msg.Offset), slog.String(\"order_id\", order.OrderID), slog.Float64(\"amount\", order.Total), ) if err := p.saveOrder(ctx, order); err != nil { log.ErrorContext(ctx, \"Failed to save order\", kafka.TopicAttr(msg.Topic), kafka.PartitionAttr(msg.Partition), kafka.OffsetAttr(msg.Offset), slog.String(\"order_id\", order.OrderID), slog.Any(\"error\", err), ) return err } return nil } Available Kafka Attributes The kafka package provides slog attributes:\n// Kafka-specific attributes kafka.GroupIDAttr(groupID string) // Consumer group ID kafka.TopicAttr(topic string) // Topic name kafka.PartitionAttr(partition int32) // Partition number kafka.OffsetAttr(offset int64) // Message offset Example:\nlog.Info(\"Consumer group started\", kafka.GroupIDAttr(\"order-processors\"), kafka.TopicAttr(\"orders\"), ) Log Correlation with Traces Logs are automatically correlated with traces when using log/slog with context:\nfunc (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Log with context - automatically includes trace ID log.InfoContext(ctx, \"Processing message\", kafka.TopicAttr(msg.Topic), kafka.OffsetAttr(msg.Offset), ) // This log will have the same trace ID in your logging backend return nil } Metrics Automatic Metrics The Kafka runtime automatically exports metrics:\nConsumer Metrics:\nConsumer lag per partition Messages consumed per second Bytes consumed per second Fetch latency Commit latency Processing Metrics:\nMessages processed per second Processing errors per second Processing duration histogram Custom Metrics Add application-specific metrics:\nimport ( \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/metric\" ) type OrderProcessor struct { db *sql.DB ordersProcessed metric.Int64Counter orderValue metric.Float64Histogram } func NewOrderProcessor(db *sql.DB) (*OrderProcessor, error) { meter := otel.Meter(\"order-processor\") ordersProcessed, err := meter.Int64Counter( \"orders.processed\", metric.WithDescription(\"Number of orders processed\"), ) if err != nil { return nil, err } orderValue, err := meter.Float64Histogram( \"orders.value\", metric.WithDescription(\"Order total value\"), metric.WithUnit(\"USD\"), ) if err != nil { return nil, err } return \u0026OrderProcessor{ db: db, ordersProcessed: ordersProcessed, orderValue: orderValue, }, nil } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order Order json.Unmarshal(msg.Value, \u0026order) // Process order if err := p.saveOrder(ctx, order); err != nil { return err } // Record metrics p.ordersProcessed.Add(ctx, 1, metric.WithAttributes( attribute.String(\"status\", order.Status), ), ) p.orderValue.Record(ctx, order.Total, metric.WithAttributes( attribute.String(\"customer.id\", order.CustomerID), ), ) return nil } Configuration Enable OpenTelemetry Configure OTel in your config.yaml:\notel: service: name: \"order-processor\" version: \"1.0.0\" sdk: disabled: false # Enable OTel exporter: otlp: endpoint: \"localhost:4317\" protocol: grpc insecure: true traces: sampler: type: \"always_on\" # Or \"traceidratio\" for sampling metrics: interval: 60s # Export interval OTLP Exporter Export to an OTLP collector (Jaeger, Tempo, etc.):\notel: exporter: otlp: endpoint: \"otel-collector:4317\" protocol: grpc headers: - key: \"authorization\" value: \"Bearer {{env \\\"OTEL_TOKEN\\\"}}\" Sampling Configure trace sampling for high-throughput scenarios:\notel: traces: sampler: type: \"traceidratio\" arg: 0.1 # Sample 10% of traces Visualization Jaeger UI View distributed traces in Jaeger:\nTrace: Process Order (trace-id: abc123) │ ├─ HTTP POST /orders [orders-api] 250ms │ └─ kafka.publish [orders-api] 5ms │ └─ kafka.process [order-processor] 45ms ├─ deserialize-order 2ms ├─ save-order 40ms │ └─ sql.insert 38ms └─ publish-event 3ms Grafana Dashboard Monitor Kafka consumer metrics:\nKey Dashboard Panels:\nConsumer Lag\nkafka_consumer_lag{group=\"order-processors\",topic=\"orders\"} Messages Processed per Second\nrate(kafka_messages_processed_total{topic=\"orders\"}[1m]) Processing Duration\nhistogram_quantile(0.99, rate(kafka_processing_duration_seconds_bucket[5m]) ) Error Rate\nrate(kafka_processing_errors_total[1m]) Debugging Find Slow Messages Use trace queries to find slow processing:\nJaeger Query:\nservice=\"order-processor\" minDuration=1s This finds all messages that took over 1 second to process.\nIdentify Error Patterns Find errors in logs correlated with traces:\nLog Query (Loki):\n{service=\"order-processor\"} |= \"error\" | json | trace_id=\"abc123\" Monitor Partition Lag Check lag per partition:\nkafka-consumer-groups.sh \\ --bootstrap-server localhost:9092 \\ --group order-processors \\ --describe Correlate with processing traces to find bottlenecks.\nBest Practices Always Use Context Pass context through your call chain for trace propagation:\nfunc (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Good: Pass context return p.saveOrder(ctx, msg) } func (p *OrderProcessor) saveOrder(ctx context.Context, msg kafka.Message) error { // Good: Use context in DB calls _, err := p.db.ExecContext(ctx, ...) return err } Add Business Context Include business IDs in spans and logs:\nspan.SetAttributes( attribute.String(\"order.id\", order.OrderID), attribute.String(\"customer.id\", order.CustomerID), ) log.InfoContext(ctx, \"Order processed\", slog.String(\"order_id\", order.OrderID), slog.String(\"customer_id\", order.CustomerID), ) Sample High-Volume Topics For high-throughput topics, use sampling:\notel: traces: sampler: type: \"traceidratio\" arg: 0.01 # 1% sampling for high volume Monitor Consumer Lag Set up alerts for increasing lag:\n# Prometheus alert - alert: KafkaConsumerLag expr: kafka_consumer_lag{group=\"order-processors\"} \u003e 1000 for: 5m annotations: summary: \"Consumer group {{ $labels.group }} is lagging\" Use Structured Logging Always use structured logs (slog), not formatted strings:\n// Good: Structured log.InfoContext(ctx, \"Order processed\", slog.String(\"order_id\", order.OrderID), slog.Int64(\"partition\", msg.Partition), ) // Bad: Unstructured log.Printf(\"Order %s processed on partition %d\", order.OrderID, msg.Partition) Example: Complete Observability package main import ( \"context\" \"encoding/json\" \"log/slog\" \"github.com/z5labs/humus/queue\" \"github.com/z5labs/humus/queue/kafka\" \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/attribute\" \"go.opentelemetry.io/otel/metric\" ) var tracer = otel.Tracer(\"order-processor\") type OrderProcessor struct { db *sql.DB ordersProcessed metric.Int64Counter } func NewOrderProcessor(db *sql.DB) (*OrderProcessor, error) { meter := otel.Meter(\"order-processor\") counter, err := meter.Int64Counter(\"orders.processed\") if err != nil { return nil, err } return \u0026OrderProcessor{ db: db, ordersProcessed: counter, }, nil } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Create custom span ctx, span := tracer.Start(ctx, \"process-order\") defer span.End() // Deserialize var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { slog.ErrorContext(ctx, \"Failed to deserialize\", kafka.TopicAttr(msg.Topic), kafka.OffsetAttr(msg.Offset), slog.Any(\"error\", err), ) span.RecordError(err) return err } // Add attributes span.SetAttributes( attribute.String(\"order.id\", order.OrderID), attribute.Float64(\"order.total\", order.Total), ) // Log processing slog.InfoContext(ctx, \"Processing order\", kafka.TopicAttr(msg.Topic), kafka.PartitionAttr(msg.Partition), kafka.OffsetAttr(msg.Offset), slog.String(\"order_id\", order.OrderID), ) // Save to database if err := p.saveOrder(ctx, order); err != nil { slog.ErrorContext(ctx, \"Failed to save order\", kafka.TopicAttr(msg.Topic), kafka.OffsetAttr(msg.Offset), slog.String(\"order_id\", order.OrderID), slog.Any(\"error\", err), ) span.RecordError(err) return err } // Record metrics p.ordersProcessed.Add(ctx, 1, metric.WithAttributes( attribute.String(\"status\", order.Status), ), ) slog.InfoContext(ctx, \"Order processed successfully\", slog.String(\"order_id\", order.OrderID), ) return nil } func (p *OrderProcessor) saveOrder(ctx context.Context, order Order) error { ctx, span := tracer.Start(ctx, \"save-order-db\") defer span.End() _, err := p.db.ExecContext(ctx, \"INSERT INTO orders (order_id, total) VALUES ($1, $2)\", order.OrderID, order.Total, ) if err != nil { span.RecordError(err) } return err } Next Steps Configure Production Settings for deployment Review Quick Start for complete examples Explore Message Structure for additional context ","categories":"","description":"OpenTelemetry integration for Kafka processing","excerpt":"OpenTelemetry integration for Kafka processing","ref":"/humus/pr-preview/pr-396/features/queue/kafka/observability/","tags":"","title":"Observability"},{"body":"Humus automatically generates OpenAPI 3.0 specifications for all REST APIs, providing comprehensive API documentation with zero manual effort.\nOverview Every Humus REST API includes:\nAutomatic OpenAPI 3.0 schema generation from Go types Live specification endpoint at GET /openapi.json Request/response schemas via reflection Parameter validation rules in the specification Security scheme documentation for authentication No manual annotation required - schemas generated from code This enables seamless integration with tools like Swagger UI, Postman, ReDoc, and API client generators.\nAccessing the OpenAPI Specification Every API created with rest.NewApi() automatically serves its OpenAPI specification:\n# Get the full OpenAPI spec curl http://localhost:8080/openapi.json # Pretty-print with jq curl http://localhost:8080/openapi.json | jq Basic Example package main import ( \"context\" \"net/http\" \"github.com/z5labs/humus/rest\" ) type CreateUserRequest struct { Email string `json:\"email\"` Name string `json:\"name\"` Age int `json:\"age,omitempty\"` } type UserResponse struct { ID string `json:\"id\"` Email string `json:\"email\"` Name string `json:\"name\"` Age int `json:\"age,omitempty\"` Created string `json:\"created\"` } func Init(ctx context.Context, cfg rest.Config) (*rest.Api, error) { handler := rest.HandlerFunc[CreateUserRequest, UserResponse]( func(ctx context.Context, req *CreateUserRequest) (*UserResponse, error) { // Implementation return \u0026UserResponse{ ID: \"user-123\", Email: req.Email, Name: req.Name, Age: req.Age, Created: \"2025-01-15T10:30:00Z\", }, nil }, ) api := rest.NewApi( \"User Management API\", \"1.0.0\", rest.Handle( http.MethodPost, rest.BasePath(\"/users\"), rest.HandleJson(handler), ), ) return api, nil } Generated OpenAPI Schema:\n{ \"openapi\": \"3.0\", \"info\": { \"title\": \"User Management API\", \"version\": \"1.0.0\" }, \"paths\": { \"/users\": { \"post\": { \"requestBody\": { \"content\": { \"application/json\": { \"schema\": { \"type\": \"object\", \"properties\": { \"email\": {\"type\": \"string\"}, \"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"} }, \"required\": [\"email\", \"name\"] } } } }, \"responses\": { \"200\": { \"content\": { \"application/json\": { \"schema\": { \"type\": \"object\", \"properties\": { \"id\": {\"type\": \"string\"}, \"email\": {\"type\": \"string\"}, \"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"}, \"created\": {\"type\": \"string\"} }, \"required\": [\"id\", \"email\", \"name\", \"created\"] } } } } } } } } } Schema Generation Humus uses reflection via github.com/swaggest/jsonschema-go to automatically generate JSON schemas from Go types.\nBasic Types Go types map to JSON schema types:\ntype Product struct { Name string `json:\"name\"` // \"type\": \"string\" Price float64 `json:\"price\"` // \"type\": \"number\" Quantity int `json:\"quantity\"` // \"type\": \"integer\" Available bool `json:\"available\"` // \"type\": \"boolean\" Tags []string `json:\"tags\"` // \"type\": \"array\", \"items\": {\"type\": \"string\"} } Optional Fields Use omitempty to mark fields as optional:\ntype User struct { ID string `json:\"id\"` // Required Email string `json:\"email\"` // Required Phone string `json:\"phone,omitempty\"` // Optional } Generated Schema:\n{ \"type\": \"object\", \"properties\": { \"id\": {\"type\": \"string\"}, \"email\": {\"type\": \"string\"}, \"phone\": {\"type\": \"string\"} }, \"required\": [\"id\", \"email\"] } Nested Objects Nested structs are automatically expanded:\ntype Address struct { Street string `json:\"street\"` City string `json:\"city\"` Country string `json:\"country\"` } type Customer struct { Name string `json:\"name\"` Address Address `json:\"address\"` } Generated Schema:\n{ \"type\": \"object\", \"properties\": { \"name\": {\"type\": \"string\"}, \"address\": { \"type\": \"object\", \"properties\": { \"street\": {\"type\": \"string\"}, \"city\": {\"type\": \"string\"}, \"country\": {\"type\": \"string\"} }, \"required\": [\"street\", \"city\", \"country\"] } }, \"required\": [\"name\", \"address\"] } Arrays and Slices type Catalog struct { Products []Product `json:\"products\"` Categories []string `json:\"categories\"` Tags map[string]string `json:\"tags\"` } Generated Schema:\n{ \"type\": \"object\", \"properties\": { \"products\": { \"type\": \"array\", \"items\": { \"$ref\": \"#/components/schemas/Product\" } }, \"categories\": { \"type\": \"array\", \"items\": {\"type\": \"string\"} }, \"tags\": { \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"} } } } Pointers Pointer types become optional in the schema:\ntype UpdateRequest struct { Name *string `json:\"name,omitempty\"` // Optional, can be null Email *string `json:\"email,omitempty\"` // Optional, can be null } Enumerations Use type aliases or constants for enumerations:\ntype Status string const ( StatusPending Status = \"pending\" StatusActive Status = \"active\" StatusCompleted Status = \"completed\" ) type Order struct { ID string `json:\"id\"` Status Status `json:\"status\"` } Note: Basic Go enums don’t automatically generate enum constraints in the schema. For strict validation, implement custom schema methods or use validation in your handler.\nTime and Date Use time.Time for timestamps:\nimport \"time\" type Event struct { Name string `json:\"name\"` StartTime time.Time `json:\"start_time\"` EndTime time.Time `json:\"end_time,omitempty\"` } Generated Schema:\n{ \"type\": \"object\", \"properties\": { \"name\": {\"type\": \"string\"}, \"start_time\": {\"type\": \"string\", \"format\": \"date-time\"}, \"end_time\": {\"type\": \"string\", \"format\": \"date-time\"} }, \"required\": [\"name\", \"start_time\"] } Path Parameters Path parameters are automatically included in the OpenAPI spec:\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\").Param(\"id\").Path(\"orders\").Param(\"orderId\"), getOrderHandler, ) Generated Operation:\n{ \"paths\": { \"/users/{id}/orders/{orderId}\": { \"get\": { \"parameters\": [ { \"name\": \"id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\"} }, { \"name\": \"orderId\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\"} } ] } } } } Query Parameters Query parameters defined with rest.QueryParam() appear in the spec:\nrest.Handle( http.MethodGet, rest.BasePath(\"/search\"), searchHandler, rest.QueryParam(\"q\", rest.Required()), rest.QueryParam(\"limit\", rest.Regex(regexp.MustCompile(`^\\d+$`))), rest.QueryParam(\"offset\"), ) Generated Parameters:\n{ \"parameters\": [ { \"name\": \"q\", \"in\": \"query\", \"required\": true, \"schema\": {\"type\": \"string\"} }, { \"name\": \"limit\", \"in\": \"query\", \"required\": false, \"schema\": { \"type\": \"string\", \"pattern\": \"^\\\\d+$\" } }, { \"name\": \"offset\", \"in\": \"query\", \"required\": false, \"schema\": {\"type\": \"string\"} } ] } Headers Header parameters are documented in the specification:\nrest.Handle( http.MethodGet, rest.BasePath(\"/data\"), handler, rest.Header(\"X-API-Key\", rest.Required()), rest.Header(\"X-Request-ID\"), ) Generated Parameters:\n{ \"parameters\": [ { \"name\": \"X-API-Key\", \"in\": \"header\", \"required\": true, \"schema\": {\"type\": \"string\"} }, { \"name\": \"X-Request-ID\", \"in\": \"header\", \"required\": false, \"schema\": {\"type\": \"string\"} } ] } Security Schemes Authentication schemes are automatically documented. See Authentication for details.\nJWT Authentication rest.Handle( http.MethodPost, rest.BasePath(\"/orders\"), createOrderHandler, rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ) Generated Security Scheme:\n{ \"components\": { \"securitySchemes\": { \"jwt\": { \"type\": \"http\", \"scheme\": \"bearer\", \"bearerFormat\": \"JWT\" } } }, \"paths\": { \"/orders\": { \"post\": { \"security\": [{\"jwt\": []}] } } } } API Key Authentication rest.Handle( http.MethodGet, rest.BasePath(\"/data\"), handler, rest.Header(\"X-API-Key\", rest.Required(), rest.APIKey(\"api-key\")), ) Generated Security Scheme:\n{ \"components\": { \"securitySchemes\": { \"api-key\": { \"type\": \"apiKey\", \"in\": \"header\", \"name\": \"X-API-Key\" } } }, \"paths\": { \"/data\": { \"get\": { \"security\": [{\"api-key\": []}] } } } } Integration with Tools Swagger UI Serve Swagger UI to visualize your API:\nimport ( \"net/http\" \"github.com/z5labs/humus/rest\" ) func Init(ctx context.Context, cfg rest.Config) (*rest.Api, error) { api := rest.NewApi( \"My API\", \"1.0.0\", // Your operations... ) // Serve Swagger UI (manual setup) http.Handle(\"/docs/\", http.StripPrefix(\"/docs/\", http.FileServer(http.Dir(\"./swagger-ui\")))) return api, nil } Configure Swagger UI to point to /openapi.json:\n\u003c!-- swagger-ui/index.html --\u003e \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003clink rel=\"stylesheet\" href=\"https://unpkg.com/swagger-ui-dist@latest/swagger-ui.css\"\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv id=\"swagger-ui\"\u003e\u003c/div\u003e \u003cscript src=\"https://unpkg.com/swagger-ui-dist@latest/swagger-ui-bundle.js\"\u003e\u003c/script\u003e \u003cscript\u003e SwaggerUIBundle({ url: \"/openapi.json\", dom_id: '#swagger-ui' }); \u003c/script\u003e \u003c/body\u003e \u003c/html\u003e Access at: http://localhost:8080/docs/\nPostman Import the OpenAPI spec into Postman:\nOpen Postman Click Import Select Link and enter http://localhost:8080/openapi.json Postman generates a complete API collection with all endpoints ReDoc Serve ReDoc for clean API documentation:\n\u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eAPI Documentation\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003credoc spec-url=\"/openapi.json\"\u003e\u003c/redoc\u003e \u003cscript src=\"https://cdn.redoc.ly/redoc/latest/bundles/redoc.standalone.js\"\u003e\u003c/script\u003e \u003c/body\u003e \u003c/html\u003e OpenAPI Generator Generate client libraries in any language:\n# Generate TypeScript client openapi-generator-cli generate \\ -i http://localhost:8080/openapi.json \\ -g typescript-fetch \\ -o ./generated/typescript-client # Generate Python client openapi-generator-cli generate \\ -i http://localhost:8080/openapi.json \\ -g python \\ -o ./generated/python-client # Generate Go client openapi-generator-cli generate \\ -i http://localhost:8080/openapi.json \\ -g go \\ -o ./generated/go-client Complete Example package main import ( \"context\" \"net/http\" \"time\" \"github.com/z5labs/humus/rest\" ) // Request/Response Types type CreateBookRequest struct { Title string `json:\"title\"` Author string `json:\"author\"` ISBN string `json:\"isbn,omitempty\"` PublishDate string `json:\"publish_date,omitempty\"` Tags []string `json:\"tags,omitempty\"` } type Book struct { ID string `json:\"id\"` Title string `json:\"title\"` Author string `json:\"author\"` ISBN string `json:\"isbn,omitempty\"` PublishDate string `json:\"publish_date,omitempty\"` Tags []string `json:\"tags,omitempty\"` CreatedAt time.Time `json:\"created_at\"` } type BookList struct { Books []Book `json:\"books\"` TotalCount int `json:\"total_count\"` Page int `json:\"page\"` } // Handlers func createBookHandler(ctx context.Context, req *CreateBookRequest) (*Book, error) { return \u0026Book{ ID: \"book-123\", Title: req.Title, Author: req.Author, ISBN: req.ISBN, PublishDate: req.PublishDate, Tags: req.Tags, CreatedAt: time.Now(), }, nil } func listBooksHandler(ctx context.Context) (*BookList, error) { page := rest.QueryParamValue(ctx, \"page\") // Use page parameter... return \u0026BookList{ Books: []Book{ { ID: \"book-1\", Title: \"Go Programming\", Author: \"John Doe\", CreatedAt: time.Now(), }, }, TotalCount: 1, Page: 1, }, nil } func getBookHandler(ctx context.Context) (*Book, error) { bookID := rest.PathParamValue(ctx, \"id\") // Fetch book by ID... return \u0026Book{ ID: bookID, Title: \"Go Programming\", Author: \"John Doe\", CreatedAt: time.Now(), }, nil } type Config struct { rest.Config `config:\",squash\"` } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi( \"Bookstore API\", \"1.0.0\", // Create a book rest.Handle( http.MethodPost, rest.BasePath(\"/books\"), rest.HandleJson(rest.HandlerFunc[CreateBookRequest, Book](createBookHandler)), ), // List books with pagination rest.Handle( http.MethodGet, rest.BasePath(\"/books\"), rest.ProduceJson(rest.ProducerFunc[BookList](listBooksHandler)), rest.QueryParam(\"page\"), rest.QueryParam(\"limit\"), ), // Get a specific book rest.Handle( http.MethodGet, rest.BasePath(\"/books\").Param(\"id\"), rest.ProduceJson(rest.ProducerFunc[Book](getBookHandler)), ), ) return api, nil } func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } Access the specification:\n# Start the service go run main.go # Get the OpenAPI spec curl http://localhost:8080/openapi.json | jq # The spec includes: # - All three endpoints (/books POST, GET, /books/{id} GET) # - Request schemas (CreateBookRequest) # - Response schemas (Book, BookList) # - Path parameters ({id}) # - Query parameters (page, limit) # - Proper HTTP methods and status codes Best Practices 1. Use Descriptive Type Names Type names appear in the OpenAPI schema:\n// Good - clear and descriptive type CreateUserRequest struct { ... } type UserResponse struct { ... } // Avoid - vague names type Request struct { ... } type Response struct { ... } 2. Add JSON Tags Always use json tags for consistent field naming:\ntype User struct { UserID string `json:\"user_id\"` // snake_case in JSON FirstName string `json:\"first_name\"` // consistent naming LastName string `json:\"last_name\"` } 3. Document with Comments While Humus doesn’t currently extract comments into the OpenAPI spec, they help developers:\n// User represents a registered user in the system type User struct { // Unique identifier for the user ID string `json:\"id\"` // User's email address (must be unique) Email string `json:\"email\"` } 4. Use Separate Request/Response Types Don’t reuse types for both requests and responses:\n// Good - separate types type CreateUserRequest struct { Email string `json:\"email\"` Name string `json:\"name\"` } type UserResponse struct { ID string `json:\"id\"` Email string `json:\"email\"` Name string `json:\"name\"` Created time.Time `json:\"created\"` } // Avoid - single type for both type User struct { ID string `json:\"id,omitempty\"` // Confusing: required in response, not in request Email string `json:\"email\"` Name string `json:\"name\"` Created time.Time `json:\"created,omitempty\"` } 5. Version Your API Include version in the API title or base path:\napi := rest.NewApi( \"Bookstore API\", \"2.0.0\", // Semantic versioning // ... ) // Or use versioned base paths rest.Handle( http.MethodGet, rest.BasePath(\"/api/v2/books\"), handler, ) 6. Validate the Generated Spec Use OpenAPI validators to ensure correctness:\n# Using openapi-generator-cli openapi-generator-cli validate -i http://localhost:8080/openapi.json # Using Spectral (advanced linting) spectral lint http://localhost:8080/openapi.json 7. Cache the Spec for Performance For high-traffic APIs, consider caching the generated spec:\n// Cache the spec at startup var cachedSpec []byte func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"My API\", \"1.0.0\", ...) // Pre-generate and cache the spec resp := httptest.NewRecorder() req := httptest.NewRequest(http.MethodGet, \"/openapi.json\", nil) api.ServeHTTP(resp, req) cachedSpec = resp.Body.Bytes() return api, nil } Limitations No Description Fields Currently, Humus does not extract Go comments into OpenAPI description fields. Type and field descriptions must be added manually if needed.\nLimited Validation Constraints While parameter validators (regex, required) appear in the spec, complex field-level validations (min/max length, numeric ranges) are not automatically reflected. Implement these in your handlers.\nNo Response Status Code Customization Response types currently default to 200 OK. Custom status codes require implementing the TypedResponse interface directly.\nNext Steps Learn about Authentication to add security schemes Explore Error Handling for error response schemas Read Routing for path and parameter configuration See the OpenAPI 3.0 Specification for complete schema reference Check pkg.go.dev for API reference ","categories":"","description":"Working with generated specs","excerpt":"Working with generated specs","ref":"/humus/pr-preview/pr-396/features/rest/openapi/","tags":"","title":"OpenAPI"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/features/grpc/petstore-example/","tags":"","title":"Petstore Example"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/advanced/testing/","tags":"","title":"Testing"},{"body":"Humus provides built-in health check endpoints and flexible health monitoring abstractions to support liveness and readiness probes in container orchestration platforms like Kubernetes.\nOverview Health checks help orchestration platforms determine:\nLiveness - Is the application running? Should it be restarted? Readiness - Is the application ready to serve traffic? Should it receive requests? All Humus REST services automatically include default health endpoints that return 200 OK. You can customize these endpoints to check actual application health, such as database connectivity, dependency availability, or internal state.\nBuilt-in Endpoints Every REST API created with rest.NewApi() automatically provides:\nGET /health/liveness Indicates whether the application is alive and running. If this endpoint fails, the container orchestrator should restart the service.\nDefault Behavior: Returns 200 OK immediately\nUse Cases:\nDetect deadlocks or infinite loops Identify unrecoverable application states Trigger automatic restarts for frozen services GET /health/readiness Indicates whether the application is ready to accept traffic. If this endpoint fails, the orchestrator should stop routing requests to this instance.\nDefault Behavior: Returns 200 OK immediately\nUse Cases:\nDatabase connection checks Cache warmup completion Dependency service availability Configuration loading completion Custom Health Checks Override the default health endpoints by passing custom handlers to rest.NewApi().\nBasic Custom Handler func Init(ctx context.Context, cfg Config) (*rest.Api, error) { readinessHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { // Check your dependencies if !isDatabaseConnected() { w.WriteHeader(http.StatusServiceUnavailable) w.Write([]byte(\"database unavailable\")) return } w.WriteHeader(http.StatusOK) w.Write([]byte(\"ready\")) }) api := rest.NewApi( \"My Service\", \"1.0.0\", rest.Readiness(readinessHandler), // ... your operations ) return api, nil } Using Health Monitors Humus provides the health.Monitor interface for composable health checking:\ntype Monitor interface { Healthy(context.Context) (bool, error) } This abstraction allows you to:\nCompose multiple health checks Share health monitoring logic between liveness and readiness Test health logic independently Health Monitor Implementations Binary Monitor A simple thread-safe monitor with two states: healthy or unhealthy.\nimport \"github.com/z5labs/humus/health\" func Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Create a binary health monitor var appHealth health.Binary // Start as unhealthy (zero value) // Mark healthy after initialization completes defer appHealth.MarkHealthy() // Initialize database db, err := sql.Open(\"postgres\", cfg.DatabaseURL) if err != nil { return nil, err } // Create readiness handler readinessHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { healthy, err := appHealth.Healthy(r.Context()) if err != nil || !healthy { w.WriteHeader(http.StatusServiceUnavailable) w.Write([]byte(\"not ready\")) return } w.WriteHeader(http.StatusOK) w.Write([]byte(\"ready\")) }) api := rest.NewApi( \"My Service\", \"1.0.0\", rest.Readiness(readinessHandler), ) // Mark unhealthy on shutdown lc, _ := lifecycle.FromContext(ctx) lc.OnPreShutdown(lifecycle.HookFunc(func(ctx context.Context) error { appHealth.MarkUnhealthy() return nil })) return api, nil } Thread Safety: Binary monitors use atomic.Bool internally and are safe for concurrent use.\nAndMonitor - Fail Fast Composition Combines multiple monitors with logical AND (\u0026\u0026) semantics. Returns healthy only if all monitors are healthy.\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Create individual monitors var dbHealth health.Binary var cacheHealth health.Binary var queueHealth health.Binary // Combine with AND - all must be healthy readinessMonitor := health.And(\u0026dbHealth, \u0026cacheHealth, \u0026queueHealth) // Initialize components db, err := initDatabase(cfg) if err != nil { return nil, err } dbHealth.MarkHealthy() cache, err := initCache(cfg) if err != nil { return nil, err } cacheHealth.MarkHealthy() queue, err := initQueue(cfg) if err != nil { return nil, err } queueHealth.MarkHealthy() // Create handler using combined monitor readinessHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { healthy, err := readinessMonitor.Healthy(r.Context()) if err != nil { w.WriteHeader(http.StatusServiceUnavailable) w.Write([]byte(fmt.Sprintf(\"health check error: %v\", err))) return } if !healthy { w.WriteHeader(http.StatusServiceUnavailable) w.Write([]byte(\"not ready\")) return } w.WriteHeader(http.StatusOK) w.Write([]byte(\"ready\")) }) api := rest.NewApi( \"My Service\", \"1.0.0\", rest.Readiness(readinessHandler), ) return api, nil } Behavior:\nFail-fast - Stops checking after the first unhealthy monitor Returns immediately on the first error encountered All must pass for the combined check to be healthy OrMonitor - Check All Composition Combines multiple monitors with logical OR (||) semantics. Returns healthy if any monitor is healthy.\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Create monitors for primary and fallback databases var primaryDBHealth health.Binary var fallbackDBHealth health.Binary // Combine with OR - at least one must be healthy dbMonitor := health.Or(\u0026primaryDBHealth, \u0026fallbackDBHealth) // Initialize primary database primaryDB, err := sql.Open(\"postgres\", cfg.PrimaryDatabaseURL) if err == nil { primaryDBHealth.MarkHealthy() } // Initialize fallback database fallbackDB, err := sql.Open(\"postgres\", cfg.FallbackDatabaseURL) if err == nil { fallbackDBHealth.MarkHealthy() } // Create handler - service is ready if either DB is available readinessHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { healthy, err := dbMonitor.Healthy(r.Context()) if err != nil { // OrMonitor collects all errors via errors.Join w.WriteHeader(http.StatusServiceUnavailable) w.Write([]byte(fmt.Sprintf(\"all health checks failed: %v\", err))) return } if !healthy { w.WriteHeader(http.StatusServiceUnavailable) w.Write([]byte(\"no healthy database available\")) return } w.WriteHeader(http.StatusOK) w.Write([]byte(\"ready\")) }) api := rest.NewApi( \"My Service\", \"1.0.0\", rest.Readiness(readinessHandler), ) return api, nil } Behavior:\nChecks all monitors even if one is healthy Collects all errors and returns them joined via errors.Join() At least one must pass for the combined check to be healthy Custom Monitor Implementation Implement the health.Monitor interface for custom health logic:\nimport ( \"context\" \"database/sql\" \"github.com/z5labs/humus/health\" ) // DatabaseMonitor checks database connectivity type DatabaseMonitor struct { db *sql.DB } func NewDatabaseMonitor(db *sql.DB) *DatabaseMonitor { return \u0026DatabaseMonitor{db: db} } func (m *DatabaseMonitor) Healthy(ctx context.Context) (bool, error) { // Ping with timeout from context err := m.db.PingContext(ctx) if err != nil { return false, err } return true, nil } // CacheMonitor checks cache connectivity type CacheMonitor struct { client *redis.Client } func NewCacheMonitor(client *redis.Client) *CacheMonitor { return \u0026CacheMonitor{client: client} } func (m *CacheMonitor) Healthy(ctx context.Context) (bool, error) { err := m.client.Ping(ctx).Err() if err != nil { return false, err } return true, nil } // Usage func Init(ctx context.Context, cfg Config) (*rest.Api, error) { db, err := sql.Open(\"postgres\", cfg.DatabaseURL) if err != nil { return nil, err } cache, err := initRedis(cfg) if err != nil { return nil, err } // Combine custom monitors dbMonitor := NewDatabaseMonitor(db) cacheMonitor := NewCacheMonitor(cache) readinessMonitor := health.And(dbMonitor, cacheMonitor) readinessHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { healthy, err := readinessMonitor.Healthy(r.Context()) if err != nil { w.WriteHeader(http.StatusServiceUnavailable) json.NewEncoder(w).Encode(map[string]string{ \"status\": \"unhealthy\", \"error\": err.Error(), }) return } if !healthy { w.WriteHeader(http.StatusServiceUnavailable) json.NewEncoder(w).Encode(map[string]string{ \"status\": \"unhealthy\", }) return } w.WriteHeader(http.StatusOK) json.NewEncoder(w).Encode(map[string]string{ \"status\": \"healthy\", }) }) api := rest.NewApi( \"My Service\", \"1.0.0\", rest.Readiness(readinessHandler), ) return api, nil } Kubernetes Integration Configure liveness and readiness probes in your Kubernetes deployment:\napiVersion: apps/v1 kind: Deployment metadata: name: my-service spec: template: spec: containers: - name: my-service image: my-service:latest ports: - containerPort: 8080 # Liveness probe - restart if unhealthy livenessProbe: httpGet: path: /health/liveness port: 8080 initialDelaySeconds: 10 periodSeconds: 30 timeoutSeconds: 5 failureThreshold: 3 # Readiness probe - remove from load balancer if unhealthy readinessProbe: httpGet: path: /health/readiness port: 8080 initialDelaySeconds: 5 periodSeconds: 10 timeoutSeconds: 3 failureThreshold: 2 successThreshold: 1 Best Practices 1. Separate Liveness from Readiness Liveness should detect unrecoverable states:\nApplication deadlocks Out of memory conditions Corrupted internal state Readiness should detect recoverable dependencies:\nDatabase connectivity External API availability Cache connectivity // Liveness - simple alive check livenessHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { // Just return OK - server is running if it responds w.WriteHeader(http.StatusOK) }) // Readiness - dependency checks readinessHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { // Check database, cache, etc. if !checkDependencies(r.Context()) { w.WriteHeader(http.StatusServiceUnavailable) return } w.WriteHeader(http.StatusOK) }) api := rest.NewApi( \"My Service\", \"1.0.0\", rest.Liveness(livenessHandler), rest.Readiness(readinessHandler), ) 2. Use Context Timeouts Health checks should respect context deadlines:\nfunc (m *DatabaseMonitor) Healthy(ctx context.Context) (bool, error) { // Add timeout if context doesn't have one ctx, cancel := context.WithTimeout(ctx, 2*time.Second) defer cancel() err := m.db.PingContext(ctx) return err == nil, err } 3. Mark Unhealthy During Graceful Shutdown Prevent new traffic during shutdown:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { var appHealth health.Binary appHealth.MarkHealthy() // ... initialization ... lc, _ := lifecycle.FromContext(ctx) lc.OnPreShutdown(lifecycle.HookFunc(func(ctx context.Context) error { // Stop accepting new requests appHealth.MarkUnhealthy() // Give load balancer time to detect time.Sleep(5 * time.Second) return nil })) return api, nil } 4. Return Descriptive Error Messages Help operators diagnose issues:\nreadinessHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { type HealthStatus struct { Status string `json:\"status\"` Components map[string]string `json:\"components,omitempty\"` } status := HealthStatus{ Status: \"healthy\", Components: make(map[string]string), } // Check database if err := db.PingContext(r.Context()); err != nil { status.Status = \"unhealthy\" status.Components[\"database\"] = err.Error() } else { status.Components[\"database\"] = \"healthy\" } // Check cache if err := cache.Ping(r.Context()).Err(); err != nil { status.Status = \"unhealthy\" status.Components[\"cache\"] = err.Error() } else { status.Components[\"cache\"] = \"healthy\" } if status.Status == \"unhealthy\" { w.WriteHeader(http.StatusServiceUnavailable) } else { w.WriteHeader(http.StatusOK) } json.NewEncoder(w).Encode(status) }) 5. Avoid Heavy Operations Health checks run frequently - keep them lightweight:\n// Good - quick ping func (m *DatabaseMonitor) Healthy(ctx context.Context) (bool, error) { return m.db.PingContext(ctx) == nil, nil } // Bad - expensive query func (m *DatabaseMonitor) Healthy(ctx context.Context) (bool, error) { var count int err := m.db.QueryRowContext(ctx, \"SELECT COUNT(*) FROM large_table\").Scan(\u0026count) return err == nil, err } 6. Test Health Checks Write tests for your health monitoring logic:\nfunc TestDatabaseMonitor(t *testing.T) { // Setup test database db, mock, err := sqlmock.New() require.NoError(t, err) defer db.Close() monitor := NewDatabaseMonitor(db) t.Run(\"returns healthy when database is connected\", func(t *testing.T) { mock.ExpectPing() healthy, err := monitor.Healthy(context.Background()) require.NoError(t, err) assert.True(t, healthy) }) t.Run(\"returns unhealthy when database is disconnected\", func(t *testing.T) { mock.ExpectPing().WillReturnError(sql.ErrConnDone) healthy, err := monitor.Healthy(context.Background()) require.Error(t, err) assert.False(t, healthy) }) } Complete Example package main import ( \"context\" \"database/sql\" \"encoding/json\" \"net/http\" \"time\" \"github.com/z5labs/bedrock/lifecycle\" \"github.com/z5labs/humus/health\" \"github.com/z5labs/humus/rest\" ) type DatabaseMonitor struct { db *sql.DB } func (m *DatabaseMonitor) Healthy(ctx context.Context) (bool, error) { ctx, cancel := context.WithTimeout(ctx, 2*time.Second) defer cancel() err := m.db.PingContext(ctx) return err == nil, err } type Config struct { rest.Config `config:\",squash\"` DatabaseURL string `config:\"database_url\"` } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Initialize database db, err := sql.Open(\"postgres\", cfg.DatabaseURL) if err != nil { return nil, err } // Create health monitors var appHealth health.Binary dbMonitor := \u0026DatabaseMonitor{db: db} // Combine for readiness readinessMonitor := health.And(\u0026appHealth, dbMonitor) // Create health handlers livenessHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) w.Write([]byte(\"alive\")) }) readinessHandler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { healthy, err := readinessMonitor.Healthy(r.Context()) status := map[string]interface{}{ \"status\": \"healthy\", } if err != nil || !healthy { status[\"status\"] = \"unhealthy\" if err != nil { status[\"error\"] = err.Error() } w.WriteHeader(http.StatusServiceUnavailable) } else { w.WriteHeader(http.StatusOK) } json.NewEncoder(w).Encode(status) }) // Create API api := rest.NewApi( \"My Service\", \"1.0.0\", rest.Liveness(livenessHandler), rest.Readiness(readinessHandler), // ... your operations ) // Mark healthy after initialization appHealth.MarkHealthy() // Mark unhealthy during shutdown lc, _ := lifecycle.FromContext(ctx) lc.OnPreShutdown(lifecycle.HookFunc(func(ctx context.Context) error { appHealth.MarkUnhealthy() time.Sleep(5 * time.Second) // Grace period return nil })) lc.OnPostRun(lifecycle.HookFunc(func(ctx context.Context) error { return db.Close() })) return api, nil } func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } Next Steps Learn about OpenAPI generation and documentation Explore Error Handling for custom error responses Read Authentication for securing health endpoints (if needed) See the health package documentation for API reference ","categories":"","description":"Monitoring service health","excerpt":"Monitoring service health","ref":"/humus/pr-preview/pr-396/features/rest/health-checks/","tags":"","title":"Health Checks"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/advanced/multi-source-config/","tags":"","title":"Multi Source Config"},{"body":"This guide covers best practices for deploying and configuring Kafka processors in production environments.\nComplete Configuration Example A production-ready configuration:\nkafka: brokers: - \"{{env \"KAFKA_BROKER_1\" | default \"kafka-1:9092\"}}\" - \"{{env \"KAFKA_BROKER_2\" | default \"kafka-2:9092\"}}\" - \"{{env \"KAFKA_BROKER_3\" | default \"kafka-3:9092\"}}\" group_id: \"{{env \"KAFKA_GROUP_ID\" | default \"order-processors\"}}\" topics: - name: \"orders\" semantic: \"at-least-once\" - name: \"payments\" semantic: \"at-least-once\" session_timeout: \"{{env \"KAFKA_SESSION_TIMEOUT\" | default \"30s\"}}\" rebalance_timeout: \"{{env \"KAFKA_REBALANCE_TIMEOUT\" | default \"45s\"}}\" fetch_max_bytes: {{env \"KAFKA_FETCH_MAX_BYTES\" | default \"52428800\"}} max_concurrent_fetches: {{env \"KAFKA_MAX_CONCURRENT_FETCHES\" | default \"10\"}} otel: service: name: \"{{env \"SERVICE_NAME\" | default \"order-processor\"}}\" version: \"{{env \"SERVICE_VERSION\" | default \"1.0.0\"}}\" environment: \"{{env \"ENVIRONMENT\" | default \"production\"}}\" sdk: disabled: false exporter: otlp: endpoint: \"{{env \"OTEL_ENDPOINT\" | default \"otel-collector:4317\"}}\" protocol: grpc traces: sampler: type: \"{{env \"OTEL_TRACE_SAMPLER\" | default \"traceidratio\"}}\" arg: {{env \"OTEL_TRACE_SAMPLE_RATE\" | default \"0.1\"}} metrics: interval: 60s database: host: \"{{env \"DB_HOST\" | default \"postgres\"}}\" port: {{env \"DB_PORT\" | default \"5432\"}} name: \"{{env \"DB_NAME\" | default \"orders\"}}\" user: \"{{env \"DB_USER\" | default \"orders_user\"}}\" password: \"{{env \"DB_PASSWORD\"}}\" max_connections: {{env \"DB_MAX_CONNECTIONS\" | default \"25\"}} Kafka Configuration Broker Configuration Multiple Brokers:\nkafka: brokers: - \"kafka-1.prod:9092\" - \"kafka-2.prod:9092\" - \"kafka-3.prod:9092\" Best practices:\nUse at least 3 brokers for redundancy Use DNS names, not IP addresses Configure all brokers, not just one Consumer Group ID Unique per deployment:\nkafka: group_id: \"order-processors-prod\" # Different from staging Naming conventions:\n{service}-{environment} Examples: - order-processors-prod - payment-processors-staging - analytics-processors-dev Topic Configuration Production topics:\nkafka: topics: - name: \"orders\" semantic: \"at-least-once\" - name: \"payments\" semantic: \"at-least-once\" - name: \"analytics\" semantic: \"at-most-once\" Topic naming:\n{domain}.{entity}.{event} Examples: - ecommerce.orders.created - ecommerce.payments.completed - analytics.events.tracked Timeouts Production timeouts:\nkafka: session_timeout: \"30s\" # Balance between failure detection and GC tolerance rebalance_timeout: \"45s\" # Must be \u003e session_timeout Guidelines:\nSession timeout: 20-45s (default: 30s) Rebalance timeout: 45-90s (default: 60s) Increase if frequent rebalances occur Decrease for faster failure detection Fetch Settings Production fetch settings:\nkafka: fetch_max_bytes: 52428800 # 50 MB max_concurrent_fetches: 10 # Limit concurrent requests Tuning guidelines:\nSmall messages: 10-25 MB fetch size Large messages: 100+ MB fetch size High throughput: Increase concurrent fetches Memory constrained: Decrease fetch size Application Configuration Idempotency Database-backed idempotency:\ntype Config struct { queue.Config `config:\",squash\"` Kafka struct { // ... kafka config } `config:\"kafka\"` Database struct { Host string `config:\"host\"` Port int `config:\"port\"` Name string `config:\"name\"` User string `config:\"user\"` Password string `config:\"password\"` MaxConnections int `config:\"max_connections\"` } `config:\"database\"` IdempotencyWindow time.Duration `config:\"idempotency_window\"` } func Init(ctx context.Context, cfg Config) (*queue.App, error) { // Setup database connection pool db, err := sql.Open(\"postgres\", fmt.Sprintf( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=require\", cfg.Database.Host, cfg.Database.Port, cfg.Database.User, cfg.Database.Password, cfg.Database.Name, )) if err != nil { return nil, err } db.SetMaxOpenConns(cfg.Database.MaxConnections) db.SetMaxIdleConns(cfg.Database.MaxConnections / 2) db.SetConnMaxLifetime(time.Hour) processor := \u0026OrderProcessor{ db: db, idempotencyWindow: cfg.IdempotencyWindow, } runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), kafka.SessionTimeout(cfg.Kafka.SessionTimeout), kafka.RebalanceTimeout(cfg.Kafka.RebalanceTimeout), ) return queue.NewApp(runtime), nil } Resource Limits Database connection pooling:\ndb.SetMaxOpenConns(25) // Limit total connections db.SetMaxIdleConns(10) // Idle connections db.SetConnMaxLifetime(1 * time.Hour) // Connection lifetime Calculation:\nMax connections = Partitions × Concurrent operations per partition Example: 12 partitions × 2 ops = 24 connections (use 25) Deployment Container Configuration Dockerfile:\nFROM golang:1.21-alpine AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 GOOS=linux go build -o processor ./cmd/processor FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=builder /app/processor . COPY config.yaml . CMD [\"./processor\"] Docker Compose:\nversion: '3.8' services: order-processor: image: order-processor:latest environment: - KAFKA_BROKER_1=kafka-1:9092 - KAFKA_BROKER_2=kafka-2:9092 - KAFKA_GROUP_ID=order-processors-prod - KAFKA_SESSION_TIMEOUT=30s - DB_HOST=postgres - DB_PASSWORD=${DB_PASSWORD} - OTEL_ENDPOINT=otel-collector:4317 depends_on: - kafka - postgres restart: unless-stopped deploy: replicas: 3 # Scale horizontally resources: limits: cpus: '1' memory: 512M reservations: cpus: '0.5' memory: 256M Kubernetes Deployment Deployment manifest:\napiVersion: apps/v1 kind: Deployment metadata: name: order-processor namespace: production spec: replicas: 3 selector: matchLabels: app: order-processor template: metadata: labels: app: order-processor spec: containers: - name: processor image: order-processor:1.0.0 env: - name: KAFKA_BROKER_1 value: \"kafka-1.kafka.svc:9092\" - name: KAFKA_BROKER_2 value: \"kafka-2.kafka.svc:9092\" - name: KAFKA_GROUP_ID value: \"order-processors-prod\" - name: DB_PASSWORD valueFrom: secretKeyRef: name: db-credentials key: password - name: OTEL_ENDPOINT value: \"otel-collector.monitoring.svc:4317\" resources: requests: memory: \"256Mi\" cpu: \"500m\" limits: memory: \"512Mi\" cpu: \"1000m\" livenessProbe: httpGet: path: /health/liveness port: 8080 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /health/readiness port: 8080 initialDelaySeconds: 5 periodSeconds: 5 Scaling Horizontal scaling:\n# Scale to 6 replicas kubectl scale deployment order-processor --replicas=6 Scaling guidelines:\nMax replicas ≤ number of partitions Start with 2-3 replicas for HA Scale up if consumer lag increases Monitor CPU/memory usage Example:\n12 partitions: - 3 replicas: 4 partitions each - 6 replicas: 2 partitions each - 12 replicas: 1 partition each (max) Monitoring Metrics to Monitor Consumer lag:\nkafka_consumer_lag{group=\"order-processors\",topic=\"orders\"} Alert when lag \u003e 1000:\n- alert: HighConsumerLag expr: kafka_consumer_lag \u003e 1000 for: 5m annotations: summary: \"Consumer group {{ $labels.group }} lagging\" Processing rate:\nrate(kafka_messages_processed_total[1m]) Error rate:\nrate(kafka_processing_errors_total[1m]) Health Checks Kubernetes probes:\nlivenessProbe: httpGet: path: /health/liveness port: 8080 initialDelaySeconds: 30 periodSeconds: 10 failureThreshold: 3 readinessProbe: httpGet: path: /health/readiness port: 8080 initialDelaySeconds: 5 periodSeconds: 5 failureThreshold: 3 Custom health check:\ntype HealthCheck struct { processor *OrderProcessor } func (h *HealthCheck) Healthy(ctx context.Context) (bool, error) { // Check database connection if err := h.processor.db.PingContext(ctx); err != nil { return false, err } // Check consumer lag (if available) // ... return true, nil } Security TLS Configuration Kafka TLS:\nimport \"github.com/twmb/franz-go/pkg/kgo\" tlsConfig := \u0026tls.Config{ MinVersion: tls.VersionTLS12, } runtime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.WithKafkaOptions( kgo.DialTLSConfig(tlsConfig), ), ) SASL Authentication SASL/SCRAM:\nimport ( \"github.com/twmb/franz-go/pkg/kgo\" \"github.com/twmb/franz-go/pkg/sasl/scram\" ) scramAuth := scram.Auth{ User: cfg.Kafka.User, Pass: cfg.Kafka.Password, } runtime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.WithKafkaOptions( kgo.SASL(scramAuth.AsSha256Mechanism()), ), ) Secrets Management Kubernetes secrets:\napiVersion: v1 kind: Secret metadata: name: kafka-credentials type: Opaque stringData: username: \"order-processor-user\" password: \"secure-password-here\" Reference in deployment:\nenv: - name: KAFKA_USER valueFrom: secretKeyRef: name: kafka-credentials key: username - name: KAFKA_PASSWORD valueFrom: secretKeyRef: name: kafka-credentials key: password Performance Tuning Partition Count Optimal partition count:\nPartitions = Target throughput / Partition throughput Example: Target: 100k msgs/sec Per partition: 10k msgs/sec Partitions: 100k / 10k = 10 Recommendations:\nStart with 3× expected consumer count Monitor lag and throughput Increase if lag grows under load Cannot decrease (Kafka limitation) Batch Processing Process messages in batches:\ntype BatchProcessor struct { db *sql.DB batchSize int } func (p *BatchProcessor) ProcessBatch(ctx context.Context, messages []kafka.Message) error { tx, err := p.db.BeginTx(ctx, nil) if err != nil { return err } defer tx.Rollback() for _, msg := range messages { var order Order json.Unmarshal(msg.Value, \u0026order) _, err := tx.ExecContext(ctx, \"INSERT INTO orders (order_id, total) VALUES ($1, $2)\", order.OrderID, order.Total, ) if err != nil { return err } } return tx.Commit() } Connection Pooling Optimize database connections:\n// Formula: Max connections = Partitions × 2 db.SetMaxOpenConns(cfg.Partitions * 2) db.SetMaxIdleConns(cfg.Partitions) db.SetConnMaxLifetime(1 * time.Hour) db.SetConnMaxIdleTime(10 * time.Minute) Disaster Recovery Consumer Group Reset Reset to earliest:\nkafka-consumer-groups.sh \\ --bootstrap-server kafka:9092 \\ --group order-processors \\ --reset-offsets \\ --to-earliest \\ --all-topics \\ --execute Reset to specific offset:\nkafka-consumer-groups.sh \\ --bootstrap-server kafka:9092 \\ --group order-processors \\ --reset-offsets \\ --topic orders:0 \\ --to-offset 12345 \\ --execute Backup and Restore Export offsets:\nkafka-consumer-groups.sh \\ --bootstrap-server kafka:9092 \\ --group order-processors \\ --describe \u003e offsets-backup.txt Dead Letter Queue:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { if err := p.processMessage(ctx, msg); err != nil { // Send to DLQ on failure return p.sendToDLQ(ctx, msg, err) } return nil } func (p *Processor) sendToDLQ(ctx context.Context, msg kafka.Message, err error) error { dlqRecord := \u0026kgo.Record{ Topic: msg.Topic + \".dlq\", Key: msg.Key, Value: msg.Value, Headers: append(msg.Headers, kafka.Header{ Key: \"error\", Value: []byte(err.Error()), }), } p.producer.Produce(ctx, dlqRecord, nil) return nil // Don't return error, message handled } Troubleshooting High Consumer Lag Causes:\nSlow processing Insufficient consumers Hot partitions Solutions:\n# Scale up consumers kubectl scale deployment order-processor --replicas=6 # Check for hot partitions kafka-consumer-groups.sh --describe --group order-processors # Optimize processing code # Add database indexes # Batch operations Frequent Rebalances Causes:\nShort session timeout Long processing time Network issues Solutions:\nkafka: session_timeout: \"45s\" # Increase rebalance_timeout: \"90s\" # Increase Memory Issues Causes:\nLarge fetch buffers Too many partitions Memory leaks Solutions:\nkafka: fetch_max_bytes: 25000000 # Reduce to 25 MB max_concurrent_fetches: 5 # Limit concurrent fetches Next Steps Review Quick Start for basic setup Learn Idempotency patterns Explore Observability for monitoring ","categories":"","description":"Deploying Kafka processors in production","excerpt":"Deploying Kafka processors in production","ref":"/humus/pr-preview/pr-396/features/queue/kafka/configuration/","tags":"","title":"Production Configuration"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-396/advanced/concurrent-utilities/","tags":"","title":"Concurrent Utilities"},{"body":"","categories":"","description":"","excerpt":"","ref":"/humus/pr-preview/pr-396/categories/","tags":"","title":"Categories"},{"body":"A modular Go framework for building production-ready services with standardized observability, health checks, and graceful shutdown.\nHumus is built on top of Bedrock and provides opinionated patterns for three types of applications:\nService Types REST/HTTP Services Build OpenAPI-compliant web applications with automatic schema generation, built-in health endpoints, and flexible request/response handling.\nGet Started with REST →\ngRPC Services Create gRPC microservices with automatic OpenTelemetry instrumentation, health service registration, and seamless integration with the gRPC ecosystem.\nGet Started with gRPC →\nJob Services Build one-off job executors for batch processing, migrations, or scheduled tasks with the same observability and lifecycle management as long-running services.\nGet Started with Jobs →\nKey Features Batteries Included Observability\nAutomatic OpenTelemetry SDK initialization for traces, metrics, and logs Integrated structured logging with slog Zero-configuration instrumentation for HTTP and gRPC Production Ready\nGraceful shutdown with OS signal handling Standardized health check patterns Panic recovery middleware YAML-based configuration with templating support Developer Friendly\nMinimal boilerplate with Builder + Runner pattern Automatic OpenAPI schema generation from Go types Type-safe request/response handling Comprehensive examples and documentation Quick Example package main import ( \"context\" \"net/http\" \"github.com/z5labs/humus/rest\" ) type Config struct { rest.Config `config:\",squash\"` } func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { handler := rest.ProducerFunc[string](func(ctx context.Context) (*string, error) { msg := \"Hello, World!\" return \u0026msg, nil }) api := rest.NewApi( \"My Service\", \"v1.0.0\", rest.Handle( http.MethodGet, rest.BasePath(\"/hello\"), rest.ProduceJson(handler), ), ) return api, nil } Next Steps Installation and Getting Started Core Concepts API Reference Resources GitHub Repository GitHub Discussions Go Package Documentation ","categories":"","description":"","excerpt":"A modular Go framework for building production-ready services with …","ref":"/humus/pr-preview/pr-396/","tags":"","title":"Humus"},{"body":"","categories":"","description":"","excerpt":"","ref":"/humus/pr-preview/pr-396/tags/","tags":"","title":"Tags"}]