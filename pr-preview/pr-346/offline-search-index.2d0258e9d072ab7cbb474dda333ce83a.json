[{"body":"This guide will help you get started with Humus, from installation to building your first service.\nPrerequisites Before you begin, ensure you have:\nGo 1.21 or later installed on your system Basic familiarity with Go programming Understanding of REST APIs, gRPC, or batch processing (depending on your use case) What You’ll Learn This section covers:\nInstallation - Installing Humus and setting up your environment Your First Service - Building a Hello World REST service Configuration - Understanding the YAML configuration system Project Structure - Recommended project layout patterns Quick Start If you want to jump right in:\n# Install Humus go get github.com/z5labs/humus # Create a new project mkdir my-service \u0026\u0026 cd my-service go mod init my-service Then follow the First Service guide to build your first application.\nNext Steps Once you’re comfortable with the basics, explore:\nCore Concepts - Understand Humus architecture REST Services - Build HTTP APIs gRPC Services - Build gRPC microservices Job Services - Build batch processors ","categories":"","description":"Get up and running with Humus","excerpt":"Get up and running with Humus","ref":"/humus/pr-preview/pr-346/getting-started/","tags":"","title":"Getting Started"},{"body":"This section covers the fundamental concepts and patterns that power Humus applications.\nWhat You’ll Learn Understanding these core concepts will help you build better applications with Humus:\nConfiguration System - Deep dive into YAML config composition and templating Observability - OpenTelemetry integration for traces, metrics, and logs Lifecycle Management - Graceful shutdown and signal handling Architecture Overview Humus is built on Bedrock, a foundational framework for application lifecycle management. Humus extends Bedrock with:\nService-Specific Builders - Pre-configured builders for REST, gRPC, and Job services Automatic Instrumentation - Built-in OpenTelemetry integration Standardized Patterns - Consistent interfaces across service types Key Principles Convention Over Configuration Humus provides sensible defaults so you can start quickly. Override only what you need:\n// Minimal configuration needed type Config struct { rest.Config `config:\",squash\"` } Composition Over Inheritance Build complex applications by composing simple pieces:\n// Compose multiple configuration sources source := config.MultiSource( config.FromYaml(\"defaults.yaml\"), config.FromYaml(\"overrides.yaml\"), ) Separation of Concerns Humus separates:\nConfiguration - What to run Initialization - How to build it Execution - When to run it func main() { // Configuration source source := rest.YamlSource(\"config.yaml\") // Initialization function init := app.Init // Execution rest.Run(source, init) } Common Patterns The Init Function Every Humus service has an Init function that receives configuration and returns the service:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Build and return your service } This function is called after configuration is loaded but before the service starts.\nConfiguration Embedding Embed framework configs to inherit standard fields:\ntype Config struct { rest.Config `config:\",squash\"` // Inherits HTTP server config Database DatabaseConfig `config:\"database\"` // Your custom config } Error Handling Use errors to fail fast during initialization:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { if cfg.Database.Host == \"\" { return nil, fmt.Errorf(\"database host required\") } // ... } Next Steps Dive deeper into specific concepts:\nConfiguration System - Master config composition Observability - Learn about traces, metrics, and logs Lifecycle Management - Understand graceful shutdown Or explore service-specific features:\nREST Services gRPC Services Job Services ","categories":"","description":"Understanding Humus architecture and patterns","excerpt":"Understanding Humus architecture and patterns","ref":"/humus/pr-preview/pr-346/concepts/","tags":"","title":"Core Concepts"},{"body":"Humus provides four distinct service types, each optimized for different use cases.\nService Types REST/HTTP Services Build OpenAPI-compliant HTTP APIs with automatic schema generation and type-safe handlers.\nBest For:\nWeb APIs and microservices Public-facing services Services requiring OpenAPI documentation Key Features:\nAutomatic OpenAPI 3.0 generation Built-in health endpoints Type-safe request/response handling Path parameter and query validation Get Started with REST →\ngRPC Services Create high-performance gRPC microservices with automatic instrumentation and health checks.\nBest For:\nInternal microservices High-performance service-to-service communication Services with strongly-typed contracts Key Features:\nAutomatic gRPC health service Built-in interceptors for OTel Service registration Protocol Buffers support Get Started with gRPC →\nJob Services Build one-off job executors for batch processing, migrations, and scheduled tasks.\nBest For:\nDatabase migrations Batch processing Scheduled tasks (with external scheduler) One-time operations Key Features:\nSimple Handler interface Full observability support Same lifecycle management as long-running services Context-aware execution Get Started with Jobs →\nQueue Services Process messages from message queues with flexible delivery semantics and automatic concurrency management.\nBest For:\nEvent-driven architectures Message queue processing (Kafka, RabbitMQ, etc.) Asynchronous workloads Stream processing Key Features:\nAt-most-once and at-least-once delivery semantics Pluggable queue runtimes Automatic concurrency management (Kafka: goroutine-per-partition) Built-in OpenTelemetry instrumentation Get Started with Queues →\nCommon Features All service types include:\nOpenTelemetry Integration - Automatic traces, metrics, and logs Graceful Shutdown - Clean shutdown on SIGTERM/SIGINT Configuration Management - YAML with template support Panic Recovery - Automatic panic recovery in handlers Lifecycle Management - Managed by Bedrock framework ","categories":"","description":"Service types and capabilities","excerpt":"Service types and capabilities","ref":"/humus/pr-preview/pr-346/features/","tags":"","title":"Features"},{"body":"This section covers advanced patterns, customizations, and deep dives into Humus internals.\nTopics Covered Builder + Runner Pattern - Deep dive into the core architecture Custom Health Monitors - Building custom health checks OTel Integration - Advanced OpenTelemetry configuration Middleware - Creating custom middleware and interceptors Testing - Testing patterns for Humus applications Multi-Source Config - Advanced configuration composition Concurrent Utilities - Thread-safe utilities Prerequisites Before diving into advanced topics, ensure you’re familiar with:\nGetting Started Core Concepts At least one service type (REST, gRPC, or Job) When to Use Advanced Topics These topics are useful when:\nYou need to customize framework behavior You’re building reusable components You want to understand how Humus works internally You need advanced configuration strategies You’re implementing custom patterns Next Steps Choose a topic based on your needs, or start with Builder + Runner Pattern to understand the core architecture.\n","categories":"","description":"Deep dives into Humus internals and patterns","excerpt":"Deep dives into Humus internals and patterns","ref":"/humus/pr-preview/pr-346/advanced/","tags":"","title":"Advanced Topics"},{"body":"Quick reference guides for Humus packages. For complete API documentation, see pkg.go.dev/github.com/z5labs/humus.\nPackage References REST Package - HTTP services and routing gRPC Package - gRPC services Job Package - Job services Health Package - Health monitoring Config Package - Configuration types External Documentation Go Package Documentation - Complete API reference Bedrock Documentation - Foundation framework OpenTelemetry Go - Observability SDK chi Router - HTTP router (used by REST) gRPC-Go - gRPC framework Quick Links REST Services rest.NewApi rest.Handle rpc.NewOperation rpc.Handle gRPC Services grpc.NewApi grpc.Run Job Services job.Run job.Handler Health Monitoring health.Monitor health.Binary health.AndMonitor health.OrMonitor ","categories":"","description":"Quick reference for Humus packages","excerpt":"Quick reference for Humus packages","ref":"/humus/pr-preview/pr-346/reference/","tags":"","title":"API Reference"},{"body":"Guides for migrating existing applications to Humus and integrating with other systems.\nMigration Guides From Vanilla Go HTTP - Migrate from net/http From chi Router - Migrate from chi-based applications From gRPC-Go - Migrate from vanilla gRPC applications Bedrock Integration - Deep dive into Bedrock framework Why Migrate to Humus? Consistency Standardized patterns across REST, gRPC, and Job services Common configuration format Unified observability approach Built-in Observability Automatic OpenTelemetry integration No manual instrumentation needed Consistent logging with trace correlation Production Ready Graceful shutdown out of the box Health check endpoints Panic recovery Signal handling Developer Experience Type-safe handlers (REST/RPC) Automatic OpenAPI generation Minimal boilerplate Clear separation of concerns Migration Strategy Assess Your Application - Identify service type and dependencies Start Small - Migrate one endpoint or service at a time Test Thoroughly - Ensure behavior matches original Deploy Incrementally - Use feature flags or canary deployments Compatibility Humus is compatible with:\nExisting HTTP Middleware - chi middleware works with REST services gRPC Interceptors - Standard interceptors work alongside Humus interceptors OpenTelemetry Collectors - Any OTLP-compatible backend Configuration Sources - YAML files, environment variables, or custom sources Next Steps Choose a migration guide based on your current stack, or explore Bedrock Integration to understand the foundation.\n","categories":"","description":"Migrating to Humus and integrating with other systems","excerpt":"Migrating to Humus and integrating with other systems","ref":"/humus/pr-preview/pr-346/integration/","tags":"","title":"Migration \u0026 Integration"},{"body":"Common questions, issues, and solutions for Humus applications.\nSections Frequently Asked Questions - Common questions about Humus Troubleshooting - Common issues and solutions Best Practices - Recommended patterns Quick Answers General Q: What’s the difference between Humus and Bedrock?\nA: Bedrock is the foundation framework providing application lifecycle management. Humus builds on Bedrock to provide service-specific patterns for REST, gRPC, and Job services.\nQ: Do I need to know Bedrock to use Humus?\nA: No. Humus abstracts Bedrock’s complexity. You only need to know Humus APIs for most use cases.\nQ: Can I use Humus with existing Go code?\nA: Yes. Humus is compatible with standard Go HTTP middleware, gRPC interceptors, and other Go libraries.\nREST Services Q: Can I use Humus with my existing chi router?\nA: Humus uses chi internally, so chi middleware is compatible. However, you should use Humus’s routing APIs for full OpenAPI support.\nQ: How do I add custom middleware?\nA: See Advanced Topics - Middleware.\nQ: Can I disable OpenAPI generation?\nA: Currently, OpenAPI generation is always enabled. It adds minimal overhead.\ngRPC Services Q: Can I use Protocol Buffers v2?\nA: Humus works with both proto2 and proto3. We recommend proto3 for new projects.\nQ: How do I add custom interceptors?\nA: See Advanced Topics - Middleware.\nConfiguration Q: Can I use environment variables directly without YAML?\nA: Currently, you need a YAML file, but it can reference environment variables using templates. You can also implement a custom config source.\nQ: How do I handle secrets?\nA: Use environment variables in your YAML templates: {{env \"SECRET_KEY\"}}. Never commit secrets to YAML files.\nObservability Q: Can I use Humus without OpenTelemetry?\nA: Yes. Set otel.sdk.disabled: true in your configuration.\nQ: How do I send telemetry to multiple backends?\nA: Use an OpenTelemetry Collector to fan out to multiple backends.\nQ: Can I use Prometheus metrics?\nA: Yes. Use the OpenTelemetry Prometheus exporter or collector.\nStill Have Questions? Check the Documentation Visit GitHub Discussions Review Troubleshooting Guide See Best Practices ","categories":"","description":"Common questions and solutions","excerpt":"Common questions and solutions","ref":"/humus/pr-preview/pr-346/faq/","tags":"","title":"FAQ \u0026 Troubleshooting"},{"body":"Thank you for your interest in contributing to Humus! This guide will help you get started.\nWays to Contribute Report Bugs - File issues on GitHub Suggest Features - Start a discussion Improve Documentation - Fix typos, add examples, clarify explanations Write Code - Fix bugs or implement features Share Examples - Contribute example applications Getting Started Fork the Repository - https://github.com/z5labs/humus Clone Your Fork git clone https://github.com/YOUR-USERNAME/humus.git cd humus Set Up Development Environment - See Development Setup Create a Branch git checkout -b feature/my-feature Development Workflow Running Tests # Run all tests with race detection and coverage go test -race -cover ./... # Run tests for a specific package go test -race -cover ./rest/rpc # Run a specific test go test -race -run TestName ./path/to/package Linting # Run golangci-lint golangci-lint run # Auto-fix issues where possible golangci-lint run --fix Building # Build all packages go build ./... # Verify no build errors go vet ./... Code Guidelines Style Follow standard Go conventions Use gofmt for formatting Run golangci-lint before committing Write clear, descriptive commit messages Testing Write tests for new features Maintain or improve code coverage Use table-driven tests where appropriate Mock external dependencies Documentation Document exported types and functions Include examples in godoc Update relevant documentation pages Add package-level documentation Pull Request Process Ensure Tests Pass\ngo test -race -cover ./... Ensure Linting Passes\ngolangci-lint run Update Documentation - If you’re adding features or changing behavior\nWrite a Clear PR Description\nWhat does this PR do? Why is this change needed? How was it tested? Link Related Issues - Use “Fixes #123” or “Relates to #456”\nBe Responsive - Address review comments promptly\nCode of Conduct Be respectful and inclusive Welcome newcomers Focus on constructive feedback Assume good intentions Questions? Development Questions - See Development Setup Testing Questions - See Testing Guide Documentation - See Documentation Guide General Questions - Visit GitHub Discussions Resources GitHub Repository Issue Tracker Discussions Development Setup Guide Testing Guide ","categories":"","description":"How to contribute to Humus","excerpt":"How to contribute to Humus","ref":"/humus/pr-preview/pr-346/contributing/","tags":"","title":"Contributing"},{"body":"Humus uses a powerful configuration system built on YAML with Go template support and multi-source composition.\nConfiguration Anatomy Basic Structure A Humus configuration file has three main sections:\n# 1. Service Configuration (REST, gRPC, or omitted for Jobs) rest: port: 8080 host: 0.0.0.0 # 2. OpenTelemetry Configuration (optional but recommended) otel: service: name: my-service version: 1.0.0 # 3. Application-Specific Configuration database: host: localhost port: 5432 Config Struct Mapping The YAML maps to a Go struct:\ntype Config struct { // 1. Service config (embedded with squash) rest.Config `config:\",squash\"` // 2. OTel is embedded in rest.Config/grpc.Config // No need to explicitly include it // 3. Application-specific fields Database struct { Host string `config:\"host\"` Port int `config:\"port\"` } `config:\"database\"` } Template Engine Template Functions Humus supports Go template syntax with two key functions:\nenv - Read Environment Variables otel: service: name: {{env \"SERVICE_NAME\"}} Reads the SERVICE_NAME environment variable.\ndefault - Provide Fallback Values rest: port: {{env \"PORT\" | default \"8080\"}} Uses PORT environment variable, falling back to \"8080\" if not set.\nTemplate Examples Database Configuration:\ndatabase: host: {{env \"DB_HOST\" | default \"localhost\"}} port: {{env \"DB_PORT\" | default \"5432\"}} name: {{env \"DB_NAME\"}} user: {{env \"DB_USER\"}} password: {{env \"DB_PASSWORD\"}} # No default for secrets! Feature Flags:\nfeatures: enable_cache: {{env \"ENABLE_CACHE\" | default \"true\"}} enable_auth: {{env \"ENABLE_AUTH\" | default \"false\"}} Environment-Specific Values:\notel: sdk: disabled: {{env \"OTEL_DISABLED\" | default \"false\"}} traces: exporter: otlp: endpoint: {{env \"OTEL_ENDPOINT\" | default \"http://localhost:4318\"}} Multi-Source Configuration Compose multiple configuration files with config.MultiSource:\nimport ( bedrockcfg \"github.com/z5labs/bedrock/pkg/config\" \"github.com/z5labs/humus/rest\" ) func main() { source := bedrockcfg.MultiSource( bedrockcfg.FromYaml(\"defaults.yaml\"), // Base configuration bedrockcfg.FromYaml(\"config.yaml\"), // Overrides ) rest.Run(source, Init) } Use Cases for Multi-Source 1. Framework Defaults + App Config source := bedrockcfg.MultiSource( bedrockcfg.FromYaml(\"default_config.yaml\"), // Humus framework defaults bedrockcfg.FromYaml(\"config.yaml\"), // Your application config ) default_config.yaml (framework):\notel: service: name: unnamed-service sdk: disabled: false config.yaml (your app):\notel: service: name: my-actual-service # Overrides framework default 2. Environment-Specific Overrides import \"os\" func main() { env := os.Getenv(\"ENV\") if env == \"\" { env = \"dev\" } sources := []bedrockcfg.Source{ bedrockcfg.FromYaml(\"config.base.yaml\"), } envConfig := fmt.Sprintf(\"config.%s.yaml\", env) if _, err := os.Stat(envConfig); err == nil { sources = append(sources, bedrockcfg.FromYaml(envConfig)) } rest.Run(bedrockcfg.MultiSource(sources...), Init) } config.base.yaml:\nrest: port: 8080 otel: service: name: my-service config.prod.yaml:\nrest: host: 0.0.0.0 # Only override what changes otel: traces: exporter: otlp: endpoint: https://otel-collector.prod.example.com 3. Local Development Overrides source := bedrockcfg.MultiSource( bedrockcfg.FromYaml(\"config.yaml\"), bedrockcfg.FromYaml(\"config.local.yaml\"), // Gitignored local overrides ) Add to .gitignore:\nconfig.local.yaml Developers can create config.local.yaml for personal settings without affecting others.\nStruct Tags The squash Tag Embeds fields directly into the parent:\ntype Config struct { rest.Config `config:\",squash\"` // Fields embedded at root level } Without squash:\nrest_config: # Would need this nesting port: 8080 With squash:\nrest: # Direct access port: 8080 Custom Field Names type Config struct { DatabaseURL string `config:\"database_url\"` // Maps to database_url in YAML APIKey string `config:\"api_key\"` // Maps to api_key } Nested Structures type Config struct { rest.Config `config:\",squash\"` Database struct { Primary struct { Host string `config:\"host\"` Port int `config:\"port\"` } `config:\"primary\"` Replica struct { Host string `config:\"host\"` Port int `config:\"port\"` } `config:\"replica\"` } `config:\"database\"` } Corresponding YAML:\ndatabase: primary: host: primary.db.example.com port: 5432 replica: host: replica.db.example.com port: 5432 OpenTelemetry Configuration Full OTel Config Structure otel: service: name: my-service # Required version: 1.0.0 # Optional namespace: production # Optional instance_id: pod-1234 # Optional sdk: disabled: false # Set true to disable all OTel # Resource attributes (optional) resource: attributes: deployment.environment: production service.team: platform # Trace configuration traces: sampler: type: parentbased_traceidratio # or always_on, always_off, etc. arg: 0.1 # Sample 10% of traces exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf # or grpc headers: x-custom-header: value # Metrics configuration metrics: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf # Logs configuration logs: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf Disabling OTel For development or testing:\notel: sdk: disabled: true Or via environment variable:\notel: sdk: disabled: {{env \"OTEL_DISABLED\" | default \"false\"}} Best Practices 1. Secrets Management Never commit secrets:\n# Bad database: password: super-secret-password # Good database: password: {{env \"DB_PASSWORD\"}} 2. Required vs Optional Use templates for optional values:\nrest: port: {{env \"PORT\" | default \"8080\"}} # Optional, has default Validate required values in Init:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { if cfg.Database.Password == \"\" { return nil, fmt.Errorf(\"DB_PASSWORD environment variable required\") } // ... } 3. Environment Variable Naming Use consistent prefixes:\n# Good database: host: {{env \"MYAPP_DB_HOST\"}} port: {{env \"MYAPP_DB_PORT\"}} # Avoids conflicts with other apps 4. Document Your Config Add comments to YAML files:\n# HTTP Server Configuration rest: # Port to listen on. Set via PORT environment variable. # Default: 8080 port: {{env \"PORT\" | default \"8080\"}} # Host to bind to. Use 0.0.0.0 for all interfaces. # Default: localhost (for security) host: {{env \"HOST\" | default \"localhost\"}} 5. Config Validation Validate in Init function:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Validate ranges if cfg.REST.Port \u003c 1024 || cfg.REST.Port \u003e 65535 { return nil, fmt.Errorf(\"port must be between 1024 and 65535\") } // Validate required fields if cfg.Database.Host == \"\" { return nil, fmt.Errorf(\"database host is required\") } // Validate mutually exclusive options if cfg.Features.UseCache \u0026\u0026 cfg.Features.UseMemory { return nil, fmt.Errorf(\"cannot enable both cache and memory mode\") } // Continue with initialization... } Next Steps Learn about Observability to understand OTel configuration Explore Lifecycle Management for runtime behavior See Getting Started - Configuration for basic examples ","categories":"","description":"Deep dive into config composition and templating","excerpt":"Deep dive into config composition and templating","ref":"/humus/pr-preview/pr-346/concepts/configuration-system/","tags":"","title":"Configuration System"},{"body":"Installing Humus Humus is installed as a Go module dependency. Add it to your project using go get:\ngo get github.com/z5labs/humus This will download Humus and all its dependencies, including:\nBedrock - Core application lifecycle framework OpenTelemetry SDK - For observability Service-specific dependencies (chi router for REST, gRPC for gRPC services, etc.) Verifying Installation Create a simple main.go file to verify the installation:\npackage main import ( \"fmt\" \"github.com/z5labs/humus/rest\" ) func main() { fmt.Println(\"Humus installed successfully!\") } Run it:\ngo run main.go If you see “Humus installed successfully!”, you’re ready to go!\nDependency Management Humus follows semantic versioning. To ensure reproducible builds, use Go modules:\n# Initialize a new module (if not already done) go mod init your-module-name # Install Humus go get github.com/z5labs/humus # Tidy up dependencies go mod tidy Version Pinning To pin to a specific version:\n# Install a specific version go get github.com/z5labs/humus@v0.1.0 # Or use the latest patch release go get github.com/z5labs/humus@latest Service-Specific Dependencies Depending on which service type you’re building, you may need additional tools:\nFor REST Services No additional dependencies required - everything is included with Humus.\nFor gRPC Services You’ll need the Protocol Buffers compiler and Go plugins:\n# Install protoc (see https://grpc.io/docs/protoc-installation/) # On macOS: brew install protobuf # On Linux: apt install -y protobuf-compiler # Install Go plugins go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest For Job Services No additional dependencies required.\nDevelopment Tools While not required, these tools are recommended for development:\n# golangci-lint for code quality go install github.com/golangci/golangci-lint/cmd/golangci-lint@latest # Air for hot reloading during development go install github.com/air-verse/air@latest Next Steps Now that Humus is installed, continue to Your First Service to build your first application.\n","categories":"","description":"Installing Humus and dependencies","excerpt":"Installing Humus and dependencies","ref":"/humus/pr-preview/pr-346/getting-started/installation/","tags":"","title":"Installation"},{"body":"The Humus Kafka runtime provides a production-ready integration with Apache Kafka, offering concurrent per-partition processing, automatic OpenTelemetry instrumentation, and flexible delivery semantics.\nOverview The Kafka runtime is built on:\nfranz-go - Modern, high-performance Kafka client Goroutine-per-partition - Automatic concurrency with partition isolation OpenTelemetry Integration - Built-in tracing via franz-go kotel plugin Delivery Semantics - Both at-most-once and at-least-once processing Quick Start package main import ( \"context\" \"encoding/json\" \"github.com/z5labs/humus/queue\" \"github.com/z5labs/humus/queue/kafka\" ) type OrderMessage struct { OrderID string `json:\"order_id\"` Amount float64 `json:\"amount\"` } type OrderProcessor struct{} func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Deserialize var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Process order (should be idempotent) return nil } func Init(ctx context.Context, cfg Config) (*queue.App, error) { processor := \u0026OrderProcessor{} runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), ) return queue.NewApp(runtime), nil } func main() { queue.Run(queue.YamlSource(\"config.yaml\"), Init) } Core Components Runtime The main Kafka runtime that manages the Kafka client, consumer group, and partition processing:\nruntime := kafka.NewRuntime( brokers []string, groupID string, opts ...Option, ) Features:\nConsumer group management Automatic rebalancing Graceful shutdown OpenTelemetry integration Message Represents a Kafka message with all metadata:\ntype Message struct { Key []byte Value []byte Headers []Header Timestamp time.Time Topic string Partition int32 Offset int64 Attrs uint8 } Your processor receives this type and must deserialize Value into your application’s message format.\nDelivery Semantics Configure processing semantics per topic:\nAt-Least-Once:\nkafka.AtLeastOnce(topic, processor) Messages acknowledged after successful processing. Requires idempotent processors.\nAt-Most-Once:\nkafka.AtMostOnce(topic, processor) Messages acknowledged before processing. Fast but may lose messages on failures.\nConfiguration Options Consumer Group Settings SessionTimeout:\nkafka.SessionTimeout(10 * time.Second) Default: 45 seconds. Maximum time between heartbeats before considered dead.\nRebalanceTimeout:\nkafka.RebalanceTimeout(30 * time.Second) Default: 30 seconds. Maximum time for rebalance operations.\nFetch Settings FetchMaxBytes:\nkafka.FetchMaxBytes(100 * 1024 * 1024) // 100 MB Default: 50 MB. Maximum bytes to fetch across all partitions per request.\nMaxConcurrentFetches:\nkafka.MaxConcurrentFetches(5) Default: unlimited. Limit concurrent fetch requests to Kafka.\nMulti-Topic Processing Process multiple topics in a single runtime:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.AtLeastOnce(\"events\", eventsProcessor), kafka.AtMostOnce(\"metrics\", metricsProcessor), ) Each topic can have different processors and delivery semantics.\nConcurrency Model The Kafka runtime uses a goroutine-per-partition architecture:\nTopic \"orders\" with 3 partitions: ├─ Partition 0 → Goroutine 1 ├─ Partition 1 → Goroutine 2 └─ Partition 2 → Goroutine 3 Benefits:\nNatural parallelism scaling with partition count Partition isolation (one slow partition doesn’t block others) Automatic coordination via consumer group Rebalancing:\nAssigned partitions spawn new goroutines Revoked partitions gracefully shut down Context cancellation coordinates all goroutines See Concurrency Model for details.\nBuilt-in Features Every Kafka runtime automatically includes:\nConsumer Group Management - Automatic partition assignment and rebalancing OpenTelemetry Tracing - Spans per message with context propagation Graceful Shutdown - Clean shutdown on SIGTERM/SIGINT with offset commits Error Handling - Structured logging with message metadata What You’ll Learn This section covers:\nQuick Start - Build your first Kafka processor Runtime Configuration - Advanced configuration options Message Structure - Working with Kafka messages Concurrency Model - Understanding partition processing Idempotency - Handling duplicate messages Multi-Topic Processing - Processing multiple topics Observability - OpenTelemetry integration Configuration - Production deployment Next Steps Start with the Quick Start Guide to build your first Kafka message processor.\n","categories":"","description":"Apache Kafka integration for message processing","excerpt":"Apache Kafka integration for message processing","ref":"/humus/pr-preview/pr-346/features/queue/kafka/","tags":"","title":"Kafka Runtime"},{"body":"The Humus queue framework implements a three-phase message processing pattern that separates concerns for consuming, processing, and acknowledging messages from a queue.\nCore Interfaces The queue framework defines four core interfaces that work together to process messages:\nConsumer Retrieves messages from a queue:\ntype Consumer[T any] interface { Consume(ctx context.Context) (T, error) } Responsibilities:\nFetch the next message from the queue Handle connection management and retries Return ErrEndOfQueue when the queue is exhausted (for graceful shutdown) Example:\ntype KafkaConsumer struct { client *kgo.Client } func (c *KafkaConsumer) Consume(ctx context.Context) (*Message, error) { fetches := c.client.PollFetches(ctx) if fetches.Empty() { return nil, queue.ErrEndOfQueue } // Return first message return \u0026Message{...}, nil } Processor Executes business logic on messages:\ntype Processor[T any] interface { Process(ctx context.Context, msg T) error } Responsibilities:\nImplement business logic for message handling Be idempotent (for at-least-once processing) Return errors to trigger acknowledgment logic Example:\ntype OrderProcessor struct { db *sql.DB } func (p *OrderProcessor) Process(ctx context.Context, msg *OrderMessage) error { // Check idempotency exists, err := p.orderExists(ctx, msg.OrderID) if err != nil { return err } if exists { return nil // Already processed } // Process the order return p.createOrder(ctx, msg) } Acknowledger Confirms successful processing back to the queue:\ntype Acknowledger[T any] interface { Acknowledge(ctx context.Context, msg T) error } Responsibilities:\nCommit offsets or delete messages from the queue Ensure the queue knows the message was processed Handle acknowledgment failures Example:\ntype KafkaAcknowledger struct { client *kgo.Client } func (a *KafkaAcknowledger) Acknowledge(ctx context.Context, msg *Message) error { // Commit the offset for this message return a.client.CommitRecords(ctx, msg.record) } Runtime Orchestrates the three phases and manages the application lifecycle:\ntype Runtime interface { ProcessQueue(ctx context.Context) error } Responsibilities:\nCoordinate Consumer, Processor, and Acknowledger Implement the delivery semantics (order of phases) Handle graceful shutdown when context is cancelled Manage concurrency (e.g., goroutines per partition) Example:\ntype MyRuntime struct { consumer queue.Consumer[Message] processor queue.Processor[Message] acknowledger queue.Acknowledger[Message] } func (r *MyRuntime) ProcessQueue(ctx context.Context) error { for { // Phase 1: Consume msg, err := r.consumer.Consume(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil // Graceful shutdown } if err != nil { return err } // Phase 2: Process if err := r.processor.Process(ctx, msg); err != nil { return err } // Phase 3: Acknowledge if err := r.acknowledger.Acknowledge(ctx, msg); err != nil { return err } } } Built-in Item Processors The queue package provides two built-in processors that implement different delivery semantics:\nProcessAtMostOnce At-most-once processing acknowledges messages before processing:\nprocessor := queue.ProcessAtMostOnce(consumer, processor, acknowledger) for { err := processor.ProcessItem(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil } // Continue even on errors - message already acknowledged } Processing Order: Consume → Acknowledge → Process\nGuarantees:\nEach message processed at most once Messages may be lost on processing failures Fast throughput ProcessAtLeastOnce At-least-once processing acknowledges messages after successful processing:\nprocessor := queue.ProcessAtLeastOnce(consumer, processor, acknowledger) for { err := processor.ProcessItem(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil } if err != nil { return err // Message not acknowledged, will be retried } } Processing Order: Consume → Process → Acknowledge\nGuarantees:\nEach message processed at least once Messages may be duplicated on failures Requires idempotent processors See Delivery Semantics for a detailed comparison.\nApp Wrapper The queue.App type wraps a Runtime and integrates it with the Bedrock framework:\nfunc NewApp(runtime Runtime) *App Features:\nCalls runtime.ProcessQueue(ctx) on startup Handles context cancellation Returns errors to the framework for logging Example:\nfunc Init(ctx context.Context, cfg Config) (*queue.App, error) { runtime := \u0026MyRuntime{...} return queue.NewApp(runtime), nil } Builder Pattern The queue.Builder function creates a Bedrock AppBuilder with automatic instrumentation:\nfunc Builder[T any](init func(context.Context, T) (*App, error)) bedrock.AppBuilder[T] Automatic Features:\nOpenTelemetry SDK initialization Panic recovery in handlers OS signal handling (SIGTERM, SIGINT, SIGKILL) Graceful shutdown coordination Usage:\nbuilder := queue.Builder(Init) app, err := builder(ctx, cfg) Run Function The queue.Run function provides a complete entry point for queue services:\nfunc Run[T any]( reader io.Reader, init func(context.Context, T) (*App, error), opts ...RunOption, ) error Workflow:\nRead YAML configuration from reader Parse config into type T Call init function to build App Run app until completion or error Log errors and exit Example:\nfunc main() { queue.Run(queue.YamlSource(\"config.yaml\"), Init) } Graceful Shutdown The framework handles graceful shutdown automatically:\nSignal Handling - SIGTERM/SIGINT triggers context cancellation Consumer Stops - Consumer returns ErrEndOfQueue In-Flight Processing - Completes current messages Final Acknowledgment - Commits final offsets (at-least-once) Cleanup - Closes connections and resources Implementation:\nfunc (r *MyRuntime) ProcessQueue(ctx context.Context) error { for { select { case \u003c-ctx.Done(): // Context cancelled, stop consuming return nil default: } msg, err := r.consumer.Consume(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil } // ... process message } } Error Handling The framework provides structured error handling:\nErrEndOfQueue:\nSpecial error signaling queue exhaustion Triggers graceful shutdown Not treated as a failure Processing Errors:\nReturn errors from Processor for at-least-once retry Log and continue for at-most-once (message lost) Fatal Errors:\nConsumer/Acknowledger errors typically fatal Return from ProcessQueue to shut down OpenTelemetry Integration All queue processing is automatically instrumented:\nAutomatic Tracing:\nSpan per message Processing order visible in traces Context propagation through phases Automatic Logging:\nStructured logs with message metadata Error recording in spans Performance metrics No additional configuration needed in your Processor implementation.\nNext Steps Learn about Delivery Semantics to choose the right processing model Build your first processor with the Kafka Quick Start ","categories":"","description":"Core abstractions and patterns for queue processing","excerpt":"Core abstractions and patterns for queue processing","ref":"/humus/pr-preview/pr-346/features/queue/queue-framework/","tags":"","title":"Queue Framework"},{"body":"This guide walks you through building a complete Kafka message processor with at-least-once delivery semantics.\nPrerequisites Go 1.21 or later Kafka cluster (local or remote) Humus installed (go get github.com/z5labs/humus) Running Kafka Locally If you don’t have Kafka running, start it with Docker:\ndocker run -d \\ --name kafka \\ -p 9092:9092 \\ -e KAFKA_ENABLE_KRAFT=yes \\ -e KAFKA_CFG_NODE_ID=1 \\ -e KAFKA_CFG_PROCESS_ROLES=broker,controller \\ -e KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@127.0.0.1:9093 \\ -e KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 \\ -e KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://127.0.0.1:9092 \\ -e KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT \\ -e KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER \\ bitnami/kafka:latest Create a test topic:\ndocker exec kafka kafka-topics.sh \\ --create \\ --topic orders \\ --bootstrap-server localhost:9092 \\ --partitions 3 \\ --replication-factor 1 Project Setup mkdir order-processor cd order-processor go mod init order-processor go get github.com/z5labs/humus Configuration Create config.yaml:\nkafka: brokers: - \"localhost:9092\" group_id: \"order-processors\" topic: \"orders\" otel: service: name: order-processor sdk: disabled: true # Disable for this example Define Your Message Create main.go:\npackage main import ( \"context\" \"encoding/json\" \"fmt\" \"sync\" \"github.com/z5labs/humus/queue\" \"github.com/z5labs/humus/queue/kafka\" ) // OrderMessage represents an order from Kafka type OrderMessage struct { OrderID string `json:\"order_id\"` CustomerID string `json:\"customer_id\"` Amount float64 `json:\"amount\"` } // Config holds application configuration type Config struct { queue.Config `config:\",squash\"` Kafka struct { Brokers []string `config:\"brokers\"` GroupID string `config:\"group_id\"` Topic string `config:\"topic\"` } `config:\"kafka\"` } Implement the Processor An idempotent processor that tracks processed orders:\n// OrderProcessor processes order messages type OrderProcessor struct { mu sync.RWMutex processed map[string]bool } func NewOrderProcessor() *OrderProcessor { return \u0026OrderProcessor{ processed: make(map[string]bool), } } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Deserialize the message var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return fmt.Errorf(\"failed to unmarshal order: %w\", err) } // Idempotency check p.mu.RLock() if p.processed[order.OrderID] { p.mu.RUnlock() fmt.Printf(\"Order %s already processed, skipping\\n\", order.OrderID) return nil } p.mu.RUnlock() // Process the order fmt.Printf(\"Processing order: ID=%s, Customer=%s, Amount=$%.2f\\n\", order.OrderID, order.CustomerID, order.Amount, ) // Simulate order processing // In production: save to database, call payment service, etc. // Mark as processed p.mu.Lock() p.processed[order.OrderID] = true p.mu.Unlock() return nil } Initialize the Runtime Configure the Kafka runtime with at-least-once processing:\nfunc Init(ctx context.Context, cfg Config) (*queue.App, error) { processor := NewOrderProcessor() runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), ) return queue.NewApp(runtime), nil } func main() { queue.Run(queue.YamlSource(\"config.yaml\"), Init) } Run the Processor go run main.go You should see output indicating the processor is running:\nINFO Starting order-processor INFO Kafka consumer group initialized group_id=order-processors Test with Messages In another terminal, produce test messages to Kafka:\n# Message 1 echo '{\"order_id\":\"ord-001\",\"customer_id\":\"cust-123\",\"amount\":99.99}' | \\ docker exec -i kafka kafka-console-producer.sh \\ --broker-list localhost:9092 \\ --topic orders # Message 2 echo '{\"order_id\":\"ord-002\",\"customer_id\":\"cust-456\",\"amount\":149.99}' | \\ docker exec -i kafka kafka-console-producer.sh \\ --broker-list localhost:9092 \\ --topic orders # Duplicate of Message 1 (to test idempotency) echo '{\"order_id\":\"ord-001\",\"customer_id\":\"cust-123\",\"amount\":99.99}' | \\ docker exec -i kafka kafka-console-producer.sh \\ --broker-list localhost:9092 \\ --topic orders Your processor should output:\nProcessing order: ID=ord-001, Customer=cust-123, Amount=$99.99 Processing order: ID=ord-002, Customer=cust-456, Amount=$149.99 Order ord-001 already processed, skipping Notice the duplicate message was detected and skipped.\nWhat’s Happening Let’s break down the key components:\n1. Configuration The YAML config provides Kafka connection details and consumer group settings:\nkafka: brokers: [\"localhost:9092\"] # Kafka broker addresses group_id: \"order-processors\" # Consumer group for offset tracking topic: \"orders\" # Topic to consume from 2. Message Deserialization The processor receives kafka.Message with raw bytes:\nfunc (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage json.Unmarshal(msg.Value, \u0026order) // ... } msg.Value contains the JSON bytes, which we deserialize into OrderMessage.\n3. Idempotent Processing The processor tracks processed order IDs to handle duplicates:\nif p.processed[order.OrderID] { return nil // Skip duplicate } // Process... p.processed[order.OrderID] = true This is critical for at-least-once processing where Kafka may redeliver messages.\n4. At-Least-Once Semantics kafka.AtLeastOnce(cfg.Kafka.Topic, processor) This ensures:\nMessages are processed before offsets are committed Failed processing results in message redelivery No messages are lost due to processing failures 5. Graceful Shutdown Press Ctrl+C to stop the processor. You’ll see:\nINFO Shutting down gracefully INFO Committing final offsets INFO Kafka client closed The framework ensures in-flight messages complete before shutdown.\nProduction Considerations This example uses in-memory state. For production:\nDatabase-Backed Idempotency Replace the in-memory map with database storage:\nfunc (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Check database for existing order var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM orders WHERE order_id = $1)\", order.OrderID, ).Scan(\u0026exists) if err != nil { return err } if exists { return nil // Already processed } // Process in transaction tx, err := p.db.BeginTx(ctx, nil) if err != nil { return err } defer tx.Rollback() _, err = tx.ExecContext(ctx, \"INSERT INTO orders (order_id, customer_id, amount) VALUES ($1, $2, $3)\", order.OrderID, order.CustomerID, order.Amount, ) if err != nil { return err } return tx.Commit() } See Idempotency for detailed patterns.\nEnable OpenTelemetry For production observability, enable OTel in config.yaml:\notel: service: name: order-processor sdk: disabled: false exporter: otlp: endpoint: \"localhost:4317\" protocol: grpc See Observability for details.\nTune Performance Adjust fetch settings for your workload:\nruntime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), kafka.FetchMaxBytes(100 * 1024 * 1024), // 100 MB kafka.MaxConcurrentFetches(10), ) See Configuration for tuning guidance.\nComplete Code Full main.go (click to expand) package main import ( \"context\" \"encoding/json\" \"fmt\" \"sync\" \"github.com/z5labs/humus/queue\" \"github.com/z5labs/humus/queue/kafka\" ) type OrderMessage struct { OrderID string `json:\"order_id\"` CustomerID string `json:\"customer_id\"` Amount float64 `json:\"amount\"` } type Config struct { queue.Config `config:\",squash\"` Kafka struct { Brokers []string `config:\"brokers\"` GroupID string `config:\"group_id\"` Topic string `config:\"topic\"` } `config:\"kafka\"` } type OrderProcessor struct { mu sync.RWMutex processed map[string]bool } func NewOrderProcessor() *OrderProcessor { return \u0026OrderProcessor{ processed: make(map[string]bool), } } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return fmt.Errorf(\"failed to unmarshal order: %w\", err) } p.mu.RLock() if p.processed[order.OrderID] { p.mu.RUnlock() fmt.Printf(\"Order %s already processed, skipping\\n\", order.OrderID) return nil } p.mu.RUnlock() fmt.Printf(\"Processing order: ID=%s, Customer=%s, Amount=$%.2f\\n\", order.OrderID, order.CustomerID, order.Amount, ) p.mu.Lock() p.processed[order.OrderID] = true p.mu.Unlock() return nil } func Init(ctx context.Context, cfg Config) (*queue.App, error) { processor := NewOrderProcessor() runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), ) return queue.NewApp(runtime), nil } func main() { queue.Run(queue.YamlSource(\"config.yaml\"), Init) } Next Steps Learn about Message Structure to work with headers and metadata Explore Multi-Topic Processing to handle multiple topics Understand Concurrency Model for partition-level parallelism Implement robust Idempotency patterns for production ","categories":"","description":"Build your first Kafka message processor","excerpt":"Build your first Kafka message processor","ref":"/humus/pr-preview/pr-346/features/queue/kafka/quick-start/","tags":"","title":"Quick Start"},{"body":"This guide walks you through building a complete REST API with CRUD operations.\nPrerequisites Go 1.21 or later Humus installed (go get github.com/z5labs/humus) Project Setup mkdir todo-api cd todo-api go mod init todo-api go get github.com/z5labs/humus Configuration Create config.yaml:\nrest: port: 8080 otel: service: name: todo-api sdk: disabled: true # Disable for this example Define Your Model Create main.go:\npackage main import ( \"context\" \"fmt\" \"net/http\" \"sync\" \"github.com/z5labs/humus/rest\" \"github.com/z5labs/humus/rest/rpc\" ) type Todo struct { ID string `json:\"id\"` Title string `json:\"title\"` Completed bool `json:\"completed\"` } // In-memory store type TodoStore struct { mu sync.RWMutex todos map[string]Todo } func NewTodoStore() *TodoStore { return \u0026TodoStore{ todos: make(map[string]Todo), } } func (s *TodoStore) Create(todo Todo) { s.mu.Lock() defer s.mu.Unlock() s.todos[todo.ID] = todo } func (s *TodoStore) Get(id string) (Todo, bool) { s.mu.RLock() defer s.mu.RUnlock() todo, ok := s.todos[id] return todo, ok } func (s *TodoStore) List() []Todo { s.mu.RLock() defer s.mu.RUnlock() todos := make([]Todo, 0, len(s.todos)) for _, todo := range s.todos { todos = append(todos, todo) } return todos } func (s *TodoStore) Update(todo Todo) bool { s.mu.Lock() defer s.mu.Unlock() if _, exists := s.todos[todo.ID]; !exists { return false } s.todos[todo.ID] = todo return true } func (s *TodoStore) Delete(id string) bool { s.mu.Lock() defer s.mu.Unlock() if _, exists := s.todos[id]; !exists { return false } delete(s.todos, id) return true } Configuration Struct type Config struct { rest.Config `config:\",squash\"` } Main Function func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } Initialize API func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"Todo API\", \"1.0.0\") store := NewTodoStore() // Register handlers registerHandlers(api, store) return api, nil } func registerHandlers(api *rest.Api, store *TodoStore) { // Create todo createHandler := rpc.NewOperation( rpc.ConsumeJson( rpc.ReturnJson( rpc.Handle(func(ctx context.Context, req Todo) (Todo, error) { if req.ID == \"\" { req.ID = fmt.Sprintf(\"todo-%d\", len(store.todos)+1) } store.Create(req) return req, nil }), ), ), ) rest.Handle(http.MethodPost, rest.BasePath(\"/todos\"), createHandler) // List todos listHandler := rpc.NewOperation( rpc.ReturnJson( rpc.Handle(func(ctx context.Context, _ any) ([]Todo, error) { return store.List(), nil }), ), ) rest.Handle(http.MethodGet, rest.BasePath(\"/todos\"), listHandler) // Get todo getHandler := rpc.NewOperation( rpc.ReturnJson( rpc.Handle(func(ctx context.Context, req PathParams) (Todo, error) { todo, ok := store.Get(req.ID) if !ok { return Todo{}, fmt.Errorf(\"todo not found\") } return todo, nil }), ), ) rest.Handle(http.MethodGet, rest.BasePath(\"/todos\").Param(\"id\"), getHandler) // Update todo updateHandler := rpc.NewOperation( rpc.ConsumeJson( rpc.ReturnJson( rpc.Handle(func(ctx context.Context, req UpdateRequest) (Todo, error) { req.Todo.ID = req.ID if !store.Update(req.Todo) { return Todo{}, fmt.Errorf(\"todo not found\") } return req.Todo, nil }), ), ), ) rest.Handle(http.MethodPut, rest.BasePath(\"/todos\").Param(\"id\"), updateHandler) // Delete todo deleteHandler := rpc.NewOperation( rpc.Handle(func(ctx context.Context, req PathParams) (string, error) { if !store.Delete(req.ID) { return \"\", fmt.Errorf(\"todo not found\") } return \"deleted\", nil }), ) rest.Handle(http.MethodDelete, rest.BasePath(\"/todos\").Param(\"id\"), deleteHandler) } type PathParams struct { ID string `path:\"id\"` } type UpdateRequest struct { ID string `path:\"id\"` Todo Todo `json:\",inline\"` } Complete Code Put it all together in main.go:\npackage main import ( \"context\" \"fmt\" \"net/http\" \"sync\" \"github.com/z5labs/humus/rest\" \"github.com/z5labs/humus/rest/rpc\" ) type Todo struct { ID string `json:\"id\"` Title string `json:\"title\"` Completed bool `json:\"completed\"` } type TodoStore struct { mu sync.RWMutex todos map[string]Todo } func NewTodoStore() *TodoStore { return \u0026TodoStore{ todos: make(map[string]Todo), } } func (s *TodoStore) Create(todo Todo) { s.mu.Lock() defer s.mu.Unlock() s.todos[todo.ID] = todo } func (s *TodoStore) Get(id string) (Todo, bool) { s.mu.RLock() defer s.mu.RUnlock() todo, ok := s.todos[id] return todo, ok } func (s *TodoStore) List() []Todo { s.mu.RLock() defer s.mu.RUnlock() todos := make([]Todo, 0, len(s.todos)) for _, todo := range s.todos { todos = append(todos, todo) } return todos } func (s *TodoStore) Update(todo Todo) bool { s.mu.Lock() defer s.mu.Unlock() if _, exists := s.todos[todo.ID]; !exists { return false } s.todos[todo.ID] = todo return true } func (s *TodoStore) Delete(id string) bool { s.mu.Lock() defer s.mu.Unlock() if _, exists := s.todos[id]; !exists { return false } delete(s.todos, id) return true } type Config struct { rest.Config `config:\",squash\"` } func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"Todo API\", \"1.0.0\") store := NewTodoStore() registerHandlers(api, store) return api, nil } func registerHandlers(api *rest.Api, store *TodoStore) { createHandler := rpc.NewOperation( rpc.ConsumeJson( rpc.ReturnJson( rpc.Handle(func(ctx context.Context, req Todo) (Todo, error) { if req.ID == \"\" { req.ID = fmt.Sprintf(\"todo-%d\", len(store.todos)+1) } store.Create(req) return req, nil }), ), ), ) rest.Handle(http.MethodPost, rest.BasePath(\"/todos\"), createHandler) listHandler := rpc.NewOperation( rpc.ReturnJson( rpc.Handle(func(ctx context.Context, _ any) ([]Todo, error) { return store.List(), nil }), ), ) rest.Handle(http.MethodGet, rest.BasePath(\"/todos\"), listHandler) getHandler := rpc.NewOperation( rpc.ReturnJson( rpc.Handle(func(ctx context.Context, req PathParams) (Todo, error) { todo, ok := store.Get(req.ID) if !ok { return Todo{}, fmt.Errorf(\"todo not found\") } return todo, nil }), ), ) rest.Handle(http.MethodGet, rest.BasePath(\"/todos\").Param(\"id\"), getHandler) updateHandler := rpc.NewOperation( rpc.ConsumeJson( rpc.ReturnJson( rpc.Handle(func(ctx context.Context, req UpdateRequest) (Todo, error) { req.Todo.ID = req.ID if !store.Update(req.Todo) { return Todo{}, fmt.Errorf(\"todo not found\") } return req.Todo, nil }), ), ), ) rest.Handle(http.MethodPut, rest.BasePath(\"/todos\").Param(\"id\"), updateHandler) deleteHandler := rpc.NewOperation( rpc.Handle(func(ctx context.Context, req PathParams) (string, error) { if !store.Delete(req.ID) { return \"\", fmt.Errorf(\"todo not found\") } return \"deleted\", nil }), ) rest.Handle(http.MethodDelete, rest.BasePath(\"/todos\").Param(\"id\"), deleteHandler) } type PathParams struct { ID string `path:\"id\"` } type UpdateRequest struct { ID string `path:\"id\"` Todo Todo `json:\",inline\"` } Run the Service go run main.go Test the API # Create a todo curl -X POST http://localhost:8080/todos \\ -H \"Content-Type: application/json\" \\ -d '{\"title\": \"Learn Humus\", \"completed\": false}' # List all todos curl http://localhost:8080/todos # Get a specific todo curl http://localhost:8080/todos/todo-1 # Update a todo curl -X PUT http://localhost:8080/todos/todo-1 \\ -H \"Content-Type: application/json\" \\ -d '{\"title\": \"Learn Humus\", \"completed\": true}' # Delete a todo curl -X DELETE http://localhost:8080/todos/todo-1 # View OpenAPI spec curl http://localhost:8080/openapi.json What’s Happening rest.Run() loads config and calls Init rest.NewApi() creates the API with name and version rpc.NewOperation() wraps handlers with type-safe serialization rest.Handle() registers handlers at specific paths Automatic instrumentation traces all requests OpenAPI generation creates /openapi.json from your types Securing Your API (Optional) Add JWT authentication to protect write operations:\n1. Create a Simple JWT Verifier import ( \"context\" \"fmt\" ) type SimpleJWTVerifier struct{} func (v *SimpleJWTVerifier) Verify(ctx context.Context, token string) (context.Context, error) { // In production, verify the JWT signature and claims // For this example, we just accept any non-empty token if token == \"\" { return nil, fmt.Errorf(\"empty token\") } // Extract user info (in production, parse from JWT claims) userID := \"user-from-token\" return context.WithValue(ctx, \"user_id\", userID), nil } 2. Protect Create/Update/Delete Operations func registerHandlers(api *rest.Api, store *TodoStore) { verifier := \u0026SimpleJWTVerifier{} // Public endpoint - no auth required listHandler := rpc.NewOperation( rpc.ReturnJson( rpc.Handle(func(ctx context.Context, _ any) ([]Todo, error) { return store.List(), nil }), ), ) rest.Handle(http.MethodGet, rest.BasePath(\"/todos\"), listHandler) // Protected endpoint - JWT required createHandler := rpc.NewOperation( rpc.ConsumeJson( rpc.ReturnJson( rpc.Handle(func(ctx context.Context, req Todo) (Todo, error) { if req.ID == \"\" { req.ID = fmt.Sprintf(\"todo-%d\", len(store.todos)+1) } store.Create(req) return req, nil }), ), ), ) rest.Handle( http.MethodPost, rest.BasePath(\"/todos\"), createHandler, rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ) // Other endpoints... } 3. Test with Authentication # Fails - no Authorization header curl -X POST http://localhost:8080/todos \\ -H \"Content-Type: application/json\" \\ -d '{\"title\": \"Protected todo\"}' # Returns: 401 Unauthorized # Success - with Bearer token curl -X POST http://localhost:8080/todos \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer my-token\" \\ -d '{\"title\": \"Protected todo\"}' # Returns: 200 OK For production JWT implementation with proper signature verification, see Authentication.\nNext Steps Learn about Authentication for production-ready JWT verification Explore REST API for advanced API configuration Read RPC Pattern for type-safe handlers See Routing for path parameters and validation Understand Error Handling for custom error responses ","categories":"","description":"Build your first REST API","excerpt":"Build your first REST API","ref":"/humus/pr-preview/pr-346/features/rest/quick-start/","tags":"","title":"Quick Start"},{"body":"Humus REST services provide a complete framework for building OpenAPI-compliant HTTP APIs with automatic schema generation, type-safe handlers, and built-in observability.\nOverview REST services in Humus are built on:\nchi - Fast, lightweight HTTP router OpenAPI 3.0 - Automatic API documentation Type Safety - Compile-time type checking for requests and responses OpenTelemetry - Automatic tracing and metrics Quick Start package main import ( \"context\" \"net/http\" \"github.com/z5labs/humus/rest\" \"github.com/z5labs/humus/rest/rpc\" ) type Config struct { rest.Config `config:\",squash\"` } type HelloResponse struct { Message string `json:\"message\"` } func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"Hello Service\", \"1.0.0\") handler := rpc.NewOperation( rpc.ReturnJson( rpc.Handle(func(ctx context.Context, _ any) (HelloResponse, error) { return HelloResponse{Message: \"Hello, World!\"}, nil }), ), ) rest.Handle(http.MethodGet, rest.BasePath(\"/hello\"), handler) return api, nil } Core Components rest.Api The main API object that combines:\nHTTP router (chi) OpenAPI spec generator Health check endpoints Middleware management rest/rpc Pattern Type-safe handler abstraction:\nrpc.Handler[Req, Resp] - Business logic interface Request deserialization (JSON, XML, etc.) Response serialization OpenAPI schema generation Path Building Flexible path definition:\nStatic paths: /users Path parameters: /users/{id} Nested paths: /users/{id}/posts/{postId} Built-in Endpoints Every REST service automatically includes:\nGET /openapi.json - OpenAPI 3.0 specification GET /health/liveness - Liveness probe GET /health/readiness - Readiness probe What You’ll Learn This section covers:\nQuick Start - Build your first REST API REST API - Understanding rest.Api RPC Pattern - Type-safe handlers Routing - Paths and parameters Authentication - JWT, API keys, and security Request/Response - Serialization patterns Error Handling - Custom error responses OpenAPI - Working with generated specs Health Checks - Monitoring service health Next Steps Start with the Quick Start Guide to build your first REST service.\n","categories":"","description":"Building OpenAPI-compliant HTTP APIs","excerpt":"Building OpenAPI-compliant HTTP APIs","ref":"/humus/pr-preview/pr-346/features/rest/","tags":"","title":"REST Services"},{"body":"Queue processing services must choose between two fundamental delivery guarantees: at-most-once and at-least-once. This choice affects how your application handles failures and determines the reliability guarantees you can provide.\nQuick Comparison Aspect At-Most-Once At-Least-Once Processing Order Consume → Acknowledge → Process Consume → Process → Acknowledge On Failure Message lost Message retried Duplicates Never Possible Throughput Higher Lower Processor Requirements None Must be idempotent Use Cases Metrics, logs, caching Transactions, database updates At-Most-Once Processing At-most-once processing acknowledges messages before processing them. This provides fast throughput but means messages can be lost if processing fails.\nProcessing Flow 1. Consume message from queue 2. Acknowledge message (commit offset) 3. Process message If step 3 fails, the message has already been acknowledged and is permanently lost.\nWhen to Use At-most-once is appropriate when:\nPerformance is critical - Lower latency and higher throughput Data loss is acceptable - Occasional message loss won’t impact your application Messages are non-critical - Informational data that can be recreated or ignored Common Use Cases Metrics Collection:\ntype MetricsProcessor struct { client *prometheus.Client } func (p *MetricsProcessor) Process(ctx context.Context, msg *MetricMessage) error { // Send metric to monitoring system // If this fails, we can tolerate losing a few data points return p.client.RecordMetric(ctx, msg.Name, msg.Value) } Log Aggregation:\ntype LogProcessor struct { writer *LogWriter } func (p *LogProcessor) Process(ctx context.Context, msg *LogMessage) error { // Write log to aggregation system // Missing a few log entries is acceptable return p.writer.Write(ctx, msg.Level, msg.Message) } Cache Updates:\ntype CacheProcessor struct { cache *redis.Client } func (p *CacheProcessor) Process(ctx context.Context, msg *CacheUpdate) error { // Update cache entry // Cache can be rebuilt if some updates are lost return p.cache.Set(ctx, msg.Key, msg.Value, msg.TTL) } Advantages Higher throughput - No waiting for processing to complete before acknowledging Lower latency - Messages acknowledged immediately Simpler implementation - No idempotency requirements Faster recovery - Failures don’t block message consumption Disadvantages Data loss - Processing failures result in lost messages No retry logic - Failed messages are not retried Weaker guarantees - Cannot ensure all messages are processed Implementation processor := queue.ProcessAtMostOnce(consumer, processor, acknowledger) for { err := processor.ProcessItem(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil // Graceful shutdown } // Continue processing even on errors // Message already acknowledged and lost } At-Least-Once Processing At-least-once processing acknowledges messages after successful processing. This provides reliable delivery but means messages may be processed multiple times.\nProcessing Flow 1. Consume message from queue 2. Process message 3. Acknowledge message (commit offset) If step 2 fails, the message is not acknowledged and will be redelivered for retry.\nWhen to Use At-least-once is appropriate when:\nReliability is critical - Every message must be processed successfully Data loss is unacceptable - Missing messages would corrupt data or business logic Idempotency is achievable - Your processor can handle duplicate messages safely Common Use Cases Financial Transactions:\ntype PaymentProcessor struct { db *sql.DB } func (p *PaymentProcessor) Process(ctx context.Context, msg *Payment) error { // Check if already processed (idempotency) var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM payments WHERE transaction_id = $1)\", msg.TransactionID, ).Scan(\u0026exists) if err != nil { return err } if exists { return nil // Already processed, skip } // Process payment _, err = p.db.ExecContext(ctx, \"INSERT INTO payments (transaction_id, amount, status) VALUES ($1, $2, 'completed')\", msg.TransactionID, msg.Amount, ) return err } Database Updates:\ntype OrderProcessor struct { db *sql.DB } func (p *OrderProcessor) Process(ctx context.Context, msg *Order) error { // Upsert: idempotent database operation _, err := p.db.ExecContext(ctx, `INSERT INTO orders (order_id, customer_id, total) VALUES ($1, $2, $3) ON CONFLICT (order_id) DO UPDATE SET customer_id = EXCLUDED.customer_id, total = EXCLUDED.total`, msg.OrderID, msg.CustomerID, msg.Total, ) return err } Event Sourcing:\ntype EventProcessor struct { store EventStore } func (p *EventProcessor) Process(ctx context.Context, msg *Event) error { // Event store handles deduplication return p.store.Append(ctx, msg.StreamID, msg) } Advantages Reliable delivery - All messages are processed successfully Automatic retry - Failed messages are retried automatically Stronger guarantees - Can ensure critical operations complete Data integrity - No messages lost or skipped Disadvantages Lower throughput - Must wait for processing to complete before acknowledging Higher latency - Acknowledgment delayed until processing succeeds Duplicate processing - Messages may be processed multiple times Idempotency required - Processors must handle duplicates correctly Implementation processor := queue.ProcessAtLeastOnce(consumer, processor, acknowledger) for { err := processor.ProcessItem(ctx) if errors.Is(err, queue.ErrEndOfQueue) { return nil // Graceful shutdown } if err != nil { return err // Stop processing on error } } Choosing the Right Semantic Use this decision tree to choose the appropriate semantic:\nCan your application tolerate message loss? ├─ Yes → Is performance critical? │ ├─ Yes → At-Most-Once │ └─ No → Either (prefer At-Most-Once for simplicity) └─ No → Can you implement idempotent processing? ├─ Yes → At-Least-Once └─ No → Redesign to support idempotency or accept data loss Questions to Ask What happens if a message is lost?\nCritical failure → At-Least-Once Acceptable loss → At-Most-Once Can your processor handle duplicate messages?\nYes (idempotent) → At-Least-Once is safe No → At-Most-Once or redesign What are your performance requirements?\nHigh throughput needed → At-Most-Once Reliability more important → At-Least-Once What is the cost of duplicate processing?\nLow (read-only, idempotent) → At-Least-Once is safe High (side effects, non-idempotent) → At-Most-Once or redesign Idempotency Strategies At-least-once processing requires idempotent processors. Common strategies:\nUnique ID Tracking Store processed message IDs in a database:\nfunc (p *Processor) Process(ctx context.Context, msg *Message) error { // Check if already processed var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM processed_messages WHERE message_id = $1)\", msg.ID, ).Scan(\u0026exists) if err != nil { return err } if exists { return nil } // Process and record in same transaction tx, err := p.db.BeginTx(ctx, nil) if err != nil { return err } defer tx.Rollback() // Do work if err := p.doWork(ctx, tx, msg); err != nil { return err } // Record processed _, err = tx.ExecContext(ctx, \"INSERT INTO processed_messages (message_id) VALUES ($1)\", msg.ID, ) if err != nil { return err } return tx.Commit() } Natural Idempotency Design operations to be naturally idempotent:\n// Idempotent: Setting a value UPDATE users SET email = 'new@example.com' WHERE id = 123 // NOT idempotent: Incrementing a value UPDATE accounts SET balance = balance + 100 WHERE id = 456 Upsert Operations Use database upserts for idempotent writes:\n_, err := db.ExecContext(ctx, `INSERT INTO orders (order_id, total) VALUES ($1, $2) ON CONFLICT (order_id) DO UPDATE SET total = EXCLUDED.total`, msg.OrderID, msg.Total, ) See Kafka Idempotency for Kafka-specific patterns.\nMixed Semantics Some applications may need different semantics for different message types:\nfunc Init(ctx context.Context, cfg Config) (*queue.App, error) { // Critical orders: at-least-once ordersRuntime, err := kafka.NewAtLeastOnceRuntime( cfg.Kafka.Brokers, \"orders\", cfg.Kafka.GroupID, ordersProcessor, decodeOrder, ) if err != nil { return nil, err } // Non-critical metrics: at-most-once metricsRuntime, err := kafka.NewAtMostOnceRuntime( cfg.Kafka.Brokers, \"metrics\", cfg.Kafka.GroupID, metricsProcessor, decodeMetric, ) if err != nil { return nil, err } // Combine runtimes (implementation-specific) runtime := newMultiRuntime(ordersRuntime, metricsRuntime) return queue.NewApp(runtime), nil } Next Steps Implement idempotent processors with Kafka Idempotency Learn about Kafka-specific features in Kafka Runtime Build your first processor with Kafka Quick Start ","categories":"","description":"Understanding at-most-once and at-least-once processing","excerpt":"Understanding at-most-once and at-least-once processing","ref":"/humus/pr-preview/pr-346/features/queue/delivery-semantics/","tags":"","title":"Delivery Semantics"},{"body":"Humus gRPC services provide a complete framework for building high-performance microservices with automatic instrumentation, health checks, and seamless Protocol Buffers integration.\nOverview gRPC services in Humus are built on:\ngRPC-Go - Official gRPC implementation Automatic Health Service - gRPC health checking protocol OpenTelemetry Interceptors - Built-in tracing and metrics Service Registration - Simple API for registering services Quick Start package main import ( \"context\" \"github.com/z5labs/humus/grpc\" pb \"your-module/gen/proto/user\" ) type Config struct { grpc.Config `config:\",squash\"` } type userService struct { pb.UnimplementedUserServiceServer } func (s *userService) GetUser(ctx context.Context, req *pb.GetUserRequest) (*pb.User, error) { return \u0026pb.User{ Id: req.Id, Name: \"John Doe\", Email: \"john@example.com\", }, nil } func main() { grpc.Run(grpc.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*grpc.Api, error) { api := grpc.NewApi() // Register your service pb.RegisterUserServiceServer(api, \u0026userService{}) return api, nil } Core Components grpc.Api The main API object that:\nImplements grpc.ServiceRegistrar Manages interceptors for OTel Automatically registers health service Monitors registered services Automatic Features Every gRPC service gets:\nHealth Service - Implements grpc.health.v1.Health protocol Tracing - Automatic span creation for RPCs Metrics - Request count, duration, status Service Monitoring - Health checks for services implementing health.Monitor Built-in Health Service The gRPC health service is automatically registered and provides:\n/grpc.health.v1.Health/Check - Check service health /grpc.health.v1.Health/Watch - Stream health updates No configuration needed - it works out of the box.\nWhat You’ll Learn This section covers:\nQuick Start - Build your first gRPC service gRPC API - Understanding grpc.Api Health Service - Health checking protocol Interceptors - OTel instrumentation Petstore Example - Complete walkthrough Next Steps Start with the Quick Start Guide to build your first gRPC service.\n","categories":"","description":"Building high-performance microservices","excerpt":"Building high-performance microservices","ref":"/humus/pr-preview/pr-346/features/grpc/","tags":"","title":"gRPC Services"},{"body":"Humus provides built-in observability through automatic OpenTelemetry (OTel) integration. Every service gets traces, metrics, and logs out of the box.\nOverview OpenTelemetry is automatically initialized when your Humus service starts. You get:\nDistributed Tracing - Automatic HTTP/gRPC tracing plus manual instrumentation Metrics - Built-in HTTP/gRPC metrics plus custom metrics Structured Logging - Integrated slog logger with trace correlation Automatic Instrumentation REST Services HTTP handlers are automatically instrumented with:\nRequest tracing - Each request creates a span HTTP metrics - Request count, duration, status codes Error tracking - Automatic error recording in spans // No extra code needed - automatic instrumentation! func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"My Service\", \"1.0.0\") handler := rpc.NewOperation( rpc.Handle(handleRequest), // Automatically traced ) rest.Handle(http.MethodGet, rest.BasePath(\"/users\"), handler) return api, nil } gRPC Services gRPC methods are automatically instrumented via interceptors:\nRPC tracing - Each RPC creates a span gRPC metrics - Call count, duration, status Error tracking - Automatic error recording // gRPC instrumentation is automatic via interceptors func Init(ctx context.Context, cfg Config) (*grpc.Api, error) { api := grpc.NewApi() userpb.RegisterUserServiceServer(api, \u0026userService{}) // Automatically traced return api, nil } Job Services Jobs are traced from start to finish:\ntype MyJob struct{} func (j *MyJob) Handle(ctx context.Context) error { // The entire job execution is automatically traced // ctx already contains trace context return processJob(ctx) } Configuration Basic OTel Config Minimal configuration for local development:\notel: service: name: my-service sdk: disabled: false # Enable OTel Production Config Full configuration for production:\notel: service: name: my-service version: 1.0.0 namespace: production instance_id: {{env \"POD_NAME\"}} sdk: disabled: false resource: attributes: deployment.environment: production service.team: platform k8s.cluster.name: prod-cluster traces: sampler: type: parentbased_traceidratio arg: 0.1 # Sample 10% of traces exporter: otlp: endpoint: {{env \"OTEL_EXPORTER_OTLP_ENDPOINT\"}} protocol: grpc headers: api-key: {{env \"OTEL_API_KEY\"}} metrics: exporter: otlp: endpoint: {{env \"OTEL_EXPORTER_OTLP_ENDPOINT\"}} protocol: grpc logs: exporter: otlp: endpoint: {{env \"OTEL_EXPORTER_OTLP_ENDPOINT\"}} protocol: grpc Disabling OTel For development or testing:\notel: sdk: disabled: true # No telemetry overhead Manual Instrumentation Creating Spans Use the standard OTel SDK to create custom spans:\nimport ( \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/attribute\" \"go.opentelemetry.io/otel/codes\" ) func processOrder(ctx context.Context, orderID string) error { // Get a tracer tracer := otel.Tracer(\"my-service\") // Create a span ctx, span := tracer.Start(ctx, \"processOrder\") defer span.End() // Add attributes span.SetAttributes( attribute.String(\"order.id\", orderID), attribute.Int(\"order.items\", 5), ) // Do work... if err := validateOrder(ctx, orderID); err != nil { // Record error span.RecordError(err) span.SetStatus(codes.Error, \"validation failed\") return err } span.SetStatus(codes.Ok, \"order processed\") return nil } Nested Spans Create hierarchical traces:\nfunc processOrder(ctx context.Context, orderID string) error { tracer := otel.Tracer(\"my-service\") ctx, span := tracer.Start(ctx, \"processOrder\") defer span.End() // Child span 1 if err := validateOrder(ctx, orderID); err != nil { return err } // Child span 2 if err := chargePayment(ctx, orderID); err != nil { return err } return nil } func validateOrder(ctx context.Context, orderID string) error { tracer := otel.Tracer(\"my-service\") // This will be a child of \"processOrder\" span ctx, span := tracer.Start(ctx, \"validateOrder\") defer span.End() // Validation logic... return nil } Recording Metrics Create custom metrics:\nimport ( \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/metric\" ) var ( orderCounter metric.Int64Counter orderDuration metric.Float64Histogram ) func init() { meter := otel.Meter(\"my-service\") orderCounter, _ = meter.Int64Counter( \"orders.processed\", metric.WithDescription(\"Number of orders processed\"), ) orderDuration, _ = meter.Float64Histogram( \"orders.duration\", metric.WithDescription(\"Order processing duration\"), metric.WithUnit(\"ms\"), ) } func processOrder(ctx context.Context, orderID string) error { start := time.Now() // Process order... // Record metrics orderCounter.Add(ctx, 1, metric.WithAttributes( attribute.String(\"status\", \"success\"), ), ) duration := time.Since(start).Milliseconds() orderDuration.Record(ctx, float64(duration)) return nil } Structured Logging Using Humus Logger Get an OpenTelemetry-integrated logger:\nimport \"github.com/z5labs/humus\" func processOrder(ctx context.Context, orderID string) error { log := humus.Logger(\"order-processor\") log.InfoContext(ctx, \"processing order\", \"order_id\", orderID, \"user_id\", \"user123\", ) if err := validateOrder(ctx, orderID); err != nil { log.ErrorContext(ctx, \"validation failed\", \"order_id\", orderID, \"error\", err, ) return err } log.InfoContext(ctx, \"order processed successfully\", \"order_id\", orderID, ) return nil } Log-Trace Correlation Logs automatically include trace context when using InfoContext, ErrorContext, etc.:\nfunc handleRequest(ctx context.Context, req Request) (Response, error) { log := humus.Logger(\"handler\") // This log will include trace_id and span_id log.InfoContext(ctx, \"handling request\", \"request_id\", req.ID, ) // Trace and logs are correlated automatically tracer := otel.Tracer(\"my-service\") ctx, span := tracer.Start(ctx, \"processRequest\") defer span.End() // This log also includes the same trace context log.InfoContext(ctx, \"processing\", \"step\", \"validation\", ) return processRequest(ctx, req) } Log Levels Use appropriate log levels:\nlog := humus.Logger(\"my-service\") // Debug - verbose information for debugging log.DebugContext(ctx, \"cache hit\", \"key\", cacheKey) // Info - normal operational messages log.InfoContext(ctx, \"request processed\", \"duration_ms\", duration) // Warn - warning messages log.WarnContext(ctx, \"rate limit approaching\", \"current\", current, \"limit\", limit) // Error - error messages log.ErrorContext(ctx, \"failed to connect\", \"error\", err) Minimum Log Level Filtering You can configure minimum log levels per logger name to filter out verbose logs from specific packages:\notel: log: processor: type: batch batch: export_interval: 1s max_size: 512 exporter: type: otlp otlp: type: grpc target: localhost:4317 minimum_log_level: github.com/z5labs/humus/queue/kafka: info # Filter DEBUG logs github.com/twmb/franz-go/pkg/kgo: warn # Filter DEBUG and INFO logs github.com/some/verbose-lib: error # Only ERROR logs How it works:\nLogger name matching: Uses the instrumentation scope name (package path) from humus.Logger(\"package/name\") Prefix matching: If “github.com/z5labs/humus” is configured, it matches “github.com/z5labs/humus/queue/kafka” and all subpackages Longest prefix wins: More specific configurations take precedence Fail-open: Unconfigured loggers allow all log levels Supported levels:\ndebug - All logs (most verbose) info - INFO and above warn or warning - WARN and above error - Only ERROR logs (least verbose) This is useful for reducing noise from third-party libraries or specific internal packages without affecting other loggers.\nSampling Control trace volume with sampling:\nAlways On (Development) otel: traces: sampler: type: always_on # Capture all traces Ratio-Based (Production) otel: traces: sampler: type: traceidratio arg: 0.1 # Sample 10% of traces Parent-Based (Recommended) otel: traces: sampler: type: parentbased_traceidratio arg: 0.1 # Sample 10%, but respect parent decisions This ensures distributed traces aren’t broken by different sampling decisions.\nExporters OTLP (Recommended) Send to any OTLP-compatible collector:\notel: traces: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf # or grpc Common Backends Jaeger:\notel: traces: exporter: otlp: endpoint: http://jaeger:4318 protocol: http/protobuf Grafana Tempo:\notel: traces: exporter: otlp: endpoint: https://tempo.example.com protocol: grpc headers: authorization: {{env \"GRAFANA_API_KEY\"}} Honeycomb:\notel: traces: exporter: otlp: endpoint: https://api.honeycomb.io protocol: grpc headers: x-honeycomb-team: {{env \"HONEYCOMB_API_KEY\"}} Cloud Providers:\n# AWS X-Ray (via OTLP) otel: traces: exporter: otlp: endpoint: localhost:4317 protocol: grpc # Google Cloud Trace otel: traces: exporter: otlp: endpoint: cloudtrace.googleapis.com:443 protocol: grpc Resource Attributes Add metadata to all telemetry:\notel: service: name: my-service version: 1.0.0 namespace: production resource: attributes: # Deployment info deployment.environment: production deployment.region: us-east-1 # Team info service.team: platform service.owner: team-platform@example.com # Kubernetes info (from env) k8s.pod.name: {{env \"POD_NAME\"}} k8s.namespace.name: {{env \"POD_NAMESPACE\"}} k8s.node.name: {{env \"NODE_NAME\"}} These attributes appear in all traces, metrics, and logs.\nBest Practices 1. Use Context Everywhere Always pass context.Context to propagate traces:\n// Good func processOrder(ctx context.Context, orderID string) error { result := validateOrder(ctx, orderID) // Context propagates trace return saveOrder(ctx, result) } // Bad - traces won't connect func processOrder(ctx context.Context, orderID string) error { result := validateOrder(context.Background(), orderID) // New trace! return saveOrder(ctx, result) } 2. Meaningful Span Names Use clear, hierarchical names:\n// Good tracer.Start(ctx, \"order.validate\") tracer.Start(ctx, \"payment.charge\") tracer.Start(ctx, \"inventory.reserve\") // Less useful tracer.Start(ctx, \"step1\") tracer.Start(ctx, \"process\") 3. Add Relevant Attributes Include contextual information:\nspan.SetAttributes( attribute.String(\"order.id\", orderID), attribute.String(\"user.id\", userID), attribute.Int(\"order.total_cents\", totalCents), attribute.String(\"payment.method\", \"credit_card\"), ) 4. Record Errors Always record errors in spans:\nif err := doWork(ctx); err != nil { span.RecordError(err) span.SetStatus(codes.Error, \"work failed\") return err } 5. Use Structured Logging Prefer structured fields over formatted strings:\n// Good log.InfoContext(ctx, \"order processed\", \"order_id\", orderID, \"duration_ms\", duration, ) // Less useful for querying log.InfoContext(ctx, fmt.Sprintf(\"Order %s processed in %dms\", orderID, duration)) Next Steps Learn about Lifecycle Management for service execution Explore REST Services for HTTP-specific instrumentation See gRPC Services for gRPC-specific instrumentation Read Advanced Topics for advanced OTel patterns ","categories":"","description":"OpenTelemetry integration for traces, metrics, and logs","excerpt":"OpenTelemetry integration for traces, metrics, and logs","ref":"/humus/pr-preview/pr-346/concepts/observability/","tags":"","title":"Observability"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"Understanding rest.Api","excerpt":"Understanding rest.Api","ref":"/humus/pr-preview/pr-346/features/rest/rest-api/","tags":"","title":"REST API"},{"body":"The Kafka runtime provides extensive configuration options for controlling consumer behavior, fetch settings, and topic processing.\nCreating a Runtime The basic runtime constructor:\nfunc NewRuntime( brokers []string, groupID string, opts ...Option, ) Runtime Parameters:\nbrokers - List of Kafka broker addresses (e.g., []string{\"localhost:9092\"}) groupID - Consumer group ID for offset management and rebalancing opts - Variadic configuration options Example:\nruntime := kafka.NewRuntime( []string{\"kafka1:9092\", \"kafka2:9092\", \"kafka3:9092\"}, \"my-consumer-group\", kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.SessionTimeout(10 * time.Second), ) Topic Configuration Configure which topics to consume and how to process them:\nAtLeastOnce Reliable processing with message acknowledgment after successful processing:\nfunc AtLeastOnce(topic string, processor queue.Processor[kafka.Message]) Option Example:\ntype OrderProcessor struct{} func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Process message (must be idempotent) return nil } runtime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", \u0026OrderProcessor{}), ) Guarantees:\nMessages acknowledged only after successful processing Failed processing results in redelivery Requires idempotent processors AtMostOnce Fast processing with message acknowledgment before processing:\nfunc AtMostOnce(topic string, processor queue.Processor[kafka.Message]) Option Example:\ntype MetricsProcessor struct{} func (p *MetricsProcessor) Process(ctx context.Context, msg kafka.Message) error { // Process message (may be lost on failure) return nil } runtime := kafka.NewRuntime( brokers, groupID, kafka.AtMostOnce(\"metrics\", \u0026MetricsProcessor{}), ) Guarantees:\nMessages acknowledged immediately after consumption Processing failures result in message loss Higher throughput Multiple Topics Process multiple topics with different semantics:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.AtLeastOnce(\"payments\", paymentsProcessor), kafka.AtMostOnce(\"metrics\", metricsProcessor), kafka.AtMostOnce(\"logs\", logsProcessor), ) Each topic gets its own processor and delivery semantics. See Multi-Topic Processing for details.\nConsumer Group Settings Configure consumer group behavior and rebalancing:\nSessionTimeout Maximum time between heartbeats before a consumer is considered dead:\nfunc SessionTimeout(d time.Duration) Option Default: 45 seconds\nExample:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.SessionTimeout(10 * time.Second), ) Guidelines:\nShort timeout (5-15s): Fast failure detection, but may cause false positives during GC pauses Long timeout (30-60s): Tolerates GC pauses, but slower failure detection Production recommendation: 20-30 seconds RebalanceTimeout Maximum time allowed for rebalance operations:\nfunc RebalanceTimeout(d time.Duration) Option Default: 30 seconds\nExample:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.RebalanceTimeout(60 * time.Second), ) Guidelines:\nShould be longer than session timeout Increase if rebalances frequently timeout Production recommendation: 45-60 seconds Fetch Settings Control how messages are fetched from Kafka:\nFetchMaxBytes Maximum total bytes to buffer from fetch responses across all partitions:\nfunc FetchMaxBytes(bytes int32) Option Default: 50 MB (52,428,800 bytes)\nExample:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.FetchMaxBytes(100 * 1024 * 1024), // 100 MB ) Guidelines:\nSmall messages: Lower value (10-25 MB) reduces memory usage Large messages: Higher value (100+ MB) improves throughput Must be larger than largest single message Production recommendation: 50-100 MB MaxConcurrentFetches Maximum number of concurrent fetch requests to Kafka:\nfunc MaxConcurrentFetches(fetches int) Option Default: Unlimited (0)\nExample:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.MaxConcurrentFetches(10), ) Guidelines:\nUnlimited (0): Maximum throughput, higher network load Limited (5-10): Controlled network load, predictable resource usage Production recommendation: 5-10 for most workloads Configuration Examples High-Throughput Configuration Optimize for maximum message throughput:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtMostOnce(\"events\", processor), kafka.FetchMaxBytes(200 * 1024 * 1024), // 200 MB kafka.MaxConcurrentFetches(0), // Unlimited kafka.SessionTimeout(45 * time.Second), kafka.RebalanceTimeout(60 * time.Second), ) Use cases:\nEvent streaming Log aggregation Metrics collection High-Reliability Configuration Optimize for message reliability and ordered processing:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"transactions\", processor), kafka.FetchMaxBytes(10 * 1024 * 1024), // 10 MB kafka.MaxConcurrentFetches(5), kafka.SessionTimeout(20 * time.Second), kafka.RebalanceTimeout(45 * time.Second), ) Use cases:\nFinancial transactions Database replication Critical event processing Balanced Configuration General-purpose configuration for most workloads:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.FetchMaxBytes(50 * 1024 * 1024), // 50 MB (default) kafka.MaxConcurrentFetches(10), kafka.SessionTimeout(30 * time.Second), kafka.RebalanceTimeout(45 * time.Second), ) Use cases:\nGeneral message processing Microservice communication Event-driven workflows Environment-Based Configuration Use YAML templating for environment-specific settings:\nconfig.yaml:\nkafka: brokers: - \"{{env \"KAFKA_BROKER_1\" | default \"localhost:9092\"}}\" - \"{{env \"KAFKA_BROKER_2\" | default \"localhost:9093\"}}\" group_id: \"{{env \"KAFKA_GROUP_ID\" | default \"my-service\"}}\" topic: \"{{env \"KAFKA_TOPIC\" | default \"events\"}}\" session_timeout: \"{{env \"KAFKA_SESSION_TIMEOUT\" | default \"30s\"}}\" fetch_max_bytes: {{env \"KAFKA_FETCH_MAX_BYTES\" | default \"52428800\"}} otel: service: name: \"{{env \"SERVICE_NAME\" | default \"queue-processor\"}}\" Parsing in code:\ntype Config struct { queue.Config `config:\",squash\"` Kafka struct { Brokers []string `config:\"brokers\"` GroupID string `config:\"group_id\"` Topic string `config:\"topic\"` SessionTimeout time.Duration `config:\"session_timeout\"` FetchMaxBytes int32 `config:\"fetch_max_bytes\"` } `config:\"kafka\"` } func Init(ctx context.Context, cfg Config) (*queue.App, error) { runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), kafka.SessionTimeout(cfg.Kafka.SessionTimeout), kafka.FetchMaxBytes(cfg.Kafka.FetchMaxBytes), ) return queue.NewApp(runtime), nil } Monitoring Configuration Check runtime behavior through logs and metrics:\nConsumer Group Lag:\nkafka-consumer-groups.sh \\ --bootstrap-server localhost:9092 \\ --group my-consumer-group \\ --describe Key metrics to monitor:\nConsumer lag per partition Messages processed per second Processing errors Rebalance frequency Session timeout violations See Observability for OpenTelemetry integration.\nCommon Configuration Issues Frequent Rebalances Symptoms: Consumer group frequently rebalancing\nSolutions:\n// Increase session and rebalance timeouts kafka.SessionTimeout(45 * time.Second), kafka.RebalanceTimeout(90 * time.Second), High Memory Usage Symptoms: Application consuming excessive memory\nSolutions:\n// Reduce fetch buffer size kafka.FetchMaxBytes(25 * 1024 * 1024), // 25 MB kafka.MaxConcurrentFetches(5), Slow Processing Symptoms: Consumer lag growing, messages processed slowly\nSolutions:\nCheck processor logic for inefficiencies Increase partition count for more concurrency Scale horizontally (more consumer instances) Consider at-most-once for non-critical messages Messages Larger Than FetchMaxBytes Symptoms: Errors fetching messages\nSolutions:\n// Increase fetch buffer kafka.FetchMaxBytes(200 * 1024 * 1024), // 200 MB // Or reduce message size at producer Next Steps Learn about Message Structure for working with message metadata Understand Concurrency Model for partition processing Explore Configuration for production deployment patterns ","categories":"","description":"Advanced Kafka runtime configuration","excerpt":"Advanced Kafka runtime configuration","ref":"/humus/pr-preview/pr-346/features/queue/kafka/runtime/","tags":"","title":"Runtime Configuration"},{"body":"In this guide, you’ll build a simple REST service that responds with “Hello, World!”. This will introduce you to the core concepts of Humus.\nProject Setup Create a new directory and initialize a Go module:\nmkdir hello-humus cd hello-humus go mod init hello-humus go get github.com/z5labs/humus Configuration File Create a config.yaml file in your project root:\nrest: port: 8080 otel: service: name: hello-humus sdk: disabled: true # Disable for this simple example This configuration:\nSets the HTTP server port to 8080 Names the service “hello-humus” Disables OpenTelemetry for simplicity (you’ll enable this in production) Application Code Create a main.go file:\npackage main import ( \"context\" \"net/http\" \"github.com/z5labs/humus/rest\" \"github.com/z5labs/humus/rest/rpc\" ) // Config embeds rest.Config to get HTTP server configuration type Config struct { rest.Config `config:\",squash\"` } func main() { // rest.Run handles configuration loading, app initialization, and execution rest.Run(rest.YamlSource(\"config.yaml\"), Init) } // Init is called with the loaded configuration and returns the API func Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Create a new API with name and version api := rest.NewApi(\"Hello Service\", \"1.0.0\") // Create a simple handler that returns \"Hello, World!\" handler := rpc.NewOperation( rpc.Handle(func(ctx context.Context, req any) (string, error) { return \"Hello, World!\", nil }), ) // Register the handler at GET /hello rest.Handle(http.MethodGet, rest.BasePath(\"/hello\"), handler) return api, nil } Running the Service Run your service:\ngo run main.go You should see output indicating the server has started. The service is now running on http://localhost:8080.\nTesting the Endpoint In another terminal, test your endpoint:\ncurl http://localhost:8080/hello You should see:\n\"Hello, World!\" Exploring Built-in Endpoints Humus automatically provides several endpoints:\nOpenAPI Specification curl http://localhost:8080/openapi.json This returns the OpenAPI 3.0 specification for your API, automatically generated from your code.\nHealth Checks # Liveness probe curl http://localhost:8080/health/liveness # Readiness probe curl http://localhost:8080/health/readiness Both should return 200 OK with {\"healthy\":true}.\nUnderstanding the Code Let’s break down what’s happening:\nConfiguration: The Config struct embeds rest.Config, which provides HTTP server configuration fields that are automatically populated from config.yaml.\nrest.Run(): This function orchestrates the entire application lifecycle:\nReads configuration from the YAML file Calls Init() with the parsed configuration Starts the HTTP server Handles graceful shutdown on OS signals Init Function: This is where you build your API:\nCreate an Api instance with a name and version Define handlers for your endpoints Register handlers with HTTP methods and paths Return the configured API Handler Pattern: The rpc.Handle() function creates a type-safe handler. In this example, it takes no input (using any) and returns a string.\nNext Steps Now that you have a working service, you can:\nLearn about Configuration to customize your service Explore Project Structure for organizing larger applications Read about REST Services for more advanced HTTP patterns Understand Core Concepts for a deeper dive into Humus architecture Complete Example The complete code for this example is available in the Humus repository.\n","categories":"","description":"Build a Hello World REST service with Humus","excerpt":"Build a Hello World REST service with Humus","ref":"/humus/pr-preview/pr-346/getting-started/first-service/","tags":"","title":"Your First Service"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/advanced/builder-runner-pattern/","tags":"","title":"Builder Runner Pattern"},{"body":"Humus uses YAML-based configuration with Go template support. This provides a flexible, environment-aware configuration system.\nBasic Configuration A minimal configuration file looks like this:\nrest: port: 8080 otel: service: name: my-service Configuration Structure Service Type Sections Each service type has its own configuration section:\nREST Services:\nrest: port: 8080 host: localhost # optional, defaults to all interfaces gRPC Services:\ngrpc: port: 9090 host: localhost # optional Job Services:\n# Jobs don't need server configuration # Just OTel and app-specific config OpenTelemetry Configuration The otel section configures observability:\notel: service: name: my-service version: 1.0.0 # optional sdk: disabled: false # Set to true to disable OTel entirely traces: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf metrics: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf logs: exporter: otlp: endpoint: http://localhost:4318 protocol: http/protobuf Go Template Support Configuration files support Go template syntax for dynamic values:\nEnvironment Variables Use the env function to read environment variables:\notel: service: name: {{env \"SERVICE_NAME\"}} rest: port: {{env \"PORT\"}} Default Values Use the default function to provide fallbacks:\notel: service: name: {{env \"SERVICE_NAME\" | default \"my-service\"}} rest: port: {{env \"PORT\" | default \"8080\"}} Complete Example rest: port: {{env \"HTTP_PORT\" | default \"8080\"}} host: {{env \"HTTP_HOST\" | default \"0.0.0.0\"}} otel: service: name: {{env \"OTEL_SERVICE_NAME\" | default \"my-service\"}} version: {{env \"APP_VERSION\" | default \"dev\"}} sdk: disabled: {{env \"OTEL_DISABLED\" | default \"false\"}} traces: exporter: otlp: endpoint: {{env \"OTEL_EXPORTER_OTLP_ENDPOINT\" | default \"http://localhost:4318\"}} Configuration in Code Basic Config Struct Embed the appropriate config type for your service:\ntype Config struct { rest.Config `config:\",squash\"` // For REST services // Add your custom config fields here } For gRPC:\ntype Config struct { grpc.Config `config:\",squash\"` } For Jobs:\ntype Config struct { humus.Config `config:\",squash\"` // Base OTel config only } Custom Configuration Fields Add your own configuration fields using struct tags:\ntype Config struct { rest.Config `config:\",squash\"` Database struct { Host string `config:\"host\"` Port int `config:\"port\"` Name string `config:\"name\"` } `config:\"database\"` Features struct { EnableCache bool `config:\"enable_cache\"` } `config:\"features\"` } Corresponding YAML:\nrest: port: 8080 database: host: localhost port: 5432 name: mydb features: enable_cache: true Configuration Sources YAML File The most common source:\nrest.Run(rest.YamlSource(\"config.yaml\"), Init) Multiple Sources Use bedrockcfg.MultiSource to compose configurations:\nimport ( \"github.com/z5labs/bedrock/pkg/config\" bedrockcfg \"github.com/z5labs/bedrock/pkg/config\" ) func main() { source := bedrockcfg.MultiSource( bedrockcfg.FromYaml(\"default_config.yaml\"), // Defaults bedrockcfg.FromYaml(\"config.yaml\"), // Overrides ) rest.Run(source, Init) } Environment-Specific Configs import \"os\" func main() { env := os.Getenv(\"ENV\") if env == \"\" { env = \"dev\" } configFile := fmt.Sprintf(\"config.%s.yaml\", env) rest.Run(rest.YamlSource(configFile), Init) } This allows you to have:\nconfig.dev.yaml config.staging.yaml config.prod.yaml Default Configuration Humus includes a default_config.yaml with sensible defaults for OpenTelemetry. You can compose this with your config:\nsource := bedrockcfg.MultiSource( bedrockcfg.FromYaml(\"default_config.yaml\"), // Framework defaults bedrockcfg.FromYaml(\"config.yaml\"), // Your overrides ) Best Practices Use Environment Variables for Secrets: Never commit credentials to YAML files. Use env function:\ndatabase: password: {{env \"DB_PASSWORD\"}} Provide Defaults: Always use default with env for non-secret values:\nport: {{env \"PORT\" | default \"8080\"}} Separate Environments: Use different config files or environment variables for dev/staging/prod.\nDocument Your Config: Add comments to your YAML files explaining each section.\nValidate Early: Use the Init function to validate configuration:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { if cfg.Database.Host == \"\" { return nil, fmt.Errorf(\"database host is required\") } // ... } Next Steps Learn about Project Structure for organizing your config files Explore Core Concepts for advanced configuration patterns See Observability for OTel configuration details ","categories":"","description":"Understanding the YAML configuration system","excerpt":"Understanding the YAML configuration system","ref":"/humus/pr-preview/pr-346/getting-started/configuration/","tags":"","title":"Configuration"},{"body":"Documentation coming soon.\nSee Contributing Overview for general contribution guidelines.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Contributing Overview for general …","ref":"/humus/pr-preview/pr-346/contributing/development-setup/","tags":"","title":"Development Setup"},{"body":"Documentation coming soon.\nSee GitHub Discussions for community support.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee GitHub Discussions for community …","ref":"/humus/pr-preview/pr-346/faq/faq/","tags":"","title":"Faq"},{"body":"Documentation coming soon.\nSee Getting Started for building new Humus services.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Getting Started for building new Humus …","ref":"/humus/pr-preview/pr-346/integration/from-vanilla-go/","tags":"","title":"From Vanilla Go"},{"body":"Humus Job services provide a framework for building one-off executors, batch processors, database migrations, and scheduled tasks with the same observability and lifecycle management as long-running services.\nOverview Job services in Humus are built on:\nSimple Handler Interface - Single Handle(context.Context) error method Full Observability - Same OpenTelemetry support as REST/gRPC Lifecycle Management - Graceful shutdown and context cancellation Exit Code Handling - Proper success/failure signaling Quick Start package main import ( \"context\" \"fmt\" \"github.com/z5labs/humus\" \"github.com/z5labs/humus/job\" ) type Config struct { humus.Config `config:\",squash\"` Database struct { Host string `config:\"host\"` Name string `config:\"name\"` } `config:\"database\"` } type MigrationJob struct { dbHost string dbName string } func (j *MigrationJob) Handle(ctx context.Context) error { log := humus.Logger(\"migration\") log.InfoContext(ctx, \"starting migration\", \"host\", j.dbHost, \"database\", j.dbName, ) // Run your migration logic if err := runMigrations(ctx, j.dbHost, j.dbName); err != nil { log.ErrorContext(ctx, \"migration failed\", \"error\", err) return err } log.InfoContext(ctx, \"migration completed successfully\") return nil } func main() { job.Run(job.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (job.Handler, error) { return \u0026MigrationJob{ dbHost: cfg.Database.Host, dbName: cfg.Database.Name, }, nil } func runMigrations(ctx context.Context, host, name string) error { // Your migration logic here return nil } The Handler Interface type Handler interface { Handle(context.Context) error } That’s it! Just implement one method.\nLifecycle Jobs have a simple lifecycle:\nConfiguration Loading - Config file is parsed Initialization - Init function creates the handler Execution - Handle method is called Exit: Returns nil → Exit code 0 (success) Returns error → Exit code 1 (failure) Receives SIGTERM/SIGINT → Context cancelled, job should return Use Cases Database Migrations type MigrationJob struct { db *sql.DB } func (j *MigrationJob) Handle(ctx context.Context) error { migrations := []string{ \"CREATE TABLE users...\", \"CREATE INDEX idx_users_email...\", } for i, migration := range migrations { select { case \u003c-ctx.Done(): return ctx.Err() default: log.InfoContext(ctx, \"running migration\", \"step\", i+1) if _, err := j.db.ExecContext(ctx, migration); err != nil { return fmt.Errorf(\"migration %d failed: %w\", i+1, err) } } } return nil } Batch Processing type BatchProcessor struct { source Source dest Destination } func (j *BatchProcessor) Handle(ctx context.Context) error { items, err := j.source.FetchAll(ctx) if err != nil { return err } for i, item := range items { select { case \u003c-ctx.Done(): log.InfoContext(ctx, \"shutdown requested\", \"processed\", i) return ctx.Err() default: if err := j.dest.Write(ctx, item); err != nil { return fmt.Errorf(\"failed at item %d: %w\", i, err) } } } return nil } Data Import type ImportJob struct { filePath string db *Database } func (j *ImportJob) Handle(ctx context.Context) error { file, err := os.Open(j.filePath) if err != nil { return err } defer file.Close() scanner := bufio.NewScanner(file) lineNum := 0 for scanner.Scan() { lineNum++ select { case \u003c-ctx.Done(): log.InfoContext(ctx, \"import cancelled\", \"lines_processed\", lineNum) return ctx.Err() default: if err := j.db.Insert(ctx, scanner.Text()); err != nil { return fmt.Errorf(\"line %d: %w\", lineNum, err) } } } return scanner.Err() } Scheduled Task (with external scheduler) Jobs are designed to run once. Use an external scheduler (cron, Kubernetes CronJob, etc.) to run them periodically:\n# Kubernetes CronJob example apiVersion: batch/v1 kind: CronJob metadata: name: daily-report spec: schedule: \"0 2 * * *\" # 2 AM daily jobTemplate: spec: template: spec: containers: - name: report-job image: my-report-job:latest restartPolicy: OnFailure Context Handling Always respect context cancellation:\nfunc (j *MyJob) Handle(ctx context.Context) error { for i := 0; i \u003c 1000; i++ { // Check for shutdown before each iteration select { case \u003c-ctx.Done(): log.InfoContext(ctx, \"graceful shutdown\", \"progress\", i) return ctx.Err() default: processItem(ctx, i) } } return nil } Error Handling Return errors for failures:\nfunc (j *MyJob) Handle(ctx context.Context) error { if err := validateInput(); err != nil { return fmt.Errorf(\"validation failed: %w\", err) } if err := processData(ctx); err != nil { return fmt.Errorf(\"processing failed: %w\", err) } return nil // Success } The job framework will:\nLog the error Exit with code 1 Ensure proper cleanup Configuration Minimal config for jobs:\notel: service: name: my-job sdk: disabled: false # Enable for production # Add your job-specific config database: host: localhost port: 5432 What You’ll Learn This section covers:\nQuick Start - Build your first job Job Handler - Implementing the interface Use Cases - Common patterns and examples Next Steps Start with the Quick Start Guide to build your first job service.\n","categories":"","description":"Building batch processors and one-off tasks","excerpt":"Building batch processors and one-off tasks","ref":"/humus/pr-preview/pr-346/features/job/","tags":"","title":"Job Services"},{"body":"Humus provides automatic lifecycle management for all service types, including graceful shutdown, panic recovery, and OS signal handling.\nService Lifecycle Every Humus service follows this lifecycle:\n1. Configuration Loading ↓ 2. Initialization (Init function) ↓ 3. Service Startup ↓ 4. Running (handling requests/jobs) ↓ 5. Shutdown Signal Received ↓ 6. Graceful Shutdown ↓ 7. Cleanup \u0026 Exit Lifecycle Phases 1. Configuration Loading Before your code runs, Humus loads configuration from the specified source:\nfunc main() { // Configuration is loaded here rest.Run(rest.YamlSource(\"config.yaml\"), Init) } If configuration loading fails, the service exits with an error.\n2. Initialization Your Init function is called with the loaded configuration:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { // Set up resources db, err := connectDatabase(ctx, cfg.Database) if err != nil { return nil, fmt.Errorf(\"failed to connect to database: %w\", err) } // Build the API api := rest.NewApi(\"My Service\", \"1.0.0\") // Register handlers rest.Handle(http.MethodGet, rest.BasePath(\"/users\"), userHandler) return api, nil } If Init returns an error, the service exits without starting.\nThe context passed to Init:\nContains trace context for instrumentation Is NOT cancelled when shutdown begins Should be used for initialization operations that need context 3. Service Startup After successful initialization, Humus starts the service:\nREST: HTTP server starts listening on configured port gRPC: gRPC server starts listening on configured port Job: Job handler begins execution 4. Running The service handles requests or executes jobs:\nREST/gRPC: Servers handle incoming requests Job: Handler executes once, then service waits for shutdown signal 5. Shutdown Signal Humus listens for OS signals:\nSIGINT (Ctrl+C) SIGTERM (Docker/Kubernetes termination) When received, graceful shutdown begins.\n6. Graceful Shutdown Humus gracefully shuts down the service:\nFor REST/gRPC:\nStop accepting new connections Wait for in-flight requests to complete (with timeout) Close the server For Jobs:\nCancel the job context Wait for job to return (handler should respect context cancellation) 7. Cleanup \u0026 Exit After shutdown completes, the service exits with appropriate status code.\nShutdown Handling Automatic Graceful Shutdown Shutdown is automatic - no code needed:\nfunc main() { // Graceful shutdown is built-in rest.Run(rest.YamlSource(\"config.yaml\"), Init) } When SIGTERM/SIGINT is received:\nServer stops accepting new connections Existing requests are allowed to complete Server shuts down after all requests finish (or timeout) Context Cancellation For long-running operations, respect context cancellation:\nfunc processJob(ctx context.Context) error { for { select { case \u003c-ctx.Done(): // Context was cancelled (shutdown signal received) log.Info(\"shutting down gracefully\") return ctx.Err() default: // Do work if err := processNextItem(ctx); err != nil { return err } } } } Cleanup Resources Clean up in your handlers when context is cancelled:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { db, err := connectDatabase(ctx, cfg.Database) if err != nil { return nil, err } api := rest.NewApi(\"My Service\", \"1.0.0\") handler := rpc.NewOperation( rpc.Handle(func(ctx context.Context, req Request) (Response, error) { // Handler automatically respects context cancellation return processRequest(ctx, db, req) }), ) rest.Handle(http.MethodPost, rest.BasePath(\"/process\"), handler) return api, nil } // No explicit cleanup needed - Humus handles server shutdown For resources that need explicit cleanup, use the handler’s context:\ntype JobHandler struct { db *sql.DB } func (h *JobHandler) Handle(ctx context.Context) error { // Job will be cancelled when shutdown signal received defer h.db.Close() return processWithDatabase(ctx, h.db) } Panic Recovery Humus automatically recovers from panics in handlers:\nfunc handleRequest(ctx context.Context, req Request) (Response, error) { // If this panics, Humus recovers and returns 500 panic(\"something went wrong\") } What happens:\nPanic is caught Error is logged with stack trace For REST: HTTP 500 response sent For gRPC: Internal error status returned Service continues running (not crashed) Don’t rely on panic recovery:\nUse proper error handling with error returns Panic recovery is a safety net, not a pattern Job Lifecycle Jobs have a simpler lifecycle:\ntype MyJob struct{} func (j *MyJob) Handle(ctx context.Context) error { // Job starts executing immediately log.InfoContext(ctx, \"job started\") // Do work... if err := doWork(ctx); err != nil { return err // Job fails, service exits with error } log.InfoContext(ctx, \"job completed\") return nil // Job succeeds, service exits cleanly } func main() { job.Run(job.YamlSource(\"config.yaml\"), func(ctx context.Context, cfg Config) (job.Handler, error) { return \u0026MyJob{}, nil }) } Job execution:\nJob starts immediately after initialization Context is valid until job returns OR shutdown signal received If job returns nil, service exits with code 0 If job returns error, service exits with code 1 If shutdown signal received, context is cancelled Respecting shutdown in jobs:\nfunc (j *MyJob) Handle(ctx context.Context) error { items, err := fetchItems(ctx) if err != nil { return err } for _, item := range items { // Check if shutdown was requested select { case \u003c-ctx.Done(): log.InfoContext(ctx, \"shutdown requested, stopping job\") return ctx.Err() default: // Process item if err := processItem(ctx, item); err != nil { return err } } } return nil } Timeouts Shutdown Timeout REST and gRPC servers have default shutdown timeouts. If requests don’t complete in time, the server forcefully shuts down.\nThis is managed by Bedrock and typically doesn’t need configuration.\nRequest Timeouts For long-running requests, implement your own timeouts:\nfunc handleRequest(ctx context.Context, req Request) (Response, error) { // Create timeout context ctx, cancel := context.WithTimeout(ctx, 30*time.Second) defer cancel() // This will fail if it takes \u003e 30 seconds return processRequest(ctx, req) } Job Timeouts Jobs can implement their own timeouts:\nfunc (j *MyJob) Handle(ctx context.Context) error { // Set maximum job duration ctx, cancel := context.WithTimeout(ctx, 1*time.Hour) defer cancel() return processJob(ctx) } Health During Lifecycle REST services provide health endpoints that reflect lifecycle state:\nDuring Initialization:\nLiveness: Not ready (server hasn’t started) Readiness: Not ready During Normal Operation:\nLiveness: Healthy Readiness: Healthy (unless custom health check fails) During Shutdown:\nLiveness: Healthy (but server is shutting down) Readiness: Unhealthy (stops receiving traffic) See REST Health Checks for details.\nBest Practices 1. Fast Initialization Keep Init function fast:\n// Good - quick setup func Init(ctx context.Context, cfg Config) (*rest.Api, error) { db := newDatabaseClient(cfg.Database) // Just create client api := rest.NewApi(\"My Service\", \"1.0.0\") // Register handlers... return api, nil } // Avoid - slow startup func Init(ctx context.Context, cfg Config) (*rest.Api, error) { db := newDatabaseClient(cfg.Database) if err := db.Migrate(); err != nil { // Don't run migrations here! return nil, err } // ... } Run migrations as a separate job service.\n2. Respect Context Cancellation Always check context in loops:\n// Good func processItems(ctx context.Context, items []Item) error { for _, item := range items { select { case \u003c-ctx.Done(): return ctx.Err() default: process(item) } } return nil } // Bad - ignores shutdown func processItems(ctx context.Context, items []Item) error { for _, item := range items { process(item) // Won't stop on shutdown! } return nil } 3. Proper Error Handling Return errors from Init for startup failures:\nfunc Init(ctx context.Context, cfg Config) (*rest.Api, error) { db, err := connectDatabase(ctx, cfg.Database) if err != nil { return nil, fmt.Errorf(\"database connection failed: %w\", err) } api := rest.NewApi(\"My Service\", \"1.0.0\") // ... return api, nil } This ensures the service doesn’t start in a broken state.\n4. Resource Cleanup For most resources, cleanup is automatic:\nHTTP/gRPC servers are closed by Humus Contexts are cancelled on shutdown For resources that need explicit cleanup (database connections, file handles), either:\nOption 1: Defer in handlers\nfunc (h *Handler) Handle(ctx context.Context) error { defer h.cleanup() return h.process(ctx) } Option 2: Use finalizers (advanced)\nFor advanced lifecycle hooks, use Bedrock’s lifecycle management directly. See Advanced Topics.\n5. Don’t Block Shutdown Avoid operations that might block shutdown:\n// Bad - might block shutdown indefinitely func processJob(ctx context.Context) error { for { item := blockingQueue.Get() // Blocks forever! process(item) } } // Good - respects cancellation func processJob(ctx context.Context) error { for { select { case \u003c-ctx.Done(): return ctx.Err() case item := \u003c-queue: process(item) } } } Next Steps Learn about REST Health Checks for monitoring service health Explore Advanced Topics for custom lifecycle hooks See Job Services for job-specific lifecycle patterns ","categories":"","description":"Graceful shutdown and signal handling","excerpt":"Graceful shutdown and signal handling","ref":"/humus/pr-preview/pr-346/concepts/lifecycle-management/","tags":"","title":"Lifecycle Management"},{"body":"Kafka messages in Humus are represented by the kafka.Message type, which provides access to all message data and metadata.\nMessage Type type Message struct { Key []byte Value []byte Headers []Header Timestamp time.Time Topic string Partition int32 Offset int64 Attrs uint8 } Message Fields Value The message payload as raw bytes:\ntype Message struct { Value []byte // The message content } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Deserialize JSON var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Process the order return p.processOrder(ctx, order) } Key Optional message key used for partitioning and compaction:\ntype Message struct { Key []byte // Optional partition key } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { if msg.Key != nil { customerID := string(msg.Key) // All messages for this customer go to same partition } // Process message return nil } Key purposes:\nPartition assignment (same key → same partition) Log compaction (retain latest message per key) Message ordering (within partition) Headers Key-value metadata attached to the message:\ntype Header struct { Key string Value []byte } type Message struct { Headers []Header } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Find specific header for _, header := range msg.Headers { if header.Key == \"trace-id\" { traceID := string(header.Value) // Use for distributed tracing } if header.Key == \"content-type\" { contentType := string(header.Value) // Determine deserialization format } } return nil } Common header uses:\nDistributed tracing IDs Content type/encoding Source application Schema version Correlation IDs Timestamp When the message was produced:\ntype Message struct { Timestamp time.Time } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { age := time.Since(msg.Timestamp) if age \u003e 5*time.Minute { // Handle stale message log.Warn(\"Processing stale message\", \"age\", age) } return nil } Timestamp types:\nCreateTime: When producer sent the message (default) LogAppendTime: When broker received the message (if configured) Topic The Kafka topic this message came from:\ntype Message struct { Topic string } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { switch msg.Topic { case \"orders\": return p.processOrder(ctx, msg) case \"payments\": return p.processPayment(ctx, msg) default: return fmt.Errorf(\"unknown topic: %s\", msg.Topic) } } Useful when processing multiple topics with the same processor.\nPartition The partition this message came from:\ntype Message struct { Partition int32 } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { log.Info(\"Processing message\", \"topic\", msg.Topic, \"partition\", msg.Partition, \"offset\", msg.Offset, ) return nil } Partition guarantees:\nMessages in a partition are ordered Each partition processed by one consumer at a time Same key always goes to same partition Offset The message’s position within its partition:\ntype Message struct { Offset int64 } Usage:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Save offset for manual checkpoint recovery if err := p.processMessage(ctx, msg); err != nil { return err } // Record last processed offset p.recordCheckpoint(msg.Topic, msg.Partition, msg.Offset) return nil } Offset characteristics:\nMonotonically increasing within partition Unique identifier for message position Used for offset commits (acknowledgments) Attrs Message attributes (advanced):\ntype Message struct { Attrs uint8 } Bitmap of message flags (compression, transaction markers, etc.). Rarely used directly in application code.\nDeserialization Patterns JSON Deserialization Most common pattern for JSON messages:\ntype OrderMessage struct { OrderID string `json:\"order_id\"` Amount float64 `json:\"amount\"` } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return fmt.Errorf(\"invalid JSON: %w\", err) } return p.processOrder(ctx, order) } Protobuf Deserialization For protobuf-encoded messages:\nimport \"google.golang.org/protobuf/proto\" func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order orderpb.Order if err := proto.Unmarshal(msg.Value, \u0026order); err != nil { return fmt.Errorf(\"invalid protobuf: %w\", err) } return p.processOrder(ctx, \u0026order) } Avro Deserialization With schema registry:\nimport \"github.com/linkedin/goavro/v2\" type AvroProcessor struct { codec *goavro.Codec } func (p *AvroProcessor) Process(ctx context.Context, msg kafka.Message) error { native, _, err := p.codec.NativeFromBinary(msg.Value) if err != nil { return fmt.Errorf(\"invalid avro: %w\", err) } record := native.(map[string]interface{}) return p.processRecord(ctx, record) } Content-Type Routing Use headers to determine deserialization format:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { contentType := p.getHeader(msg, \"content-type\") switch contentType { case \"application/json\": return p.processJSON(ctx, msg.Value) case \"application/protobuf\": return p.processProtobuf(ctx, msg.Value) case \"application/avro\": return p.processAvro(ctx, msg.Value) default: return fmt.Errorf(\"unsupported content-type: %s\", contentType) } } func (p *Processor) getHeader(msg kafka.Message, key string) string { for _, h := range msg.Headers { if h.Key == key { return string(h.Value) } } return \"\" } Working with Headers Reading Headers Helper function for header access:\nfunc getHeader(msg kafka.Message, key string) (string, bool) { for _, h := range msg.Headers { if h.Key == key { return string(h.Value), true } } return \"\", false } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { if traceID, ok := getHeader(msg, \"trace-id\"); ok { // Use trace ID ctx = context.WithValue(ctx, \"trace-id\", traceID) } return nil } Header-Based Filtering Skip messages based on headers:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Skip test messages if env, ok := getHeader(msg, \"environment\"); ok \u0026\u0026 env == \"test\" { return nil } // Skip old schema versions if version, ok := getHeader(msg, \"schema-version\"); ok \u0026\u0026 version != \"v2\" { log.Warn(\"Skipping old schema version\", \"version\", version) return nil } return p.processMessage(ctx, msg) } Trace Context Propagation Extract distributed tracing context from headers:\nimport ( \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/propagation\" ) func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Extract trace context from headers carrier := \u0026headerCarrier{headers: msg.Headers} ctx = otel.GetTextMapPropagator().Extract(ctx, carrier) // Now ctx contains the parent span context _, span := tracer.Start(ctx, \"process-order\") defer span.End() return p.processOrder(ctx, msg) } type headerCarrier struct { headers []kafka.Header } func (c *headerCarrier) Get(key string) string { for _, h := range c.headers { if h.Key == key { return string(h.Value) } } return \"\" } func (c *headerCarrier) Set(key, value string) { // Not needed for extraction } func (c *headerCarrier) Keys() []string { keys := make([]string, len(c.headers)) for i, h := range c.headers { keys[i] = h.Key } return keys } Note: The Kafka runtime automatically handles OTel trace propagation, so this is usually not needed.\nError Handling Validation Errors Validate messages and decide how to handle invalid data:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { // Log and skip invalid JSON log.Error(\"Invalid JSON message\", \"topic\", msg.Topic, \"partition\", msg.Partition, \"offset\", msg.Offset, \"error\", err, ) return nil // Skip message, don't retry } // Validate business rules if order.Amount \u003c 0 { log.Error(\"Invalid order amount\", \"order_id\", order.OrderID, \"amount\", order.Amount, ) return nil // Skip invalid message } return p.processOrder(ctx, order) } Dead Letter Queue Route failed messages to a DLQ:\ntype Processor struct { producer *kgo.Client // For DLQ publishing } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { // Send to DLQ return p.sendToDLQ(ctx, msg, err) } if err := p.processOrder(ctx, order); err != nil { // Send to DLQ on processing error return p.sendToDLQ(ctx, msg, err) } return nil } func (p *Processor) sendToDLQ(ctx context.Context, msg kafka.Message, err error) error { dlqRecord := \u0026kgo.Record{ Topic: msg.Topic + \".dlq\", Key: msg.Key, Value: msg.Value, Headers: append(msg.Headers, kafka.Header{ Key: \"error\", Value: []byte(err.Error()), }), } p.producer.Produce(ctx, dlqRecord, nil) return nil // Don't return error, message handled } Message Logging Log messages for debugging:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { log.Info(\"Processing message\", \"topic\", msg.Topic, \"partition\", msg.Partition, \"offset\", msg.Offset, \"key\", string(msg.Key), \"timestamp\", msg.Timestamp, \"headers\", len(msg.Headers), ) // Don't log msg.Value in production (may contain PII) // Instead log a hash or truncated version valueHash := fmt.Sprintf(\"%x\", sha256.Sum256(msg.Value)) log.Debug(\"Message content hash\", \"hash\", valueHash) return p.processMessage(ctx, msg) } Next Steps Learn about Concurrency Model for partition processing Implement Idempotency patterns Explore Observability for message tracing ","categories":"","description":"Working with Kafka messages and metadata","excerpt":"Working with Kafka messages and metadata","ref":"/humus/pr-preview/pr-346/features/queue/kafka/message/","tags":"","title":"Message Structure"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/features/grpc/quick-start/","tags":"","title":"Quick Start"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/features/job/quick-start/","tags":"","title":"Quick Start"},{"body":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","ref":"/humus/pr-preview/pr-346/reference/rest-package/","tags":"","title":"Rest Package"},{"body":"The rest/rpc package provides a type-safe abstraction for HTTP handlers with automatic OpenAPI schema generation.\nOverview Traditional HTTP handlers in Go use http.HandlerFunc:\nfunc(w http.ResponseWriter, r *http.Request) Humus provides type-safe alternatives through three specialized functions:\nProduceJson[T] - GET endpoints that return JSON (no request body) ConsumeOnlyJson[T] - POST/PUT endpoints that consume JSON (no response body) HandleJson[Req, Resp] - POST/PUT endpoints with JSON request and response This provides:\nType Safety - Compile-time type checking Automatic Serialization - JSON handled automatically OpenAPI Generation - Schemas generated from Go types Simplified Logic - Focus on business logic, not HTTP plumbing The Three Patterns ProduceJson - GET Endpoints Use ProduceJson for endpoints that return data without consuming a request body:\ntype ListUsersResponse struct { Users []User `json:\"users\"` } handler := rpc.ProducerFunc[ListUsersResponse](func(ctx context.Context) (*ListUsersResponse, error) { users, err := getUsers(ctx) if err != nil { return nil, err } return \u0026ListUsersResponse{Users: users}, nil }) rest.Handle(http.MethodGet, rest.BasePath(\"/users\"), rpc.ProduceJson(handler)) ConsumeOnlyJson - Webhook Endpoints Use ConsumeOnlyJson for endpoints that process data without returning a response body:\ntype WebhookPayload struct { Event string `json:\"event\"` Data string `json:\"data\"` } handler := rpc.ConsumerFunc[WebhookPayload](func(ctx context.Context, req *WebhookPayload) error { // Process webhook return processWebhook(ctx, req.Event, req.Data) }) rest.Handle(http.MethodPost, rest.BasePath(\"/webhooks\"), rpc.ConsumeOnlyJson(handler)) HandleJson - Full CRUD Endpoints Use HandleJson for endpoints with both request and response bodies:\ntype CreateUserRequest struct { Name string `json:\"name\"` Email string `json:\"email\"` } type User struct { ID string `json:\"id\"` Name string `json:\"name\"` Email string `json:\"email\"` } handler := rpc.HandlerFunc[CreateUserRequest, User](func(ctx context.Context, req *CreateUserRequest) (*User, error) { user := \u0026User{ ID: generateID(), Name: req.Name, Email: req.Email, } return user, nil }) rest.Handle(http.MethodPost, rest.BasePath(\"/users\"), rpc.HandleJson(handler)) Handler Interfaces Handler[Req, Resp] The main interface for handlers with both request and response:\ntype Handler[Req, Resp any] interface { Handle(context.Context, *Req) (*Resp, error) } You can implement this interface directly on your types:\ntype UserService struct { db *Database } func (s *UserService) Handle(ctx context.Context, req *CreateUserRequest) (*User, error) { // Business logic here user, err := s.db.CreateUser(ctx, req.Name, req.Email) if err != nil { return nil, err } return user, nil } // Register service := \u0026UserService{db: database} rest.Handle(http.MethodPost, rest.BasePath(\"/users\"), rpc.HandleJson(service)) Producer[T] For read-only endpoints that don’t consume a request body:\ntype Producer[T any] interface { Produce(context.Context) (*T, error) } Example implementation:\ntype ListUsersService struct { db *Database } func (s *ListUsersService) Produce(ctx context.Context) (*ListUsersResponse, error) { users, err := s.db.ListUsers(ctx) if err != nil { return nil, err } return \u0026ListUsersResponse{Users: users}, nil } // Register service := \u0026ListUsersService{db: database} rest.Handle(http.MethodGet, rest.BasePath(\"/users\"), rpc.ProduceJson(service)) Consumer[T] For write-only endpoints that don’t return a response body:\ntype Consumer[T any] interface { Consume(context.Context, *T) error } Example implementation:\ntype WebhookService struct { processor *EventProcessor } func (s *WebhookService) Consume(ctx context.Context, req *WebhookPayload) error { return s.processor.Process(ctx, req.Event, req.Data) } // Register service := \u0026WebhookService{processor: eventProcessor} rest.Handle(http.MethodPost, rest.BasePath(\"/webhooks\"), rpc.ConsumeOnlyJson(service)) Adapter Functions For simple cases, use adapter functions instead of implementing interfaces:\nHandlerFunc[Req, Resp] - Wraps a function to implement Handler[Req, Resp] ProducerFunc[T] - Wraps a function to implement Producer[T] ConsumerFunc[T] - Wraps a function to implement Consumer[T] // Handler adapter h := rpc.HandlerFunc[Request, Response](func(ctx context.Context, req *Request) (*Response, error) { return \u0026Response{}, nil }) // Producer adapter p := rpc.ProducerFunc[Response](func(ctx context.Context) (*Response, error) { return \u0026Response{}, nil }) // Consumer adapter c := rpc.ConsumerFunc[Request](func(ctx context.Context, req *Request) error { return nil }) Integration with rest.Handle All RPC handlers are registered using rest.Handle():\n// Create JWT verifier type SimpleJWTVerifier struct{} func (v *SimpleJWTVerifier) Verify(ctx context.Context, token string) (context.Context, error) { // Verify JWT and extract claims claims, err := jwt.Parse(token) if err != nil { return nil, err } return context.WithValue(ctx, \"user_id\", claims.UserID), nil } // Register operation with authentication operation := rest.Handle( http.MethodPost, rest.BasePath(\"/users\"), rpc.HandleJson(handler), rest.QueryParam(\"format\"), rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", \u0026SimpleJWTVerifier{})), ) api := rest.NewApi(\"My Service\", \"v1.0.0\", operation) The RPC handler automatically implements the rest.Handler interface, providing both HTTP handling and OpenAPI schema generation. Authentication options like JWTAuth add security schemes to the OpenAPI spec and verify credentials before the handler runs.\nRequest Types Typed Requests Define request structure with JSON tags:\ntype CreateUserRequest struct { Name string `json:\"name\"` Email string `json:\"email\"` } handler := rpc.HandlerFunc[CreateUserRequest, User](func(ctx context.Context, req *CreateUserRequest) (*User, error) { return createUser(ctx, req.Name, req.Email) }) Accessing Query Parameters Query parameters are accessed using the rest package, not through struct tags:\nhandler := rpc.ProducerFunc[ListUsersResponse](func(ctx context.Context) (*ListUsersResponse, error) { // Access query parameter limitValues := rest.QueryParamValue(ctx, \"limit\") limit, err := strconv.Atoi(limitValues[0]) if err != nil { return nil, err } return listUsers(ctx, limit) }) rest.Handle( http.MethodGet, rest.BasePath(\"/users\"), rpc.ProduceJson(handler), rest.QueryParam(\"limit\", rest.Required(), rest.Regex(regexp.MustCompile(`^\\d+$`))), ) Accessing Path Parameters Path parameters are also accessed via the rest package:\nhandler := rpc.ProducerFunc[User](func(ctx context.Context) (*User, error) { // Access path parameter id := rest.PathParamValue(ctx, \"id\") return getUserByID(ctx, id) }) rest.Handle( http.MethodGet, rest.BasePath(\"/users\").Param(\"id\"), rpc.ProduceJson(handler), ) Accessing Headers Headers are accessed using the rest package. For authentication, use JWTAuth to verify credentials and inject claims into context:\n// Define context key for type safety type contextKey string const userIDKey contextKey = \"user_id\" // JWT verifier injects user ID into context type MyVerifier struct{} func (v *MyVerifier) Verify(ctx context.Context, token string) (context.Context, error) { claims, err := jwt.Parse(token) if err != nil { return nil, err } return context.WithValue(ctx, userIDKey, claims.UserID), nil } // Handler accesses verified claims from context handler := rpc.ProducerFunc[Response](func(ctx context.Context) (*Response, error) { // Extract user ID from context (already verified by JWTAuth) userID, ok := ctx.Value(userIDKey).(string) if !ok { return nil, fmt.Errorf(\"user not authenticated\") } return processRequest(ctx, userID) }) rest.Handle( http.MethodGet, rest.BasePath(\"/data\"), rpc.ProduceJson(handler), rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", \u0026MyVerifier{})), ) This pattern separates concerns: the framework verifies authentication, and your handler focuses on business logic with verified claims.\nResponse Types Typed Responses Return any Go type with JSON tags:\ntype UserResponse struct { ID string `json:\"id\"` Name string `json:\"name\"` CreatedAt time.Time `json:\"created_at\"` } handler := rpc.ProducerFunc[UserResponse](func(ctx context.Context) (*UserResponse, error) { return \u0026UserResponse{ ID: \"user-123\", Name: \"John Doe\", CreatedAt: time.Now(), }, nil }) Collections Return slices for lists:\ntype User struct { ID string `json:\"id\"` Name string `json:\"name\"` } type ListUsersResponse []User handler := rpc.ProducerFunc[ListUsersResponse](func(ctx context.Context) (*ListUsersResponse, error) { users := \u0026ListUsersResponse{ {ID: \"1\", Name: \"Alice\"}, {ID: \"2\", Name: \"Bob\"}, } return users, nil }) Empty Response Use ConsumeOnlyJson for operations with no response body:\ntype DeleteRequest struct { ID string `json:\"id\"` } handler := rpc.ConsumerFunc[DeleteRequest](func(ctx context.Context, req *DeleteRequest) error { return deleteUser(ctx, req.ID) }) rest.Handle(http.MethodDelete, rest.BasePath(\"/users\"), rpc.ConsumeOnlyJson(handler)) This returns HTTP 200 OK with an empty response body.\nOpenAPI Schema Generation All handlers automatically generate OpenAPI schemas from Go types:\ntype CreateUserRequest struct { Name string `json:\"name\" description:\"User's full name\"` Email string `json:\"email\" format:\"email\" description:\"User's email address\"` } type User struct { ID string `json:\"id\" description:\"Unique user identifier\"` Name string `json:\"name\"` Email string `json:\"email\" format:\"email\"` } The RPC package uses reflection to generate JSON schemas that are included in the OpenAPI specification. Struct tags like description and format are automatically recognized.\nError Handling Errors returned from handlers are automatically handled by the rest package:\nhandler := rpc.HandlerFunc[Request, Response](func(ctx context.Context, req *Request) (*Response, error) { if req.Invalid { return nil, fmt.Errorf(\"invalid request\") } return \u0026Response{}, nil }) Default behavior:\nReturns HTTP 500 Body: {\"error\": \"invalid request\"} Custom Error Handling Configure custom error handlers using rest.OnError when registering the operation:\nerrorHandler := func(ctx context.Context, w http.ResponseWriter, err error) { if errors.Is(err, ErrNotFound) { w.WriteHeader(http.StatusNotFound) json.NewEncoder(w).Encode(map[string]string{\"error\": \"not found\"}) return } w.WriteHeader(http.StatusInternalServerError) json.NewEncoder(w).Encode(map[string]string{\"error\": err.Error()}) } rest.Handle( http.MethodPost, rest.BasePath(\"/users\"), rpc.HandleJson(handler), rest.OnError(errorHandler), ) Operation Options Customize operations using rest.Handle options:\n// Create JWT verifier verifier := \u0026MyJWTVerifier{} operation := rest.Handle( http.MethodPost, rest.BasePath(\"/users\"), rpc.HandleJson(handler), rest.WithOperationID(\"createUser\"), rest.WithDescription(\"Creates a new user\"), rest.WithTags(\"users\"), rest.QueryParam(\"format\"), rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ) Common options:\nrest.WithOperationID(id) - Sets OpenAPI operation ID rest.WithDescription(desc) - Adds description to OpenAPI rest.WithTags(tags...) - Organizes operations in OpenAPI rest.OnError(errorHandler) - Custom error handling rest.QueryParam(name, opts...) - Define query parameters rest.Header(name, opts...) - Define required headers rest.JWTAuth(scheme, verifier) - JWT Bearer token authentication rest.APIKey(scheme) - API key authentication rest.BasicAuth(scheme) - HTTP Basic authentication See Authentication for detailed authentication examples.\nBest Practices 1. Use Strong Types // Good type CreateUserRequest struct { Name string `json:\"name\"` Email string `json:\"email\"` } // Avoid type Request map[string]interface{} 2. Implement Interfaces on Service Structs For complex logic with dependencies, implement interfaces on structs:\ntype UserService struct { db *Database logger *slog.Logger tracer trace.Tracer } func (s *UserService) Handle(ctx context.Context, req *CreateUserRequest) (*User, error) { s.logger.InfoContext(ctx, \"creating user\", \"email\", req.Email) user, err := s.db.CreateUser(ctx, req) if err != nil { s.logger.ErrorContext(ctx, \"failed to create user\", \"error\", err) return nil, err } return user, nil } 3. Use Descriptive Names // Good type CreateUserRequest struct { ... } type UpdateUserRequest struct { ... } // Less clear type UserRequest1 struct { ... } type UserRequest2 struct { ... } 4. Document with Tags type User struct { ID string `json:\"id\" description:\"Unique user identifier\"` Name string `json:\"name\" description:\"User's full name\"` Email string `json:\"email\" format:\"email\" description:\"User's email address\"` } These tags appear in the OpenAPI schema.\n5. Choose the Right Pattern GET endpoints → Use ProduceJson with Producer[T] Webhooks/Events → Use ConsumeOnlyJson with Consumer[T] CRUD operations → Use HandleJson with Handler[Req, Resp] 6. Use Pointer Signatures All handler methods must use pointer types:\n// Correct func (h *Handler) Handle(ctx context.Context, req *Request) (*Response, error) // Incorrect - will not compile func (h *Handler) Handle(ctx context.Context, req Request) (Response, error) Next Steps Learn about Routing for path building Explore Request/Response for serialization details See Error Handling for custom errors Read OpenAPI for schema generation ","categories":"","description":"Type-safe handler abstraction","excerpt":"Type-safe handler abstraction","ref":"/humus/pr-preview/pr-346/features/rest/rpc-pattern/","tags":"","title":"RPC Pattern"},{"body":"Humus REST services provide built-in support for multiple authentication schemes with automatic OpenAPI security specification generation.\nOverview Authentication in Humus is handled through parameter validation options that you apply to headers, query parameters, or cookies. The framework:\nExtracts authentication credentials from requests Validates credentials using your custom logic Injects verified data into the request context Generates OpenAPI security schemes automatically Returns appropriate error responses (401 Unauthorized, 400 Bad Request) Parameter Validation Basics Before diving into authentication, understand parameter validation:\nRequired Parameters Mark parameters as required to ensure they’re present:\nrest.Handle( http.MethodGet, rest.BasePath(\"/protected\"), handler, rest.Header(\"Authorization\", rest.Required()), ) Missing required parameters return 400 Bad Request with a descriptive error.\nRegular Expression Validation Validate parameter format with regex:\nrest.Handle( http.MethodGet, rest.BasePath(\"/api/v1/users\"), handler, rest.QueryParam(\"api_version\", rest.Regex(regexp.MustCompile(`^v\\d+$`))), ) Invalid formats return 400 Bad Request.\nCombining Validators Chain multiple validators together:\nrest.Header( \"X-API-Key\", rest.Required(), rest.Regex(regexp.MustCompile(`^[a-f0-9]{32}$`)), rest.APIKey(\"api-key\"), ) Validators run in order. The first failure stops validation and returns an error.\nAuthentication Schemes Humus supports five authentication schemes, each adding appropriate OpenAPI security documentation.\nAPI Key Authentication API keys can be passed in headers, query parameters, or cookies:\n// Header-based API key rest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.Header(\"X-API-Key\", rest.Required(), rest.APIKey(\"api-key\")), ) // Query parameter API key rest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.QueryParam(\"key\", rest.Required(), rest.APIKey(\"api-key\")), ) // Cookie-based API key rest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.Cookie(\"api_key\", rest.Required(), rest.APIKey(\"api-key\")), ) Access the API key in your handler:\nhandler := rpc.ProducerFunc[Response](func(ctx context.Context) (*Response, error) { apiKey := rest.HeaderValue(ctx, \"X-API-Key\") // Validate API key against your database if !isValidAPIKey(apiKey[0]) { return nil, fmt.Errorf(\"invalid API key\") } return processRequest(ctx) }) Basic Authentication HTTP Basic authentication (username:password encoded in Base64):\nrest.Handle( http.MethodGet, rest.BasePath(\"/admin\"), handler, rest.Header(\"Authorization\", rest.Required(), rest.BasicAuth(\"basic\")), ) Parse Basic auth credentials in your handler:\nhandler := rpc.ProducerFunc[Response](func(ctx context.Context) (*Response, error) { authHeader := rest.HeaderValue(ctx, \"Authorization\")[0] // Parse \"Basic \u003cbase64\u003e\" format if !strings.HasPrefix(authHeader, \"Basic \") { return nil, fmt.Errorf(\"invalid authorization header\") } encoded := strings.TrimPrefix(authHeader, \"Basic \") decoded, err := base64.StdEncoding.DecodeString(encoded) if err != nil { return nil, fmt.Errorf(\"invalid base64 encoding\") } // Split username:password credentials := strings.SplitN(string(decoded), \":\", 2) if len(credentials) != 2 { return nil, fmt.Errorf(\"invalid credentials format\") } username, password := credentials[0], credentials[1] if !validateCredentials(username, password) { return nil, fmt.Errorf(\"invalid credentials\") } return processRequest(ctx) }) JWT Authentication JWT (JSON Web Token) Bearer authentication provides the most comprehensive solution with automatic token extraction and verification.\nHow It Works The framework:\nExtracts the Authorization header Validates the “Bearer ” format Strips the “Bearer \" prefix Calls your JWTVerifier.Verify() method with the clean token Returns 401 Unauthorized if verification fails Continues processing with the updated context if successful JWTVerifier Interface Implement the JWTVerifier interface to handle token verification:\ntype JWTVerifier interface { Verify(ctx context.Context, token string) (context.Context, error) } The token parameter is the JWT without the “Bearer \" prefix. Your implementation should:\nVerify the token’s signature Validate claims (expiration, issuer, audience, etc.) Extract relevant claims Inject claims into the context Return error if verification fails Complete Example with golang-jwt/jwt package main import ( \"context\" \"crypto/rsa\" \"fmt\" \"time\" \"github.com/golang-jwt/jwt/v5\" \"github.com/z5labs/humus/rest\" ) // Custom claims structure type Claims struct { UserID string `json:\"user_id\"` Email string `json:\"email\"` Roles []string `json:\"roles\"` jwt.RegisteredClaims } // JWTVerifier implementation type MyJWTVerifier struct { publicKey *rsa.PublicKey } func NewJWTVerifier(publicKey *rsa.PublicKey) *MyJWTVerifier { return \u0026MyJWTVerifier{publicKey: publicKey} } func (v *MyJWTVerifier) Verify(ctx context.Context, tokenString string) (context.Context, error) { // Parse and verify the token token, err := jwt.ParseWithClaims(tokenString, \u0026Claims{}, func(token *jwt.Token) (interface{}, error) { // Verify signing method if _, ok := token.Method.(*jwt.SigningMethodRSA); !ok { return nil, fmt.Errorf(\"unexpected signing method: %v\", token.Header[\"alg\"]) } return v.publicKey, nil }) if err != nil { return nil, fmt.Errorf(\"failed to parse token: %w\", err) } // Extract claims claims, ok := token.Claims.(*Claims) if !ok || !token.Valid { return nil, fmt.Errorf(\"invalid token claims\") } // Additional validation if claims.ExpiresAt.Before(time.Now()) { return nil, fmt.Errorf(\"token expired\") } // Inject claims into context ctx = context.WithValue(ctx, \"user_id\", claims.UserID) ctx = context.WithValue(ctx, \"email\", claims.Email) ctx = context.WithValue(ctx, \"roles\", claims.Roles) return ctx, nil } // Context keys for type-safe access type contextKey string const ( userIDKey contextKey = \"user_id\" emailKey contextKey = \"email\" rolesKey contextKey = \"roles\" ) // Helper functions to extract claims func GetUserID(ctx context.Context) (string, bool) { userID, ok := ctx.Value(userIDKey).(string) return userID, ok } func GetEmail(ctx context.Context) (string, bool) { email, ok := ctx.Value(emailKey).(string) return email, ok } func GetRoles(ctx context.Context) ([]string, bool) { roles, ok := ctx.Value(rolesKey).([]string) return roles, ok } Registering with JWT Auth func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"Secure API\", \"1.0.0\") // Load your public key (example) publicKey, err := loadPublicKey(\"public.pem\") if err != nil { return nil, err } verifier := NewJWTVerifier(publicKey) // Protected endpoint handler := rpc.ProducerFunc[UserProfile](func(ctx context.Context) (*UserProfile, error) { // Extract user info from context userID, ok := GetUserID(ctx) if !ok { return nil, fmt.Errorf(\"user ID not found in context\") } email, _ := GetEmail(ctx) roles, _ := GetRoles(ctx) return \u0026UserProfile{ ID: userID, Email: email, Roles: roles, }, nil }) rest.Handle( http.MethodGet, rest.BasePath(\"/profile\"), rpc.ProduceJson(handler), rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ) return api, nil } Testing JWT Authentication func TestJWTAuthentication(t *testing.T) { // Create test verifier privateKey, _ := rsa.GenerateKey(rand.Reader, 2048) verifier := NewJWTVerifier(\u0026privateKey.PublicKey) // Create valid token claims := \u0026Claims{ UserID: \"user-123\", Email: \"user@example.com\", Roles: []string{\"admin\"}, RegisteredClaims: jwt.RegisteredClaims{ ExpiresAt: jwt.NewNumericDate(time.Now().Add(time.Hour)), }, } token := jwt.NewWithClaims(jwt.SigningMethodRS256, claims) tokenString, _ := token.SignedString(privateKey) // Test request req := httptest.NewRequest(http.MethodGet, \"/profile\", nil) req.Header.Set(\"Authorization\", \"Bearer \"+tokenString) // Make request and verify // ... (standard HTTP testing) } Error Handling JWT authentication returns different HTTP status codes depending on the type of error:\n400 Bad Request - Token extraction failures (malformed request):\nMissing Authorization header (when JWTAuth used without Required()) Malformed header (not “Bearer ” format) Empty token after “Bearer \" prefix 401 Unauthorized - Token verification failures (authentication failed):\nInvalid JWT signature Expired token Invalid claims (issuer, audience, etc.) Any error returned by your JWTVerifier.Verify() method Example error scenarios:\n# Missing header - 400 Bad Request curl http://localhost:8080/profile # Returns: 400 Bad Request # Malformed header (missing \"Bearer\") - 400 Bad Request curl -H \"Authorization: invalid-token\" http://localhost:8080/profile # Returns: 400 Bad Request # Empty token - 400 Bad Request curl -H \"Authorization: Bearer \" http://localhost:8080/profile # Returns: 400 Bad Request # Invalid token (verification fails) - 401 Unauthorized curl -H \"Authorization: Bearer invalid.jwt.token\" http://localhost:8080/profile # Returns: 401 Unauthorized # Expired token (verification fails) - 401 Unauthorized curl -H \"Authorization: Bearer expired.jwt.token\" http://localhost:8080/profile # Returns: 401 Unauthorized Note: When combined with Required(), missing headers return 400 Bad Request from the Required() validator, which runs before JWT verification.\nOAuth 2.0 OAuth 2.0 authentication scheme:\nrest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.Header(\"Authorization\", rest.Required(), rest.OAuth2(\"oauth2\")), ) Note: OAuth 2.0 flows are not yet fully configured in the OpenAPI spec. You’ll need to implement the OAuth flow manually in your handler.\nOpenID Connect OpenID Connect authentication with discovery URL:\nrest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.Header( \"Authorization\", rest.Required(), rest.OpenIDConnect(\"oidc\", \"https://accounts.example.com/.well-known/openid-configuration\"), ), ) The discovery URL should point to your OpenID Connect provider’s configuration endpoint.\nOpenAPI Security Schemes All authentication options automatically add security schemes to your OpenAPI specification:\napi := rest.NewApi( \"Secure API\", \"1.0.0\", rest.Handle( http.MethodPost, rest.BasePath(\"/users\"), createUserHandler, rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ), ) The generated /openapi.json includes:\n{ \"components\": { \"securitySchemes\": { \"jwt\": { \"type\": \"http\", \"scheme\": \"bearer\", \"bearerFormat\": \"JWT\" } } }, \"paths\": { \"/users\": { \"post\": { \"security\": [ {\"jwt\": []} ] } } } } This integrates with Swagger UI, Postman, and other OpenAPI tools.\nSecurity Best Practices 1. Always Use HTTPS in Production Never transmit authentication credentials over unencrypted HTTP:\n// In production configuration rest.Config{ Port: 8443, TLS: \u0026rest.TLSConfig{ CertFile: \"/path/to/cert.pem\", KeyFile: \"/path/to/key.pem\", }, } 2. Validate Token Expiration Always check token expiration in your verifier:\nif claims.ExpiresAt.Before(time.Now()) { return nil, fmt.Errorf(\"token expired\") } 3. Use Strong Signing Algorithms Prefer RS256 (RSA) or ES256 (ECDSA) over HS256 (HMAC):\n// Verify signing method if _, ok := token.Method.(*jwt.SigningMethodRSA); !ok { return nil, fmt.Errorf(\"unexpected signing method\") } 4. Implement Rate Limiting Protect authentication endpoints from brute force attacks (use middleware or external service).\n5. Rotate Keys Regularly Implement key rotation for JWT signing keys and API keys.\n6. Use Context Keys with Types Avoid string collisions by using typed context keys:\ntype contextKey string const userIDKey contextKey = \"user_id\" // Set ctx = context.WithValue(ctx, userIDKey, \"user-123\") // Get with type safety userID, ok := ctx.Value(userIDKey).(string) 7. Validate All Claims Check audience, issuer, and other relevant claims:\nif claims.Issuer != \"https://auth.yourservice.com\" { return nil, fmt.Errorf(\"invalid issuer\") } if !claims.VerifyAudience(\"your-service\", true) { return nil, fmt.Errorf(\"invalid audience\") } 8. Log Authentication Failures Monitor for suspicious activity:\nfunc (v *MyJWTVerifier) Verify(ctx context.Context, token string) (context.Context, error) { // ... verification logic if err != nil { logger.WarnContext(ctx, \"JWT verification failed\", \"error\", err) return nil, err } return ctx, nil } Common Patterns Role-Based Access Control Combine JWT authentication with role checking:\nfunc requireRole(requiredRole string) rpc.OperationOption { return func(oo *rpc.OperationOptions) { oo.transforms = append(oo.transforms, func(r *http.Request) (*http.Request, error) { roles, ok := GetRoles(r.Context()) if !ok { return nil, fmt.Errorf(\"roles not found in context\") } hasRole := false for _, role := range roles { if role == requiredRole { hasRole = true break } } if !hasRole { return nil, fmt.Errorf(\"insufficient permissions\") } return r, nil }) } } // Usage rest.Handle( http.MethodDelete, rest.BasePath(\"/users\").Param(\"id\"), deleteHandler, rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), requireRole(\"admin\"), ) Optional Authentication Make authentication optional by omitting Required():\nrest.Handle( http.MethodGet, rest.BasePath(\"/public-or-private\"), handler, rest.Header(\"Authorization\", rest.JWTAuth(\"jwt\", verifier)), // No Required() ) Check in handler:\nhandler := rpc.ProducerFunc[Response](func(ctx context.Context) (*Response, error) { if userID, ok := GetUserID(ctx); ok { // Authenticated user - return personalized response return getPersonalizedResponse(ctx, userID) } // Anonymous user - return generic response return getPublicResponse(ctx) }) Multiple Authentication Schemes Support multiple authentication methods:\n// Create verifier that handles multiple schemes type MultiAuthVerifier struct { jwtVerifier *JWTVerifier apiKeyStore map[string]string } func (v *MultiAuthVerifier) Verify(ctx context.Context, token string) (context.Context, error) { // Try JWT first ctx, err := v.jwtVerifier.Verify(ctx, token) if err == nil { return ctx, nil } // Fall back to API key if userID, ok := v.apiKeyStore[token]; ok { return context.WithValue(ctx, userIDKey, userID), nil } return nil, fmt.Errorf(\"authentication failed\") } Next Steps Learn about Error Handling for custom authentication error responses Explore OpenAPI to customize security documentation Read RPC Pattern for implementing authenticated handlers See Testing for authentication test patterns ","categories":"","description":"JWT, API keys, and security","excerpt":"JWT, API keys, and security","ref":"/humus/pr-preview/pr-346/features/rest/authentication/","tags":"","title":"Authentication"},{"body":"The Kafka runtime uses a goroutine-per-partition architecture that provides automatic parallelism and partition isolation.\nArchitecture Overview Each Kafka partition is processed in its own goroutine:\nTopic \"orders\" (3 partitions): ┌─────────────┐ │ Partition 0 │──────\u003e Goroutine 1 ──\u003e Processor ├─────────────┤ │ Partition 1 │──────\u003e Goroutine 2 ──\u003e Processor ├─────────────┤ │ Partition 2 │──────\u003e Goroutine 3 ──\u003e Processor └─────────────┘ Key characteristics:\nOne goroutine per assigned partition Partitions processed independently No locks needed between partitions Scales with partition count Consumer Group Coordination The runtime coordinates with other consumers in the group:\nConsumer Group \"order-processors\" with 3 instances: Instance 1 Instance 2 Instance 3 ┌──────────────┐ ┌──────────────┐ ┌──────────────┐ │ Partition 0 │ │ Partition 1 │ │ Partition 2 │ │ Partition 3 │ │ Partition 4 │ │ Partition 5 │ └──────────────┘ └──────────────┘ └──────────────┘ Guarantees:\nEach partition assigned to exactly one consumer Automatic rebalancing when consumers join/leave Partition reassignment coordinates gracefully Partition Lifecycle Assignment When a partition is assigned to this consumer:\nReceive assignment from consumer group coordinator Spawn goroutine for the partition Start processing messages from the partition Continue until partition is revoked or app shuts down Code flow:\n// Internal runtime behavior (conceptual) func (r *Runtime) onPartitionAssigned(topic string, partition int32) { ctx, cancel := context.WithCancel(r.ctx) // Spawn goroutine for this partition go r.processPartition(ctx, topic, partition) // Store cancel function for revocation r.partitionCancels[topicPartition{topic, partition}] = cancel } Revocation When a partition is revoked (rebalance):\nReceive revocation signal from coordinator Cancel context for partition’s goroutine Wait for goroutine to finish current message Complete gracefully before rebalance proceeds Code flow:\n// Internal runtime behavior (conceptual) func (r *Runtime) onPartitionRevoked(topic string, partition int32) { // Cancel the goroutine's context cancel := r.partitionCancels[topicPartition{topic, partition}] cancel() // Wait for goroutine to finish r.wait.Wait() // Partition now free for reassignment } Concurrency Guarantees Within a Partition Messages in a partition are processed serially (one at a time):\n// For Partition 0: Message 1 ──\u003e Process ──\u003e Complete Message 2 ──\u003e Process ──\u003e Complete Message 3 ──\u003e Process ──\u003e Complete Guarantees:\nMessages processed in order No concurrent processing within partition Simple reasoning about state Across Partitions Messages in different partitions are processed concurrently:\n// Concurrent processing: Partition 0: Message 1 ──\u003e Process ──┐ Partition 1: Message 1 ──\u003e Process ──┼──\u003e All concurrent Partition 2: Message 1 ──\u003e Process ──┘ Implications:\nNo ordering guarantees across partitions Shared state needs synchronization Independent failure isolation Scaling Patterns Vertical Scaling (More Partitions) Increase partition count to enable more parallelism:\n# Create topic with 12 partitions kafka-topics.sh --create \\ --topic orders \\ --partitions 12 \\ --bootstrap-server localhost:9092 One consumer instance can process 12 partitions concurrently:\nConsumer Instance 1 (12 goroutines): Partition 0 ──\u003e Goroutine 1 Partition 1 ──\u003e Goroutine 2 ... Partition 11 ──\u003e Goroutine 12 Limits:\nMaximum parallelism = number of partitions Cannot exceed partition count with consumers Horizontal Scaling (More Consumers) Add consumer instances to distribute partitions:\n3 consumers, 12 partitions (4 partitions each): Consumer 1: Consumer 2: Consumer 3: Partition 0 Partition 4 Partition 8 Partition 1 Partition 5 Partition 9 Partition 2 Partition 6 Partition 10 Partition 3 Partition 7 Partition 11 Recommendations:\nStart with partitions = 2-4× expected consumers Allows room for scaling Example: 12 partitions supports 1-12 consumers Shared State Patterns When processors need shared state:\nThread-Safe Data Structures Use concurrent-safe types:\nimport \"sync\" type Processor struct { mu sync.RWMutex cache map[string]string } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Read from cache p.mu.RLock() value := p.cache[string(msg.Key)] p.mu.RUnlock() // Process message result := p.compute(value) // Write to cache p.mu.Lock() p.cache[string(msg.Key)] = result p.mu.Unlock() return nil } Partition-Local State Maintain separate state per partition:\ntype Processor struct { // Map from partition to its state partitionState map[int32]*PartitionState mu sync.RWMutex } type PartitionState struct { // No locks needed - only accessed by one goroutine counter int lastSeen time.Time } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Get state for this partition (no lock needed within partition) p.mu.RLock() state := p.partitionState[msg.Partition] p.mu.RUnlock() // Update state without locks state.counter++ state.lastSeen = time.Now() return nil } Atomic Operations Use atomic types for counters:\nimport \"sync/atomic\" type Processor struct { messagesProcessed atomic.Int64 } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Atomic increment - safe across goroutines p.messagesProcessed.Add(1) return nil } Rebalancing Triggering Rebalances Rebalances occur when:\nConsumer joins the group (new instance starts) Consumer leaves the group (instance stops/crashes) Topic partitions are added Session timeout expires (consumer considered dead) Rebalance Process 1. Coordinator initiates rebalance ↓ 2. All consumers stop fetching ↓ 3. Current partitions revoked ├─\u003e Cancel partition goroutines └─\u003e Wait for completion ↓ 4. New partitions assigned ├─\u003e Spawn new goroutines └─\u003e Resume processing ↓ 5. Normal processing resumes Minimizing Rebalance Impact Fast Rebalancing:\n// Use shorter timeouts for faster rebalancing runtime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.SessionTimeout(10 * time.Second), kafka.RebalanceTimeout(20 * time.Second), ) Graceful Shutdown:\n// Ensure clean shutdown to avoid forced rebalancing // The framework handles this automatically via context cancellation Performance Considerations Partition Count Too few partitions:\nLimited parallelism One slow processor blocks others Difficult to scale horizontally Too many partitions:\nHigher overhead per partition More goroutines More memory usage Recommendation:\nStart with 2-4× expected consumer count Example: 3 consumers → 12 partitions Allows scaling to 12 consumers without repartitioning Message Distribution Even distribution across partitions:\n// Producer uses key to distribute evenly key := fmt.Sprintf(\"customer-%d\", customerID % numPartitions) Avoid hot partitions:\nDon’t route all traffic to one partition Use random or round-robin for keyless messages Monitor partition lag for imbalance Processing Time Fast processors:\nCan handle more partitions per consumer Higher throughput Lower latency Slow processors:\nLimit partitions per consumer Scale horizontally Consider async I/O Monitoring Concurrency Consumer Lag per Partition kafka-consumer-groups.sh \\ --bootstrap-server localhost:9092 \\ --group my-group \\ --describe Output shows lag per partition:\nTOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG orders 0 1000 1000 0 orders 1 950 1020 70 # Lagging! orders 2 1000 1000 0 Hot partition detected: Partition 1 is lagging.\nGoroutine Count Monitor active goroutines:\nimport \"runtime\" func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Periodically log goroutine count if msg.Offset % 1000 == 0 { log.Info(\"Runtime stats\", \"goroutines\", runtime.NumGoroutine(), \"partition\", msg.Partition, ) } return nil } Error Handling Partition-Level Errors Errors in one partition don’t affect others:\nPartition 0: Processing ✓ Partition 1: Error! ← Partition 1 stops Partition 2: Processing ✓ ← Continues normally At-least-once behavior:\nFailed partition stops processing Offset not committed Messages will be redelivered after rebalance At-most-once behavior:\nFailed partition stops processing Offset already committed Messages lost Context Cancellation All partition goroutines respect context cancellation:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // Check context before expensive operation select { case \u003c-ctx.Done(): return ctx.Err() default: } // Process message return p.doWork(ctx, msg) } This ensures clean shutdown and rebalancing.\nNext Steps Learn Idempotency patterns for at-least-once processing Explore Multi-Topic Processing patterns Configure Production Settings for optimal performance ","categories":"","description":"Understanding goroutine-per-partition processing","excerpt":"Understanding goroutine-per-partition processing","ref":"/humus/pr-preview/pr-346/features/queue/kafka/concurrency/","tags":"","title":"Concurrency Model"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/advanced/custom-health-monitors/","tags":"","title":"Custom Health Monitors"},{"body":"Documentation coming soon.\nSee Getting Started for building new Humus services.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Getting Started for building new Humus …","ref":"/humus/pr-preview/pr-346/integration/from-chi/","tags":"","title":"From Chi"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/features/grpc/grpc-api/","tags":"","title":"Grpc Api"},{"body":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","ref":"/humus/pr-preview/pr-346/reference/grpc-package/","tags":"","title":"Grpc Package"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/features/job/job-handler/","tags":"","title":"Job Handler"},{"body":"This guide provides recommended patterns for organizing Humus applications.\nSimple Service For small services with a single purpose:\nmy-service/ ├── main.go # Entry point with Run() call ├── config.yaml # Configuration ├── go.mod ├── go.sum └── README.md main.go:\npackage main import ( \"context\" \"github.com/z5labs/humus/rest\" ) type Config struct { rest.Config `config:\",squash\"` } func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"My Service\", \"1.0.0\") // Register handlers... return api, nil } Organized Service For services with multiple handlers or business logic:\nmy-service/ ├── cmd/ │ └── server/ │ └── main.go # Entry point ├── internal/ │ ├── app/ │ │ └── app.go # Init function │ ├── handlers/ │ │ ├── users.go # User handlers │ │ └── posts.go # Post handlers │ └── models/ │ └── user.go # Domain models ├── config.yaml ├── go.mod └── go.sum cmd/server/main.go:\npackage main import ( \"my-service/internal/app\" \"github.com/z5labs/humus/rest\" ) func main() { rest.Run(rest.YamlSource(\"config.yaml\"), app.Init) } internal/app/app.go:\npackage app import ( \"context\" \"my-service/internal/handlers\" \"github.com/z5labs/humus/rest\" ) type Config struct { rest.Config `config:\",squash\"` // Additional config... } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"My Service\", \"1.0.0\") // Register handlers from different packages handlers.RegisterUserHandlers(api) handlers.RegisterPostHandlers(api) return api, nil } Large Application For larger applications with multiple domains:\nmy-service/ ├── cmd/ │ └── server/ │ └── main.go ├── internal/ │ ├── app/ │ │ ├── app.go │ │ └── config.go │ ├── user/ │ │ ├── handler.go # HTTP handlers │ │ ├── service.go # Business logic │ │ └── store.go # Data access │ ├── post/ │ │ ├── handler.go │ │ ├── service.go │ │ └── store.go │ └── common/ │ └── middleware.go ├── pkg/ │ └── client/ # Public client library (optional) │ └── client.go ├── configs/ │ ├── config.dev.yaml │ ├── config.staging.yaml │ └── config.prod.yaml ├── go.mod └── go.sum gRPC Service For gRPC services with Protocol Buffers:\nmy-grpc-service/ ├── cmd/ │ └── server/ │ └── main.go ├── internal/ │ ├── app/ │ │ └── app.go │ └── user/ │ └── service.go # gRPC service implementation ├── proto/ │ ├── user/ │ │ └── user.proto # Proto definitions │ └── common/ │ └── types.proto ├── gen/ # Generated code │ └── proto/ │ └── user/ │ ├── user.pb.go │ └── user_grpc.pb.go ├── Makefile # For proto generation ├── config.yaml ├── go.mod └── go.sum Example Makefile for proto generation:\n.PHONY: proto proto: protoc --go_out=gen --go_opt=paths=source_relative \\ --go-grpc_out=gen --go-grpc_opt=paths=source_relative \\ proto/**/*.proto Job Service For batch processing or one-off jobs:\nmy-job/ ├── cmd/ │ └── job/ │ └── main.go ├── internal/ │ ├── app/ │ │ └── app.go │ ├── processor/ │ │ └── processor.go # Job logic │ └── store/ │ └── database.go # Data access ├── config.yaml ├── go.mod └── go.sum internal/app/app.go:\npackage app import ( \"context\" \"my-job/internal/processor\" \"github.com/z5labs/humus/job\" ) type Config struct { humus.Config `config:\",squash\"` // Job-specific config } func Init(ctx context.Context, cfg Config) (job.Handler, error) { proc := processor.New(cfg) return proc, nil } Monorepo with Multiple Services For projects with multiple related services:\nmy-project/ ├── services/ │ ├── api/ # REST API │ │ ├── cmd/ │ │ ├── internal/ │ │ └── go.mod │ ├── worker/ # gRPC service │ │ ├── cmd/ │ │ ├── internal/ │ │ └── go.mod │ └── jobs/ # Background jobs │ ├── cmd/ │ ├── internal/ │ └── go.mod ├── pkg/ # Shared packages │ ├── models/ │ └── common/ └── proto/ # Shared proto files └── common/ Configuration Files Multiple Environments my-service/ ├── configs/ │ ├── base.yaml # Shared config │ ├── dev.yaml # Development │ ├── staging.yaml # Staging │ └── prod.yaml # Production └── cmd/server/main.go Loading environment-specific config:\nimport ( \"os\" \"github.com/z5labs/bedrock/pkg/config\" ) func main() { env := os.Getenv(\"ENV\") if env == \"\" { env = \"dev\" } source := config.MultiSource( config.FromYaml(\"configs/base.yaml\"), config.FromYaml(fmt.Sprintf(\"configs/%s.yaml\", env)), ) rest.Run(source, app.Init) } Package Organization Best Practices Use internal/ for Private Code Place code that shouldn’t be imported by other projects in internal/:\nmy-service/ ├── internal/ # Cannot be imported by external projects │ ├── app/ │ └── handlers/ └── pkg/ # Can be imported by others └── client/ Domain-Driven Structure Group by domain/feature rather than technical layer:\nGood:\ninternal/ ├── user/ │ ├── handler.go # HTTP layer │ ├── service.go # Business logic │ └── store.go # Data layer └── post/ ├── handler.go ├── service.go └── store.go Less Ideal:\ninternal/ ├── handlers/ # All HTTP handlers ├── services/ # All business logic └── stores/ # All data access Separate Main Package Keep main.go minimal - just wiring, not logic:\n// Good: main.go just calls Run func main() { rest.Run(rest.YamlSource(\"config.yaml\"), app.Init) } // Less ideal: main.go contains business logic func main() { // Lots of setup code... // Handler definitions... // Database initialization... } Testing Structure Place tests alongside the code:\ninternal/ ├── user/ │ ├── handler.go │ ├── handler_test.go │ ├── service.go │ └── service_test.go For integration tests:\nmy-service/ ├── internal/ └── test/ ├── integration/ │ └── api_test.go └── testdata/ └── fixtures.json Next Steps Now that you understand project structure:\nExplore REST Services to build HTTP APIs Learn about gRPC Services for microservices Read Job Services for batch processing Review Core Concepts for deeper understanding ","categories":"","description":"Recommended project layout patterns","excerpt":"Recommended project layout patterns","ref":"/humus/pr-preview/pr-346/getting-started/project-structure/","tags":"","title":"Project Structure"},{"body":"Humus queue services provide a complete framework for processing messages from message queues with configurable delivery semantics, automatic concurrency management, and built-in observability.\nOverview Queue services in Humus are built on:\nPluggable Runtimes - Support for different message queue systems (Kafka, and more) Delivery Semantics - Choose between at-most-once and at-least-once processing Type Safety - Compile-time type checking for message processors OpenTelemetry - Automatic tracing and metrics for message processing Quick Start package main import ( \"context\" \"encoding/json\" \"github.com/z5labs/humus/queue\" \"github.com/z5labs/humus/queue/kafka\" ) type Config struct { queue.Config `config:\",squash\"` Kafka struct { Brokers []string `config:\"brokers\"` GroupID string `config:\"group_id\"` Topic string `config:\"topic\"` } `config:\"kafka\"` } type OrderMessage struct { OrderID string `json:\"order_id\"` Amount float64 `json:\"amount\"` } type OrderProcessor struct{} func (p *OrderProcessor) Process(ctx context.Context, msg *OrderMessage) error { // Process the order // This should be idempotent for at-least-once processing return nil } func decodeOrder(data []byte) (*OrderMessage, error) { var msg OrderMessage err := json.Unmarshal(data, \u0026msg) return \u0026msg, err } func main() { queue.Run(queue.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*queue.App, error) { processor := \u0026OrderProcessor{} runtime, err := kafka.NewAtLeastOnceRuntime( cfg.Kafka.Brokers, cfg.Kafka.Topic, cfg.Kafka.GroupID, processor, decodeOrder, ) if err != nil { return nil, err } return queue.NewApp(runtime), nil } Core Concepts Three-Phase Processing Pattern Queue processing follows a three-phase pattern:\nConsumer - Retrieves messages from the queue Processor - Executes business logic on messages Acknowledger - Confirms successful processing The order of these phases determines the delivery semantics.\nDelivery Semantics At-Most-Once (Consume → Acknowledge → Process):\nMessages acknowledged before processing Fast throughput, but processing failures lose messages Suitable for non-critical data (metrics, logs, caching) At-Least-Once (Consume → Process → Acknowledge):\nMessages acknowledged after successful processing Reliable delivery, but may deliver duplicates Requires idempotent processors Suitable for critical operations (financial, database updates) Runtime Interface All queue implementations provide a Runtime that orchestrates the processing phases:\ntype Runtime interface { ProcessQueue(ctx context.Context) error } Available Runtimes Currently supported:\nKafka - Apache Kafka via franz-go client Built-in Features Every queue service automatically includes:\nGraceful Shutdown - Clean shutdown on SIGTERM/SIGINT OpenTelemetry Tracing - Automatic spans for each message Context Propagation - Distributed tracing across services Lifecycle Management - Managed by Bedrock framework What You’ll Learn This section covers:\nQueue Framework - Core abstractions and patterns Delivery Semantics - At-most-once vs at-least-once Kafka Runtime - Apache Kafka integration Next Steps Start with the Kafka Quick Start Guide to build your first queue processor.\n","categories":"","description":"Processing messages from queues with flexible delivery semantics","excerpt":"Processing messages from queues with flexible delivery semantics","ref":"/humus/pr-preview/pr-346/features/queue/","tags":"","title":"Queue Services"},{"body":"Humus REST routing provides flexible path building and comprehensive parameter validation with automatic OpenAPI documentation generation.\nPath Building Paths are constructed using rest.BasePath() and chained parameter methods.\nStatic Paths Simple static routes:\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\"), listUsersHandler, ) // Matches: GET /users Path Parameters Add dynamic segments using .Param():\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\").Param(\"id\"), getUserHandler, ) // Matches: GET /users/{id} // Examples: /users/123, /users/abc Access path parameters in your handler:\nhandler := rpc.ProducerFunc[User](func(ctx context.Context) (*User, error) { userID := rest.PathParamValue(ctx, \"id\") return getUserByID(ctx, userID) }) Nested Paths Chain multiple path segments:\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\").Param(\"id\").Path(\"posts\").Param(\"postId\"), getPostHandler, ) // Matches: GET /users/{id}/posts/{postId} // Example: /users/123/posts/456 Path Options Path parameters support validation options:\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\").Param(\"id\", rest.Regex(regexp.MustCompile(`^\\d+$`))), getUserHandler, ) // Only matches numeric IDs: /users/123 // Rejects: /users/abc (returns 400 Bad Request) Parameter Validation Parameters can be validated using various options that apply to headers, query parameters, cookies, and path parameters.\nRequired Parameters Ensure parameters are present:\nrest.Handle( http.MethodGet, rest.BasePath(\"/search\"), searchHandler, rest.QueryParam(\"q\", rest.Required()), ) Missing required parameters return 400 Bad Request:\n{ \"error\": \"missing required request parameter in query: q\" } Regular Expression Validation Validate parameter format:\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\"), handler, rest.QueryParam(\"page\", rest.Regex(regexp.MustCompile(`^\\d+$`))), rest.QueryParam(\"email\", rest.Regex(regexp.MustCompile(`^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$`))), ) Invalid formats return 400 Bad Request:\n{ \"error\": \"invalid parameter value in query: page\" } Combining Validators Chain multiple validators:\nrest.QueryParam( \"api_key\", rest.Required(), rest.Regex(regexp.MustCompile(`^[a-f0-9]{32}$`)), ) Validators run in order. The first failure stops validation and returns an error.\nQuery Parameters Query parameters are extracted from the URL query string.\nBasic Usage rest.Handle( http.MethodGet, rest.BasePath(\"/search\"), handler, rest.QueryParam(\"q\"), rest.QueryParam(\"limit\"), rest.QueryParam(\"offset\"), ) Access in handler:\nhandler := rpc.ProducerFunc[Results](func(ctx context.Context) (*Results, error) { query := rest.QueryParamValue(ctx, \"q\")[0] limit := rest.QueryParamValue(ctx, \"limit\")[0] return search(ctx, query, limit) }) Multiple Values Query parameters can have multiple values:\nrest.Handle( http.MethodGet, rest.BasePath(\"/filter\"), handler, rest.QueryParam(\"tag\"), // Allows multiple values ) Request: GET /filter?tag=go\u0026tag=rest\u0026tag=api\nhandler := rpc.ProducerFunc[Results](func(ctx context.Context) (*Results, error) { tags := rest.QueryParamValue(ctx, \"tag\") // tags = []string{\"go\", \"rest\", \"api\"} return filterByTags(ctx, tags) }) With Validation rest.Handle( http.MethodGet, rest.BasePath(\"/api/data\"), handler, rest.QueryParam(\"page\", rest.Required(), rest.Regex(regexp.MustCompile(`^\\d+$`))), rest.QueryParam(\"limit\", rest.Regex(regexp.MustCompile(`^\\d+$`))), ) Headers Headers are used for metadata, authentication, and content negotiation.\nBasic Usage rest.Handle( http.MethodGet, rest.BasePath(\"/data\"), handler, rest.Header(\"Accept-Language\"), rest.Header(\"X-Request-ID\", rest.Required()), ) Access in handler:\nhandler := rpc.ProducerFunc[Data](func(ctx context.Context) (*Data, error) { language := rest.HeaderValue(ctx, \"Accept-Language\")[0] requestID := rest.HeaderValue(ctx, \"X-Request-ID\")[0] return getData(ctx, language, requestID) }) Authentication Headers For authentication, use dedicated auth options instead of manual validation:\nrest.Handle( http.MethodPost, rest.BasePath(\"/users\"), handler, rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ) See Authentication for complete authentication examples.\nCustom Validation rest.Handle( http.MethodPost, rest.BasePath(\"/webhooks\"), handler, rest.Header(\"X-Signature\", rest.Required(), rest.Regex(regexp.MustCompile(`^sha256=[a-f0-9]{64}$`))), ) Cookies Cookies are extracted from the Cookie header.\nBasic Usage rest.Handle( http.MethodGet, rest.BasePath(\"/dashboard\"), handler, rest.Cookie(\"session\", rest.Required()), ) Access in handler:\nhandler := rpc.ProducerFunc[Dashboard](func(ctx context.Context) (*Dashboard, error) { cookies := rest.CookieValue(ctx, \"session\") sessionID := cookies[0].Value return getDashboard(ctx, sessionID) }) With Validation rest.Handle( http.MethodGet, rest.BasePath(\"/app\"), handler, rest.Cookie(\"session_id\", rest.Required(), rest.Regex(regexp.MustCompile(`^[a-f0-9]{64}$`))), ) Cookie Authentication Cookies can be used for API key authentication:\nrest.Handle( http.MethodGet, rest.BasePath(\"/api\"), handler, rest.Cookie(\"api_key\", rest.Required(), rest.APIKey(\"cookie-auth\")), ) OpenAPI Integration All parameters are automatically documented in the OpenAPI specification:\nrest.Handle( http.MethodGet, rest.BasePath(\"/users\").Param(\"id\"), handler, rest.QueryParam(\"include\", rest.Regex(regexp.MustCompile(`^(profile|posts|comments)$`))), rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)), ) Generates OpenAPI spec:\n{ \"paths\": { \"/users/{id}\": { \"get\": { \"parameters\": [ { \"name\": \"id\", \"in\": \"path\", \"required\": true }, { \"name\": \"include\", \"in\": \"query\", \"schema\": { \"type\": \"string\", \"pattern\": \"^(profile|posts|comments)$\" } }, { \"name\": \"Authorization\", \"in\": \"header\", \"required\": true } ], \"security\": [ {\"jwt\": []} ] } } } } Best Practices 1. Use Path Parameters for Resources // Good - resource identifiers in path rest.BasePath(\"/users\").Param(\"id\") // Avoid - identifiers in query string rest.BasePath(\"/users\") + rest.QueryParam(\"id\") 2. Use Query Parameters for Filtering and Options // Good - filtering/pagination via query rest.QueryParam(\"page\") rest.QueryParam(\"filter\") rest.QueryParam(\"sort\") 3. Validate Early Apply validation at the parameter level, not in business logic:\n// Good rest.QueryParam(\"limit\", rest.Required(), rest.Regex(regexp.MustCompile(`^\\d+$`))) // Avoid - validation in handler handler := func(ctx context.Context) (*Results, error) { limit := rest.QueryParamValue(ctx, \"limit\")[0] if !isNumeric(limit) { return nil, errors.New(\"invalid limit\") } // ... } 4. Use Typed Context Keys Avoid string collisions when storing values in context:\ntype contextKey string const userIDKey contextKey = \"user_id\" // Set ctx = context.WithValue(ctx, userIDKey, \"123\") // Get with type safety userID, ok := ctx.Value(userIDKey).(string) 5. Document with Examples Use descriptive parameter names and add OpenAPI descriptions:\n// Clear parameter names rest.QueryParam(\"items_per_page\") // Not just \"limit\" rest.QueryParam(\"sort_order\") // Not just \"order\" 6. Use Authentication Options For authentication, always use the built-in auth options:\n// Good rest.Header(\"Authorization\", rest.Required(), rest.JWTAuth(\"jwt\", verifier)) // Avoid - manual token parsing rest.Header(\"Authorization\", rest.Required()) // ... then parse JWT in handler See Authentication for proper authentication patterns.\nCommon Patterns Pagination rest.Handle( http.MethodGet, rest.BasePath(\"/items\"), handler, rest.QueryParam(\"page\", rest.Regex(regexp.MustCompile(`^\\d+$`))), rest.QueryParam(\"limit\", rest.Regex(regexp.MustCompile(`^\\d+$`))), ) Filtering rest.Handle( http.MethodGet, rest.BasePath(\"/products\"), handler, rest.QueryParam(\"category\"), rest.QueryParam(\"min_price\", rest.Regex(regexp.MustCompile(`^\\d+(\\.\\d{2})?$`))), rest.QueryParam(\"max_price\", rest.Regex(regexp.MustCompile(`^\\d+(\\.\\d{2})?$`))), ) Sorting rest.Handle( http.MethodGet, rest.BasePath(\"/users\"), handler, rest.QueryParam(\"sort\", rest.Regex(regexp.MustCompile(`^(name|email|created_at)$`))), rest.QueryParam(\"order\", rest.Regex(regexp.MustCompile(`^(asc|desc)$`))), ) Nested Resources // GET /organizations/{orgId}/teams/{teamId}/members rest.Handle( http.MethodGet, rest.BasePath(\"/organizations\"). Param(\"orgId\"). Path(\"teams\"). Param(\"teamId\"). Path(\"members\"), handler, ) Versioned APIs // URL versioning rest.BasePath(\"/api/v1/users\") rest.BasePath(\"/api/v2/users\") // Or header versioning rest.Handle( http.MethodGet, rest.BasePath(\"/api/users\"), handler, rest.Header(\"API-Version\", rest.Required(), rest.Regex(regexp.MustCompile(`^v\\d+$`))), ) Next Steps Learn about Authentication for securing your routes Explore RPC Pattern for type-safe handlers See OpenAPI for customizing generated documentation Read Error Handling for parameter validation errors ","categories":"","description":"Paths and parameters","excerpt":"Paths and parameters","ref":"/humus/pr-preview/pr-346/features/rest/routing/","tags":"","title":"Routing"},{"body":"Documentation coming soon.\nSee Contributing Overview for general contribution guidelines.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Contributing Overview for general …","ref":"/humus/pr-preview/pr-346/contributing/testing-guide/","tags":"","title":"Testing Guide"},{"body":"Documentation coming soon.\nSee GitHub Discussions for community support.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee GitHub Discussions for community …","ref":"/humus/pr-preview/pr-346/faq/troubleshooting/","tags":"","title":"Troubleshooting"},{"body":"Documentation coming soon.\nSee GitHub Discussions for community support.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee GitHub Discussions for community …","ref":"/humus/pr-preview/pr-346/faq/best-practices/","tags":"","title":"Best Practices"},{"body":"Documentation coming soon.\nSee Contributing Overview for general contribution guidelines.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Contributing Overview for general …","ref":"/humus/pr-preview/pr-346/contributing/documentation/","tags":"","title":"Documentation"},{"body":"Documentation coming soon.\nSee Getting Started for building new Humus services.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Getting Started for building new Humus …","ref":"/humus/pr-preview/pr-346/integration/from-grpc-go/","tags":"","title":"From Grpc Go"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/features/grpc/health-service/","tags":"","title":"Health Service"},{"body":"At-least-once processing guarantees message delivery but may deliver duplicates. Your processor must be idempotent - processing the same message multiple times produces the same result.\nWhy Idempotency Matters At-Least-Once Delivers Duplicates Common scenarios that cause duplicates:\nProcessing completes but offset commit fails\nMessage 100 → Process ✓ → Commit ✗ [Rebalance or restart] Message 100 → Process ✓ → Commit ✓ (duplicate!) Consumer crashes after processing\nMessage 100 → Process ✓ → [Crash before commit] [Restart] Message 100 → Process ✓ → Commit ✓ (duplicate!) Network partition during commit\nMessage 100 → Process ✓ → Commit [timeout] [Retry] Message 100 → Process ✓ → Commit ✓ (duplicate!) Without Idempotency Non-idempotent processors corrupt data:\n// NON-IDEMPOTENT: Incrementing a counter func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { // First processing: balance = 100 + 50 = 150 // Duplicate: balance = 150 + 50 = 200 ← Wrong! _, err := p.db.Exec( \"UPDATE accounts SET balance = balance + $1 WHERE id = $2\", msg.Amount, msg.AccountID, ) return err } Idempotency Patterns Pattern 1: Unique ID Tracking Store processed message IDs in a table:\ntype Processor struct { db *sql.DB } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Check if already processed var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM processed_orders WHERE order_id = $1)\", order.OrderID, ).Scan(\u0026exists) if err != nil { return err } if exists { // Already processed, skip return nil } // Process in transaction tx, err := p.db.BeginTx(ctx, nil) if err != nil { return err } defer tx.Rollback() // Insert order _, err = tx.ExecContext(ctx, \"INSERT INTO orders (order_id, customer_id, total) VALUES ($1, $2, $3)\", order.OrderID, order.CustomerID, order.Total, ) if err != nil { return err } // Record as processed _, err = tx.ExecContext(ctx, \"INSERT INTO processed_orders (order_id, processed_at) VALUES ($1, NOW())\", order.OrderID, ) if err != nil { return err } return tx.Commit() } Schema:\nCREATE TABLE processed_orders ( order_id VARCHAR(255) PRIMARY KEY, processed_at TIMESTAMP NOT NULL ); CREATE INDEX idx_processed_at ON processed_orders(processed_at); Cleanup old entries:\nDELETE FROM processed_orders WHERE processed_at \u003c NOW() - INTERVAL '7 days'; Pattern 2: Upsert Operations Use database upserts for natural idempotency:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Upsert: idempotent operation _, err := p.db.ExecContext(ctx, `INSERT INTO orders (order_id, customer_id, total, updated_at) VALUES ($1, $2, $3, NOW()) ON CONFLICT (order_id) DO UPDATE SET customer_id = EXCLUDED.customer_id, total = EXCLUDED.total, updated_at = NOW()`, order.OrderID, order.CustomerID, order.Total, ) return err } Result: Processing the same message multiple times produces the same database state.\nPattern 3: SET Operations Use idempotent SET operations instead of increments:\n// IDEMPOTENT: Setting absolute values func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var update AccountUpdate if err := json.Unmarshal(msg.Value, \u0026update); err != nil { return err } // Set absolute value (idempotent) _, err := p.db.ExecContext(ctx, \"UPDATE accounts SET balance = $1, updated_at = $2 WHERE id = $3\", update.NewBalance, // Absolute value update.Timestamp, // Version/timestamp update.AccountID, ) return err } Pattern 4: Unique Constraints Let the database enforce uniqueness:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var payment Payment if err := json.Unmarshal(msg.Value, \u0026payment); err != nil { return err } // Insert with unique constraint _, err := p.db.ExecContext(ctx, \"INSERT INTO payments (transaction_id, amount, status) VALUES ($1, $2, 'completed')\", payment.TransactionID, payment.Amount, ) // Handle duplicate key error if isDuplicateKeyError(err) { // Already processed, not an error return nil } return err } func isDuplicateKeyError(err error) bool { if err == nil { return false } // PostgreSQL duplicate key error code return strings.Contains(err.Error(), \"duplicate key value\") } Schema:\nCREATE TABLE payments ( transaction_id VARCHAR(255) PRIMARY KEY, amount DECIMAL(10,2) NOT NULL, status VARCHAR(50) NOT NULL, created_at TIMESTAMP DEFAULT NOW() ); Pattern 5: Offset as Idempotency Key Use Kafka’s natural ordering:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var event Event if err := json.Unmarshal(msg.Value, \u0026event); err != nil { return err } // Composite key: topic + partition + offset idempotencyKey := fmt.Sprintf(\"%s-%d-%d\", msg.Topic, msg.Partition, msg.Offset) var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM processed_events WHERE idempotency_key = $1)\", idempotencyKey, ).Scan(\u0026exists) if err != nil { return err } if exists { return nil } // Process and record tx, err := p.db.BeginTx(ctx, nil) if err != nil { return err } defer tx.Rollback() // Process event if err := p.processEvent(ctx, tx, event); err != nil { return err } // Record offset _, err = tx.ExecContext(ctx, \"INSERT INTO processed_events (idempotency_key) VALUES ($1)\", idempotencyKey, ) if err != nil { return err } return tx.Commit() } Message-Level Idempotency Producer-Assigned IDs Ensure messages have unique IDs from the producer:\n// Producer side type OrderMessage struct { OrderID string `json:\"order_id\"` // Unique ID CustomerID string `json:\"customer_id\"` Amount float64 `json:\"amount\"` } // Consumer side func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order OrderMessage if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Use order.OrderID as idempotency key return p.processOrder(ctx, order.OrderID, order) } UUID Generation Generate UUIDs at the producer:\nimport \"github.com/google/uuid\" // Producer order := OrderMessage{ OrderID: uuid.New().String(), // Globally unique CustomerID: \"cust-123\", Amount: 99.99, } Advanced Patterns Timestamp-Based Deduplication Accept only newer messages:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var update Update if err := json.Unmarshal(msg.Value, \u0026update); err != nil { return err } // Only apply if newer than current result, err := p.db.ExecContext(ctx, `UPDATE entities SET value = $1, updated_at = $2 WHERE id = $3 AND (updated_at IS NULL OR updated_at \u003c $2)`, update.Value, update.Timestamp, // Must be set by producer update.EntityID, ) if err != nil { return err } rows, _ := result.RowsAffected() if rows == 0 { // Stale update, skip (already have newer data) return nil } return nil } Event Sourcing Natural idempotency through event deduplication:\ntype EventStore interface { Append(ctx context.Context, streamID string, event Event) error } type Processor struct { eventStore EventStore } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var event Event if err := json.Unmarshal(msg.Value, \u0026event); err != nil { return err } // Event store handles deduplication by event ID return p.eventStore.Append(ctx, event.StreamID, event) } Distributed Locks Use distributed locks for complex operations:\nimport \"github.com/go-redsync/redsync/v4\" type Processor struct { rs *redsync.Redsync db *sql.DB } func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Acquire distributed lock mutex := p.rs.NewMutex( fmt.Sprintf(\"order-lock-%s\", order.OrderID), redsync.WithExpiry(10*time.Second), ) if err := mutex.LockContext(ctx); err != nil { return err } defer mutex.UnlockContext(ctx) // Check if processed var exists bool err := p.db.QueryRowContext(ctx, \"SELECT EXISTS(SELECT 1 FROM orders WHERE order_id = $1)\", order.OrderID, ).Scan(\u0026exists) if err != nil { return err } if exists { return nil } // Process order return p.processOrder(ctx, order) } Testing Idempotency Duplicate Message Test func TestProcessor_Idempotency(t *testing.T) { db := setupTestDB(t) processor := \u0026Processor{db: db} message := kafka.Message{ Value: []byte(`{\"order_id\":\"ord-123\",\"customer_id\":\"cust-456\",\"total\":99.99}`), } // Process first time err := processor.Process(context.Background(), message) require.NoError(t, err) // Verify inserted var count int db.QueryRow(\"SELECT COUNT(*) FROM orders WHERE order_id = 'ord-123'\").Scan(\u0026count) assert.Equal(t, 1, count) // Process duplicate err = processor.Process(context.Background(), message) require.NoError(t, err) // Verify still only one record db.QueryRow(\"SELECT COUNT(*) FROM orders WHERE order_id = 'ord-123'\").Scan(\u0026count) assert.Equal(t, 1, count, \"Duplicate message should not create new record\") } Concurrent Duplicate Test func TestProcessor_ConcurrentDuplicates(t *testing.T) { db := setupTestDB(t) processor := \u0026Processor{db: db} message := kafka.Message{ Value: []byte(`{\"order_id\":\"ord-123\",\"customer_id\":\"cust-456\",\"total\":99.99}`), } // Process same message concurrently var wg sync.WaitGroup errors := make([]error, 10) for i := 0; i \u003c 10; i++ { wg.Add(1) go func(idx int) { defer wg.Done() errors[idx] = processor.Process(context.Background(), message) }(i) } wg.Wait() // All should succeed (or fail with duplicate key) for _, err := range errors { if err != nil { assert.True(t, isDuplicateKeyError(err), \"Unexpected error: %v\", err) } } // Verify exactly one record var count int db.QueryRow(\"SELECT COUNT(*) FROM orders WHERE order_id = 'ord-123'\").Scan(\u0026count) assert.Equal(t, 1, count, \"Concurrent processing should create exactly one record\") } Performance Considerations Index Idempotency Keys CREATE INDEX idx_processed_orders_id ON processed_orders(order_id); CREATE INDEX idx_processed_events_key ON processed_events(idempotency_key); Cleanup Old Records Prevent unbounded growth:\n// Run cleanup periodically func (p *Processor) cleanupOldRecords(ctx context.Context) error { _, err := p.db.ExecContext(ctx, \"DELETE FROM processed_orders WHERE processed_at \u003c NOW() - INTERVAL '7 days'\", ) return err } Batch Lookups For high throughput, batch idempotency checks:\nfunc (p *Processor) ProcessBatch(ctx context.Context, messages []kafka.Message) error { // Extract all order IDs orderIDs := make([]string, len(messages)) for i, msg := range messages { var order Order json.Unmarshal(msg.Value, \u0026order) orderIDs[i] = order.OrderID } // Batch lookup rows, err := p.db.QueryContext(ctx, \"SELECT order_id FROM processed_orders WHERE order_id = ANY($1)\", pq.Array(orderIDs), ) if err != nil { return err } defer rows.Close() processed := make(map[string]bool) for rows.Next() { var id string rows.Scan(\u0026id) processed[id] = true } // Process only unprocessed messages for i, msg := range messages { if !processed[orderIDs[i]] { if err := p.processSingle(ctx, msg); err != nil { return err } } } return nil } Next Steps Explore Multi-Topic Processing patterns Learn about Observability for message tracing Configure Production Settings for deployment ","categories":"","description":"Handling duplicate messages in at-least-once processing","excerpt":"Handling duplicate messages in at-least-once processing","ref":"/humus/pr-preview/pr-346/features/queue/kafka/idempotency/","tags":"","title":"Idempotency"},{"body":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","ref":"/humus/pr-preview/pr-346/reference/job-package/","tags":"","title":"Job Package"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/advanced/otel-integration/","tags":"","title":"Otel Integration"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"Serialization patterns","excerpt":"Serialization patterns","ref":"/humus/pr-preview/pr-346/features/rest/request-response/","tags":"","title":"Request \u0026 Response"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/features/job/use-cases/","tags":"","title":"Use Cases"},{"body":"Documentation coming soon.\nSee Getting Started for building new Humus services.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee Getting Started for building new Humus …","ref":"/humus/pr-preview/pr-346/integration/bedrock-integration/","tags":"","title":"Bedrock Integration"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"Custom error responses","excerpt":"Custom error responses","ref":"/humus/pr-preview/pr-346/features/rest/error-handling/","tags":"","title":"Error Handling"},{"body":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","ref":"/humus/pr-preview/pr-346/reference/health-package/","tags":"","title":"Health Package"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/features/grpc/interceptors/","tags":"","title":"Interceptors"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/advanced/middleware/","tags":"","title":"Middleware"},{"body":"The Kafka runtime supports consuming and processing multiple topics simultaneously, each with its own processor and delivery semantics.\nBasic Multi-Topic Configuration Configure multiple topics in a single runtime:\nruntime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.AtLeastOnce(\"payments\", paymentsProcessor), kafka.AtMostOnce(\"metrics\", metricsProcessor), ) Key features:\nEach topic has its own processor Different delivery semantics per topic All topics share the same consumer group Partitions from all topics processed concurrently Processor per Topic Define separate processors for each topic:\ntype OrdersProcessor struct { db *sql.DB } func (p *OrdersProcessor) Process(ctx context.Context, msg kafka.Message) error { var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } return p.processOrder(ctx, order) } type PaymentsProcessor struct { db *sql.DB } func (p *PaymentsProcessor) Process(ctx context.Context, msg kafka.Message) error { var payment Payment if err := json.Unmarshal(msg.Value, \u0026payment); err != nil { return err } return p.processPayment(ctx, payment) } func Init(ctx context.Context, cfg Config) (*queue.App, error) { ordersProc := \u0026OrdersProcessor{db: cfg.DB} paymentsProc := \u0026PaymentsProcessor{db: cfg.DB} runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(\"orders\", ordersProc), kafka.AtLeastOnce(\"payments\", paymentsProc), ) return queue.NewApp(runtime), nil } Shared State Between Topics Processors can share state:\ntype SharedProcessor struct { mu sync.RWMutex cache map[string]string db *sql.DB } type OrdersProcessor struct { *SharedProcessor } func (p *OrdersProcessor) Process(ctx context.Context, msg kafka.Message) error { var order Order json.Unmarshal(msg.Value, \u0026order) // Access shared cache p.mu.RLock() customerData := p.cache[order.CustomerID] p.mu.RUnlock() return p.processOrder(ctx, order, customerData) } type PaymentsProcessor struct { *SharedProcessor } func (p *PaymentsProcessor) Process(ctx context.Context, msg kafka.Message) error { var payment Payment json.Unmarshal(msg.Value, \u0026payment) // Update shared cache p.mu.Lock() p.cache[payment.CustomerID] = payment.Status p.mu.Unlock() return p.processPayment(ctx, payment) } func Init(ctx context.Context, cfg Config) (*queue.App, error) { shared := \u0026SharedProcessor{ cache: make(map[string]string), db: cfg.DB, } runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(\"orders\", \u0026OrdersProcessor{shared}), kafka.AtLeastOnce(\"payments\", \u0026PaymentsProcessor{shared}), ) return queue.NewApp(runtime), nil } Topic-Based Routing Route messages by topic to a unified handler:\ntype UnifiedProcessor struct { db *sql.DB } func (p *UnifiedProcessor) Process(ctx context.Context, msg kafka.Message) error { switch msg.Topic { case \"orders\": return p.processOrder(ctx, msg) case \"payments\": return p.processPayment(ctx, msg) case \"shipments\": return p.processShipment(ctx, msg) default: return fmt.Errorf(\"unknown topic: %s\", msg.Topic) } } func (p *UnifiedProcessor) processOrder(ctx context.Context, msg kafka.Message) error { var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { return err } // Process order return nil } func (p *UnifiedProcessor) processPayment(ctx context.Context, msg kafka.Message) error { var payment Payment if err := json.Unmarshal(msg.Value, \u0026payment); err != nil { return err } // Process payment return nil } func Init(ctx context.Context, cfg Config) (*queue.App, error) { processor := \u0026UnifiedProcessor{db: cfg.DB} runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(\"orders\", processor), kafka.AtLeastOnce(\"payments\", processor), kafka.AtLeastOnce(\"shipments\", processor), ) return queue.NewApp(runtime), nil } Mixed Delivery Semantics Use different semantics for different topics:\nruntime := kafka.NewRuntime( brokers, groupID, // Critical topics: at-least-once kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.AtLeastOnce(\"payments\", paymentsProcessor), kafka.AtLeastOnce(\"inventory\", inventoryProcessor), // Non-critical topics: at-most-once kafka.AtMostOnce(\"metrics\", metricsProcessor), kafka.AtMostOnce(\"logs\", logsProcessor), kafka.AtMostOnce(\"analytics\", analyticsProcessor), ) Rationale:\nOrders, payments, inventory: Cannot lose data → at-least-once Metrics, logs, analytics: Can tolerate loss → at-most-once Configuration YAML Configuration kafka: brokers: - \"localhost:9092\" group_id: \"multi-topic-processor\" topics: orders: semantic: \"at-least-once\" payments: semantic: \"at-least-once\" metrics: semantic: \"at-most-once\" Dynamic Topic Configuration type TopicConfig struct { Name string Semantic string } type Config struct { queue.Config `config:\",squash\"` Kafka struct { Brokers []string `config:\"brokers\"` GroupID string `config:\"group_id\"` Topics []TopicConfig `config:\"topics\"` } `config:\"kafka\"` } func Init(ctx context.Context, cfg Config) (*queue.App, error) { opts := make([]kafka.Option, 0, len(cfg.Kafka.Topics)) for _, topic := range cfg.Kafka.Topics { switch topic.Semantic { case \"at-least-once\": processor := newProcessor(topic.Name) opts = append(opts, kafka.AtLeastOnce(topic.Name, processor)) case \"at-most-once\": processor := newProcessor(topic.Name) opts = append(opts, kafka.AtMostOnce(topic.Name, processor)) } } runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, opts..., ) return queue.NewApp(runtime), nil } Workflow Patterns Sequential Processing Process related messages across topics:\ntype WorkflowProcessor struct { db *sql.DB } // Orders topic func (p *WorkflowProcessor) ProcessOrder(ctx context.Context, msg kafka.Message) error { var order Order json.Unmarshal(msg.Value, \u0026order) // 1. Save order if err := p.saveOrder(ctx, order); err != nil { return err } // 2. Check payment (from payments topic) // This will be processed by ProcessPayment when payment arrives return nil } // Payments topic func (p *WorkflowProcessor) ProcessPayment(ctx context.Context, msg kafka.Message) error { var payment Payment json.Unmarshal(msg.Value, \u0026payment) // 1. Save payment if err := p.savePayment(ctx, payment); err != nil { return err } // 2. Update order status return p.updateOrderStatus(ctx, payment.OrderID, \"paid\") } Event Aggregation Aggregate events from multiple topics:\ntype AggregationProcessor struct { mu sync.RWMutex aggregates map[string]*Aggregate } type Aggregate struct { OrderReceived bool PaymentReceived bool ShipmentReceived bool } func (p *AggregationProcessor) ProcessOrder(ctx context.Context, msg kafka.Message) error { var order Order json.Unmarshal(msg.Value, \u0026order) p.mu.Lock() defer p.mu.Unlock() agg := p.getAggregate(order.OrderID) agg.OrderReceived = true if p.isComplete(agg) { return p.finalizeOrder(ctx, order.OrderID) } return nil } func (p *AggregationProcessor) ProcessPayment(ctx context.Context, msg kafka.Message) error { var payment Payment json.Unmarshal(msg.Value, \u0026payment) p.mu.Lock() defer p.mu.Unlock() agg := p.getAggregate(payment.OrderID) agg.PaymentReceived = true if p.isComplete(agg) { return p.finalizeOrder(ctx, payment.OrderID) } return nil } func (p *AggregationProcessor) isComplete(agg *Aggregate) bool { return agg.OrderReceived \u0026\u0026 agg.PaymentReceived \u0026\u0026 agg.ShipmentReceived } Topic Fan-In Multiple topics feed into one aggregator:\ntype EventAggregator struct { db *sql.DB } func (p *EventAggregator) Process(ctx context.Context, msg kafka.Message) error { // Common event envelope type Event struct { Type string `json:\"type\"` Timestamp time.Time `json:\"timestamp\"` Data json.RawMessage `json:\"data\"` } var event Event if err := json.Unmarshal(msg.Value, \u0026event); err != nil { return err } // Store in unified events table _, err := p.db.ExecContext(ctx, `INSERT INTO events (topic, type, timestamp, data) VALUES ($1, $2, $3, $4)`, msg.Topic, event.Type, event.Timestamp, event.Data, ) return err } func Init(ctx context.Context, cfg Config) (*queue.App, error) { aggregator := \u0026EventAggregator{db: cfg.DB} runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, // All topics use the same aggregator kafka.AtLeastOnce(\"user-events\", aggregator), kafka.AtLeastOnce(\"order-events\", aggregator), kafka.AtLeastOnce(\"payment-events\", aggregator), ) return queue.NewApp(runtime), nil } Partition Distribution How Partitions Are Assigned With multiple topics, partitions from all topics are distributed:\nConsumer Group \"processors\" with 2 consumers: Consumer 1: Consumer 2: orders (partition 0) orders (partition 1) orders (partition 2) payments (partition 0) payments (partition 1) payments (partition 2) Key points:\nPartitions assigned across all topics Load balanced across consumers No guarantee which consumer gets which topic Topic Affinity To ensure a consumer processes specific topics, use separate consumer groups:\n// Separate runtimes for different topics func Init(ctx context.Context, cfg Config) ([]*queue.App, error) { // Runtime 1: Critical topics runtime1 := kafka.NewRuntime( cfg.Kafka.Brokers, \"critical-processor\", // Different group ID kafka.AtLeastOnce(\"orders\", ordersProcessor), kafka.AtLeastOnce(\"payments\", paymentsProcessor), ) // Runtime 2: Analytics topics runtime2 := kafka.NewRuntime( cfg.Kafka.Brokers, \"analytics-processor\", // Different group ID kafka.AtMostOnce(\"metrics\", metricsProcessor), kafka.AtMostOnce(\"logs\", logsProcessor), ) return []*queue.App{ queue.NewApp(runtime1), queue.NewApp(runtime2), }, nil } Note: This requires running multiple apps, which is outside the standard pattern. Consider separate deployments instead.\nMonitoring Multi-Topic Processing Topic-Level Metrics Monitor lag per topic:\nkafka-consumer-groups.sh \\ --bootstrap-server localhost:9092 \\ --group multi-topic-processor \\ --describe Output:\nTOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG orders 0 1000 1000 0 orders 1 1050 1050 0 payments 0 500 520 20 # Lagging payments 1 510 510 0 metrics 0 5000 5000 0 Processor-Level Logging Log which processor handles each message:\nfunc (p *OrdersProcessor) Process(ctx context.Context, msg kafka.Message) error { log.Info(\"Processing order\", \"topic\", msg.Topic, \"partition\", msg.Partition, \"offset\", msg.Offset, \"processor\", \"orders\", ) return nil } Best Practices Separate Concerns Use different processors for different business domains:\n// Good: Separate processors kafka.AtLeastOnce(\"orders\", ordersProcessor) kafka.AtLeastOnce(\"payments\", paymentsProcessor) // Avoid: One processor for everything kafka.AtLeastOnce(\"orders\", genericProcessor) kafka.AtLeastOnce(\"payments\", genericProcessor) Match Semantics to Criticality // Critical: At-least-once kafka.AtLeastOnce(\"financial-transactions\", processor) // Non-critical: At-most-once kafka.AtMostOnce(\"usage-metrics\", processor) Shared State Needs Locks type Processor struct { mu sync.RWMutex state map[string]int } // Safe: Uses locks func (p *Processor) Process(ctx context.Context, msg kafka.Message) error { p.mu.Lock() p.state[string(msg.Key)]++ p.mu.Unlock() return nil } Independent Topic Processing Avoid dependencies between topics when possible:\n// Good: Independent processing func (p *OrdersProcessor) Process(ctx context.Context, msg kafka.Message) error { // Only processes orders, no dependencies return p.saveOrder(ctx, msg) } // Avoid: Cross-topic dependencies func (p *OrdersProcessor) Process(ctx context.Context, msg kafka.Message) error { // Waiting for payment from another topic creates coupling payment := p.waitForPayment(msg.OrderID) // Anti-pattern return p.saveOrder(ctx, msg, payment) } Next Steps Learn about Observability for multi-topic tracing Configure Production Settings for optimal performance Review Idempotency patterns for each topic ","categories":"","description":"Processing multiple Kafka topics in one runtime","excerpt":"Processing multiple Kafka topics in one runtime","ref":"/humus/pr-preview/pr-346/features/queue/kafka/multi-topic/","tags":"","title":"Multi-Topic Processing"},{"body":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for complete API reference.\n","ref":"/humus/pr-preview/pr-346/reference/config-package/","tags":"","title":"Config Package"},{"body":"The Kafka runtime provides comprehensive OpenTelemetry integration with automatic tracing, metrics, and structured logging for message processing.\nOverview Observability is built-in at every level:\nAutomatic Tracing - Spans created for each message via franz-go kotel plugin Context Propagation - Distributed tracing across services Structured Logging - Message metadata in log entries Metrics - Consumer lag, processing rates, errors (via OTel SDK) No manual instrumentation needed in your processor code.\nTracing Automatic Span Creation Every message gets a processing span automatically:\nSpan: kafka.process ├─ topic: \"orders\" ├─ partition: 0 ├─ offset: 12345 ├─ group_id: \"order-processors\" └─ duration: 45ms Trace Propagation Trace context is automatically extracted from Kafka headers:\nProducer (orders-api): HTTP Request → Span A (trace-id: abc123) └─\u003e Publish to Kafka (inject trace-id into headers) Consumer (order-processor): Consume from Kafka → Extract trace-id from headers └─\u003e Span B (trace-id: abc123, parent: Span A) This creates a distributed trace across services.\nCustom Spans in Processor Add child spans for detailed tracing:\nimport ( \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/trace\" ) var tracer = otel.Tracer(\"order-processor\") type OrderProcessor struct { db *sql.DB } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Parent span already created by kafka runtime // Add custom child span ctx, span := tracer.Start(ctx, \"deserialize-order\") var order Order err := json.Unmarshal(msg.Value, \u0026order) span.End() if err != nil { return err } // Another span for database operation ctx, span = tracer.Start(ctx, \"save-order\") defer span.End() _, err = p.db.ExecContext(ctx, \"INSERT INTO orders (order_id, total) VALUES ($1, $2)\", order.OrderID, order.Total, ) if err != nil { span.RecordError(err) } return err } Span Attributes Add custom attributes to spans:\nimport \"go.opentelemetry.io/otel/attribute\" func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { ctx, span := tracer.Start(ctx, \"process-order\") defer span.End() var order Order json.Unmarshal(msg.Value, \u0026order) // Add business context to span span.SetAttributes( attribute.String(\"order.id\", order.OrderID), attribute.String(\"customer.id\", order.CustomerID), attribute.Float64(\"order.total\", order.Total), attribute.String(\"order.status\", order.Status), ) return p.processOrder(ctx, order) } Logging Structured Logging with slog Use the built-in logger with Kafka attributes:\nimport ( \"log/slog\" \"github.com/z5labs/humus/queue/kafka\" ) func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order Order json.Unmarshal(msg.Value, \u0026order) log.InfoContext(ctx, \"Processing order\", kafka.TopicAttr(msg.Topic), kafka.PartitionAttr(msg.Partition), kafka.OffsetAttr(msg.Offset), slog.String(\"order_id\", order.OrderID), slog.Float64(\"amount\", order.Total), ) if err := p.saveOrder(ctx, order); err != nil { log.ErrorContext(ctx, \"Failed to save order\", kafka.TopicAttr(msg.Topic), kafka.PartitionAttr(msg.Partition), kafka.OffsetAttr(msg.Offset), slog.String(\"order_id\", order.OrderID), slog.Any(\"error\", err), ) return err } return nil } Available Kafka Attributes The kafka package provides slog attributes:\n// Kafka-specific attributes kafka.GroupIDAttr(groupID string) // Consumer group ID kafka.TopicAttr(topic string) // Topic name kafka.PartitionAttr(partition int32) // Partition number kafka.OffsetAttr(offset int64) // Message offset Example:\nlog.Info(\"Consumer group started\", kafka.GroupIDAttr(\"order-processors\"), kafka.TopicAttr(\"orders\"), ) Log Correlation with Traces Logs are automatically correlated with traces when using log/slog with context:\nfunc (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Log with context - automatically includes trace ID log.InfoContext(ctx, \"Processing message\", kafka.TopicAttr(msg.Topic), kafka.OffsetAttr(msg.Offset), ) // This log will have the same trace ID in your logging backend return nil } Metrics Automatic Metrics The Kafka runtime automatically exports metrics:\nConsumer Metrics:\nConsumer lag per partition Messages consumed per second Bytes consumed per second Fetch latency Commit latency Processing Metrics:\nMessages processed per second Processing errors per second Processing duration histogram Custom Metrics Add application-specific metrics:\nimport ( \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/metric\" ) type OrderProcessor struct { db *sql.DB ordersProcessed metric.Int64Counter orderValue metric.Float64Histogram } func NewOrderProcessor(db *sql.DB) (*OrderProcessor, error) { meter := otel.Meter(\"order-processor\") ordersProcessed, err := meter.Int64Counter( \"orders.processed\", metric.WithDescription(\"Number of orders processed\"), ) if err != nil { return nil, err } orderValue, err := meter.Float64Histogram( \"orders.value\", metric.WithDescription(\"Order total value\"), metric.WithUnit(\"USD\"), ) if err != nil { return nil, err } return \u0026OrderProcessor{ db: db, ordersProcessed: ordersProcessed, orderValue: orderValue, }, nil } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { var order Order json.Unmarshal(msg.Value, \u0026order) // Process order if err := p.saveOrder(ctx, order); err != nil { return err } // Record metrics p.ordersProcessed.Add(ctx, 1, metric.WithAttributes( attribute.String(\"status\", order.Status), ), ) p.orderValue.Record(ctx, order.Total, metric.WithAttributes( attribute.String(\"customer.id\", order.CustomerID), ), ) return nil } Configuration Enable OpenTelemetry Configure OTel in your config.yaml:\notel: service: name: \"order-processor\" version: \"1.0.0\" sdk: disabled: false # Enable OTel exporter: otlp: endpoint: \"localhost:4317\" protocol: grpc insecure: true traces: sampler: type: \"always_on\" # Or \"traceidratio\" for sampling metrics: interval: 60s # Export interval OTLP Exporter Export to an OTLP collector (Jaeger, Tempo, etc.):\notel: exporter: otlp: endpoint: \"otel-collector:4317\" protocol: grpc headers: - key: \"authorization\" value: \"Bearer {{env \\\"OTEL_TOKEN\\\"}}\" Sampling Configure trace sampling for high-throughput scenarios:\notel: traces: sampler: type: \"traceidratio\" arg: 0.1 # Sample 10% of traces Visualization Jaeger UI View distributed traces in Jaeger:\nTrace: Process Order (trace-id: abc123) │ ├─ HTTP POST /orders [orders-api] 250ms │ └─ kafka.publish [orders-api] 5ms │ └─ kafka.process [order-processor] 45ms ├─ deserialize-order 2ms ├─ save-order 40ms │ └─ sql.insert 38ms └─ publish-event 3ms Grafana Dashboard Monitor Kafka consumer metrics:\nKey Dashboard Panels:\nConsumer Lag\nkafka_consumer_lag{group=\"order-processors\",topic=\"orders\"} Messages Processed per Second\nrate(kafka_messages_processed_total{topic=\"orders\"}[1m]) Processing Duration\nhistogram_quantile(0.99, rate(kafka_processing_duration_seconds_bucket[5m]) ) Error Rate\nrate(kafka_processing_errors_total[1m]) Debugging Find Slow Messages Use trace queries to find slow processing:\nJaeger Query:\nservice=\"order-processor\" minDuration=1s This finds all messages that took over 1 second to process.\nIdentify Error Patterns Find errors in logs correlated with traces:\nLog Query (Loki):\n{service=\"order-processor\"} |= \"error\" | json | trace_id=\"abc123\" Monitor Partition Lag Check lag per partition:\nkafka-consumer-groups.sh \\ --bootstrap-server localhost:9092 \\ --group order-processors \\ --describe Correlate with processing traces to find bottlenecks.\nBest Practices Always Use Context Pass context through your call chain for trace propagation:\nfunc (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Good: Pass context return p.saveOrder(ctx, msg) } func (p *OrderProcessor) saveOrder(ctx context.Context, msg kafka.Message) error { // Good: Use context in DB calls _, err := p.db.ExecContext(ctx, ...) return err } Add Business Context Include business IDs in spans and logs:\nspan.SetAttributes( attribute.String(\"order.id\", order.OrderID), attribute.String(\"customer.id\", order.CustomerID), ) log.InfoContext(ctx, \"Order processed\", slog.String(\"order_id\", order.OrderID), slog.String(\"customer_id\", order.CustomerID), ) Sample High-Volume Topics For high-throughput topics, use sampling:\notel: traces: sampler: type: \"traceidratio\" arg: 0.01 # 1% sampling for high volume Monitor Consumer Lag Set up alerts for increasing lag:\n# Prometheus alert - alert: KafkaConsumerLag expr: kafka_consumer_lag{group=\"order-processors\"} \u003e 1000 for: 5m annotations: summary: \"Consumer group {{ $labels.group }} is lagging\" Use Structured Logging Always use structured logs (slog), not formatted strings:\n// Good: Structured log.InfoContext(ctx, \"Order processed\", slog.String(\"order_id\", order.OrderID), slog.Int64(\"partition\", msg.Partition), ) // Bad: Unstructured log.Printf(\"Order %s processed on partition %d\", order.OrderID, msg.Partition) Example: Complete Observability package main import ( \"context\" \"encoding/json\" \"log/slog\" \"github.com/z5labs/humus/queue\" \"github.com/z5labs/humus/queue/kafka\" \"go.opentelemetry.io/otel\" \"go.opentelemetry.io/otel/attribute\" \"go.opentelemetry.io/otel/metric\" ) var tracer = otel.Tracer(\"order-processor\") type OrderProcessor struct { db *sql.DB ordersProcessed metric.Int64Counter } func NewOrderProcessor(db *sql.DB) (*OrderProcessor, error) { meter := otel.Meter(\"order-processor\") counter, err := meter.Int64Counter(\"orders.processed\") if err != nil { return nil, err } return \u0026OrderProcessor{ db: db, ordersProcessed: counter, }, nil } func (p *OrderProcessor) Process(ctx context.Context, msg kafka.Message) error { // Create custom span ctx, span := tracer.Start(ctx, \"process-order\") defer span.End() // Deserialize var order Order if err := json.Unmarshal(msg.Value, \u0026order); err != nil { slog.ErrorContext(ctx, \"Failed to deserialize\", kafka.TopicAttr(msg.Topic), kafka.OffsetAttr(msg.Offset), slog.Any(\"error\", err), ) span.RecordError(err) return err } // Add attributes span.SetAttributes( attribute.String(\"order.id\", order.OrderID), attribute.Float64(\"order.total\", order.Total), ) // Log processing slog.InfoContext(ctx, \"Processing order\", kafka.TopicAttr(msg.Topic), kafka.PartitionAttr(msg.Partition), kafka.OffsetAttr(msg.Offset), slog.String(\"order_id\", order.OrderID), ) // Save to database if err := p.saveOrder(ctx, order); err != nil { slog.ErrorContext(ctx, \"Failed to save order\", kafka.TopicAttr(msg.Topic), kafka.OffsetAttr(msg.Offset), slog.String(\"order_id\", order.OrderID), slog.Any(\"error\", err), ) span.RecordError(err) return err } // Record metrics p.ordersProcessed.Add(ctx, 1, metric.WithAttributes( attribute.String(\"status\", order.Status), ), ) slog.InfoContext(ctx, \"Order processed successfully\", slog.String(\"order_id\", order.OrderID), ) return nil } func (p *OrderProcessor) saveOrder(ctx context.Context, order Order) error { ctx, span := tracer.Start(ctx, \"save-order-db\") defer span.End() _, err := p.db.ExecContext(ctx, \"INSERT INTO orders (order_id, total) VALUES ($1, $2)\", order.OrderID, order.Total, ) if err != nil { span.RecordError(err) } return err } Next Steps Configure Production Settings for deployment Review Quick Start for complete examples Explore Message Structure for additional context ","categories":"","description":"OpenTelemetry integration for Kafka processing","excerpt":"OpenTelemetry integration for Kafka processing","ref":"/humus/pr-preview/pr-346/features/queue/kafka/observability/","tags":"","title":"Observability"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"Working with generated specs","excerpt":"Working with generated specs","ref":"/humus/pr-preview/pr-346/features/rest/openapi/","tags":"","title":"OpenAPI"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/features/grpc/petstore-example/","tags":"","title":"Petstore Example"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/advanced/testing/","tags":"","title":"Testing"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"Monitoring service health","excerpt":"Monitoring service health","ref":"/humus/pr-preview/pr-346/features/rest/health-checks/","tags":"","title":"Health Checks"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/advanced/multi-source-config/","tags":"","title":"Multi Source Config"},{"body":"This guide covers best practices for deploying and configuring Kafka processors in production environments.\nComplete Configuration Example A production-ready configuration:\nkafka: brokers: - \"{{env \"KAFKA_BROKER_1\" | default \"kafka-1:9092\"}}\" - \"{{env \"KAFKA_BROKER_2\" | default \"kafka-2:9092\"}}\" - \"{{env \"KAFKA_BROKER_3\" | default \"kafka-3:9092\"}}\" group_id: \"{{env \"KAFKA_GROUP_ID\" | default \"order-processors\"}}\" topics: - name: \"orders\" semantic: \"at-least-once\" - name: \"payments\" semantic: \"at-least-once\" session_timeout: \"{{env \"KAFKA_SESSION_TIMEOUT\" | default \"30s\"}}\" rebalance_timeout: \"{{env \"KAFKA_REBALANCE_TIMEOUT\" | default \"45s\"}}\" fetch_max_bytes: {{env \"KAFKA_FETCH_MAX_BYTES\" | default \"52428800\"}} max_concurrent_fetches: {{env \"KAFKA_MAX_CONCURRENT_FETCHES\" | default \"10\"}} otel: service: name: \"{{env \"SERVICE_NAME\" | default \"order-processor\"}}\" version: \"{{env \"SERVICE_VERSION\" | default \"1.0.0\"}}\" environment: \"{{env \"ENVIRONMENT\" | default \"production\"}}\" sdk: disabled: false exporter: otlp: endpoint: \"{{env \"OTEL_ENDPOINT\" | default \"otel-collector:4317\"}}\" protocol: grpc traces: sampler: type: \"{{env \"OTEL_TRACE_SAMPLER\" | default \"traceidratio\"}}\" arg: {{env \"OTEL_TRACE_SAMPLE_RATE\" | default \"0.1\"}} metrics: interval: 60s database: host: \"{{env \"DB_HOST\" | default \"postgres\"}}\" port: {{env \"DB_PORT\" | default \"5432\"}} name: \"{{env \"DB_NAME\" | default \"orders\"}}\" user: \"{{env \"DB_USER\" | default \"orders_user\"}}\" password: \"{{env \"DB_PASSWORD\"}}\" max_connections: {{env \"DB_MAX_CONNECTIONS\" | default \"25\"}} Kafka Configuration Broker Configuration Multiple Brokers:\nkafka: brokers: - \"kafka-1.prod:9092\" - \"kafka-2.prod:9092\" - \"kafka-3.prod:9092\" Best practices:\nUse at least 3 brokers for redundancy Use DNS names, not IP addresses Configure all brokers, not just one Consumer Group ID Unique per deployment:\nkafka: group_id: \"order-processors-prod\" # Different from staging Naming conventions:\n{service}-{environment} Examples: - order-processors-prod - payment-processors-staging - analytics-processors-dev Topic Configuration Production topics:\nkafka: topics: - name: \"orders\" semantic: \"at-least-once\" - name: \"payments\" semantic: \"at-least-once\" - name: \"analytics\" semantic: \"at-most-once\" Topic naming:\n{domain}.{entity}.{event} Examples: - ecommerce.orders.created - ecommerce.payments.completed - analytics.events.tracked Timeouts Production timeouts:\nkafka: session_timeout: \"30s\" # Balance between failure detection and GC tolerance rebalance_timeout: \"45s\" # Must be \u003e session_timeout Guidelines:\nSession timeout: 20-45s (default: 30s) Rebalance timeout: 45-90s (default: 60s) Increase if frequent rebalances occur Decrease for faster failure detection Fetch Settings Production fetch settings:\nkafka: fetch_max_bytes: 52428800 # 50 MB max_concurrent_fetches: 10 # Limit concurrent requests Tuning guidelines:\nSmall messages: 10-25 MB fetch size Large messages: 100+ MB fetch size High throughput: Increase concurrent fetches Memory constrained: Decrease fetch size Application Configuration Idempotency Database-backed idempotency:\ntype Config struct { queue.Config `config:\",squash\"` Kafka struct { // ... kafka config } `config:\"kafka\"` Database struct { Host string `config:\"host\"` Port int `config:\"port\"` Name string `config:\"name\"` User string `config:\"user\"` Password string `config:\"password\"` MaxConnections int `config:\"max_connections\"` } `config:\"database\"` IdempotencyWindow time.Duration `config:\"idempotency_window\"` } func Init(ctx context.Context, cfg Config) (*queue.App, error) { // Setup database connection pool db, err := sql.Open(\"postgres\", fmt.Sprintf( \"host=%s port=%d user=%s password=%s dbname=%s sslmode=require\", cfg.Database.Host, cfg.Database.Port, cfg.Database.User, cfg.Database.Password, cfg.Database.Name, )) if err != nil { return nil, err } db.SetMaxOpenConns(cfg.Database.MaxConnections) db.SetMaxIdleConns(cfg.Database.MaxConnections / 2) db.SetConnMaxLifetime(time.Hour) processor := \u0026OrderProcessor{ db: db, idempotencyWindow: cfg.IdempotencyWindow, } runtime := kafka.NewRuntime( cfg.Kafka.Brokers, cfg.Kafka.GroupID, kafka.AtLeastOnce(cfg.Kafka.Topic, processor), kafka.SessionTimeout(cfg.Kafka.SessionTimeout), kafka.RebalanceTimeout(cfg.Kafka.RebalanceTimeout), ) return queue.NewApp(runtime), nil } Resource Limits Database connection pooling:\ndb.SetMaxOpenConns(25) // Limit total connections db.SetMaxIdleConns(10) // Idle connections db.SetConnMaxLifetime(1 * time.Hour) // Connection lifetime Calculation:\nMax connections = Partitions × Concurrent operations per partition Example: 12 partitions × 2 ops = 24 connections (use 25) Deployment Container Configuration Dockerfile:\nFROM golang:1.21-alpine AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 GOOS=linux go build -o processor ./cmd/processor FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=builder /app/processor . COPY config.yaml . CMD [\"./processor\"] Docker Compose:\nversion: '3.8' services: order-processor: image: order-processor:latest environment: - KAFKA_BROKER_1=kafka-1:9092 - KAFKA_BROKER_2=kafka-2:9092 - KAFKA_GROUP_ID=order-processors-prod - KAFKA_SESSION_TIMEOUT=30s - DB_HOST=postgres - DB_PASSWORD=${DB_PASSWORD} - OTEL_ENDPOINT=otel-collector:4317 depends_on: - kafka - postgres restart: unless-stopped deploy: replicas: 3 # Scale horizontally resources: limits: cpus: '1' memory: 512M reservations: cpus: '0.5' memory: 256M Kubernetes Deployment Deployment manifest:\napiVersion: apps/v1 kind: Deployment metadata: name: order-processor namespace: production spec: replicas: 3 selector: matchLabels: app: order-processor template: metadata: labels: app: order-processor spec: containers: - name: processor image: order-processor:1.0.0 env: - name: KAFKA_BROKER_1 value: \"kafka-1.kafka.svc:9092\" - name: KAFKA_BROKER_2 value: \"kafka-2.kafka.svc:9092\" - name: KAFKA_GROUP_ID value: \"order-processors-prod\" - name: DB_PASSWORD valueFrom: secretKeyRef: name: db-credentials key: password - name: OTEL_ENDPOINT value: \"otel-collector.monitoring.svc:4317\" resources: requests: memory: \"256Mi\" cpu: \"500m\" limits: memory: \"512Mi\" cpu: \"1000m\" livenessProbe: httpGet: path: /health/liveness port: 8080 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /health/readiness port: 8080 initialDelaySeconds: 5 periodSeconds: 5 Scaling Horizontal scaling:\n# Scale to 6 replicas kubectl scale deployment order-processor --replicas=6 Scaling guidelines:\nMax replicas ≤ number of partitions Start with 2-3 replicas for HA Scale up if consumer lag increases Monitor CPU/memory usage Example:\n12 partitions: - 3 replicas: 4 partitions each - 6 replicas: 2 partitions each - 12 replicas: 1 partition each (max) Monitoring Metrics to Monitor Consumer lag:\nkafka_consumer_lag{group=\"order-processors\",topic=\"orders\"} Alert when lag \u003e 1000:\n- alert: HighConsumerLag expr: kafka_consumer_lag \u003e 1000 for: 5m annotations: summary: \"Consumer group {{ $labels.group }} lagging\" Processing rate:\nrate(kafka_messages_processed_total[1m]) Error rate:\nrate(kafka_processing_errors_total[1m]) Health Checks Kubernetes probes:\nlivenessProbe: httpGet: path: /health/liveness port: 8080 initialDelaySeconds: 30 periodSeconds: 10 failureThreshold: 3 readinessProbe: httpGet: path: /health/readiness port: 8080 initialDelaySeconds: 5 periodSeconds: 5 failureThreshold: 3 Custom health check:\ntype HealthCheck struct { processor *OrderProcessor } func (h *HealthCheck) Healthy(ctx context.Context) (bool, error) { // Check database connection if err := h.processor.db.PingContext(ctx); err != nil { return false, err } // Check consumer lag (if available) // ... return true, nil } Security TLS Configuration Kafka TLS:\nimport \"github.com/twmb/franz-go/pkg/kgo\" tlsConfig := \u0026tls.Config{ MinVersion: tls.VersionTLS12, } runtime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.WithKafkaOptions( kgo.DialTLSConfig(tlsConfig), ), ) SASL Authentication SASL/SCRAM:\nimport ( \"github.com/twmb/franz-go/pkg/kgo\" \"github.com/twmb/franz-go/pkg/sasl/scram\" ) scramAuth := scram.Auth{ User: cfg.Kafka.User, Pass: cfg.Kafka.Password, } runtime := kafka.NewRuntime( brokers, groupID, kafka.AtLeastOnce(\"orders\", processor), kafka.WithKafkaOptions( kgo.SASL(scramAuth.AsSha256Mechanism()), ), ) Secrets Management Kubernetes secrets:\napiVersion: v1 kind: Secret metadata: name: kafka-credentials type: Opaque stringData: username: \"order-processor-user\" password: \"secure-password-here\" Reference in deployment:\nenv: - name: KAFKA_USER valueFrom: secretKeyRef: name: kafka-credentials key: username - name: KAFKA_PASSWORD valueFrom: secretKeyRef: name: kafka-credentials key: password Performance Tuning Partition Count Optimal partition count:\nPartitions = Target throughput / Partition throughput Example: Target: 100k msgs/sec Per partition: 10k msgs/sec Partitions: 100k / 10k = 10 Recommendations:\nStart with 3× expected consumer count Monitor lag and throughput Increase if lag grows under load Cannot decrease (Kafka limitation) Batch Processing Process messages in batches:\ntype BatchProcessor struct { db *sql.DB batchSize int } func (p *BatchProcessor) ProcessBatch(ctx context.Context, messages []kafka.Message) error { tx, err := p.db.BeginTx(ctx, nil) if err != nil { return err } defer tx.Rollback() for _, msg := range messages { var order Order json.Unmarshal(msg.Value, \u0026order) _, err := tx.ExecContext(ctx, \"INSERT INTO orders (order_id, total) VALUES ($1, $2)\", order.OrderID, order.Total, ) if err != nil { return err } } return tx.Commit() } Connection Pooling Optimize database connections:\n// Formula: Max connections = Partitions × 2 db.SetMaxOpenConns(cfg.Partitions * 2) db.SetMaxIdleConns(cfg.Partitions) db.SetConnMaxLifetime(1 * time.Hour) db.SetConnMaxIdleTime(10 * time.Minute) Disaster Recovery Consumer Group Reset Reset to earliest:\nkafka-consumer-groups.sh \\ --bootstrap-server kafka:9092 \\ --group order-processors \\ --reset-offsets \\ --to-earliest \\ --all-topics \\ --execute Reset to specific offset:\nkafka-consumer-groups.sh \\ --bootstrap-server kafka:9092 \\ --group order-processors \\ --reset-offsets \\ --topic orders:0 \\ --to-offset 12345 \\ --execute Backup and Restore Export offsets:\nkafka-consumer-groups.sh \\ --bootstrap-server kafka:9092 \\ --group order-processors \\ --describe \u003e offsets-backup.txt Dead Letter Queue:\nfunc (p *Processor) Process(ctx context.Context, msg kafka.Message) error { if err := p.processMessage(ctx, msg); err != nil { // Send to DLQ on failure return p.sendToDLQ(ctx, msg, err) } return nil } func (p *Processor) sendToDLQ(ctx context.Context, msg kafka.Message, err error) error { dlqRecord := \u0026kgo.Record{ Topic: msg.Topic + \".dlq\", Key: msg.Key, Value: msg.Value, Headers: append(msg.Headers, kafka.Header{ Key: \"error\", Value: []byte(err.Error()), }), } p.producer.Produce(ctx, dlqRecord, nil) return nil // Don't return error, message handled } Troubleshooting High Consumer Lag Causes:\nSlow processing Insufficient consumers Hot partitions Solutions:\n# Scale up consumers kubectl scale deployment order-processor --replicas=6 # Check for hot partitions kafka-consumer-groups.sh --describe --group order-processors # Optimize processing code # Add database indexes # Batch operations Frequent Rebalances Causes:\nShort session timeout Long processing time Network issues Solutions:\nkafka: session_timeout: \"45s\" # Increase rebalance_timeout: \"90s\" # Increase Memory Issues Causes:\nLarge fetch buffers Too many partitions Memory leaks Solutions:\nkafka: fetch_max_bytes: 25000000 # Reduce to 25 MB max_concurrent_fetches: 5 # Limit concurrent fetches Next Steps Review Quick Start for basic setup Learn Idempotency patterns Explore Observability for monitoring ","categories":"","description":"Deploying Kafka processors in production","excerpt":"Deploying Kafka processors in production","ref":"/humus/pr-preview/pr-346/features/queue/kafka/configuration/","tags":"","title":"Production Configuration"},{"body":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","categories":"","description":"","excerpt":"Documentation coming soon.\nSee pkg.go.dev for API reference.\n","ref":"/humus/pr-preview/pr-346/advanced/concurrent-utilities/","tags":"","title":"Concurrent Utilities"},{"body":"","categories":"","description":"","excerpt":"","ref":"/humus/pr-preview/pr-346/categories/","tags":"","title":"Categories"},{"body":"A modular Go framework for building production-ready services with standardized observability, health checks, and graceful shutdown.\nHumus is built on top of Bedrock and provides opinionated patterns for three types of applications:\nService Types REST/HTTP Services Build OpenAPI-compliant web applications with automatic schema generation, built-in health endpoints, and flexible request/response handling.\nGet Started with REST →\ngRPC Services Create gRPC microservices with automatic OpenTelemetry instrumentation, health service registration, and seamless integration with the gRPC ecosystem.\nGet Started with gRPC →\nJob Services Build one-off job executors for batch processing, migrations, or scheduled tasks with the same observability and lifecycle management as long-running services.\nGet Started with Jobs →\nKey Features Batteries Included Observability\nAutomatic OpenTelemetry SDK initialization for traces, metrics, and logs Integrated structured logging with slog Zero-configuration instrumentation for HTTP and gRPC Production Ready\nGraceful shutdown with OS signal handling Standardized health check patterns Panic recovery middleware YAML-based configuration with templating support Developer Friendly\nMinimal boilerplate with Builder + Runner pattern Automatic OpenAPI schema generation from Go types Type-safe request/response handling Comprehensive examples and documentation Quick Example package main import ( \"context\" \"net/http\" \"github.com/z5labs/humus/rest\" \"github.com/z5labs/humus/rest/rpc\" ) type Config struct { rest.Config `config:\",squash\"` } func main() { rest.Run(rest.YamlSource(\"config.yaml\"), Init) } func Init(ctx context.Context, cfg Config) (*rest.Api, error) { api := rest.NewApi(\"My Service\", \"v1.0.0\") handler := rpc.NewOperation( rpc.Handle(func(ctx context.Context, req any) (string, error) { return \"Hello, World!\", nil }), ) rest.Handle(http.MethodGet, rest.BasePath(\"/hello\"), handler) return api, nil } Next Steps Installation and Getting Started Core Concepts API Reference Resources GitHub Repository GitHub Discussions Go Package Documentation ","categories":"","description":"","excerpt":"A modular Go framework for building production-ready services with …","ref":"/humus/pr-preview/pr-346/","tags":"","title":"Humus"},{"body":"","categories":"","description":"","excerpt":"","ref":"/humus/pr-preview/pr-346/tags/","tags":"","title":"Tags"}]